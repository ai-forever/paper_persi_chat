{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add hard negatives to Deberta's train\n",
    "\n",
    "Uses SPECTER v2 proximity\n",
    "https://huggingface.co/allenai/specter2_proximity\n",
    "\n",
    "Sorts possible answers using cosine distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -U adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/test_env2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_train_upd.pkl', 'rb') as f:\n",
    "    train_squad = pickle.load(f)\n",
    "    \n",
    "with open('/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_valid_upd.pkl', 'rb') as f:\n",
    "    valid_squad = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2')\n",
    "\n",
    "#load base model\n",
    "model = AutoModel.from_pretrained('allenai/specter2')\n",
    "\n",
    "#load the adapter(s) as per the required task, provide an identifier for the adapter in load_as argument and activate it\n",
    "model.load_adapter(\"allenai/specter2_proximity\", source=\"hf\", load_as=\"specter2_proximity\", set_active=True)\n",
    "\n",
    "model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45174/45174 [03:12<00:00, 234.85it/s]\n"
     ]
    }
   ],
   "source": [
    "specter2_embeddings = {}\n",
    "all_data = train_squad + valid_squad\n",
    "i=0\n",
    "\n",
    "for d in tqdm(all_data):\n",
    "    paper_id = '_'.join(d['id'].split('_')[0:2])\n",
    "    \n",
    "    if paper_id not in specter2_embeddings:\n",
    "        t1 = '' if  type(d['title']) == float else d['title']\n",
    "        txt = t1 + tokenizer.sep_token + d['context'].replace(tokenizer.sep_token, '')\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            [txt], \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\", \n",
    "            return_token_type_ids=False, \n",
    "            max_length=512).to('cuda')\n",
    "        \n",
    "        output = model(**inputs)\n",
    "        embeddings = output.last_hidden_state[0, 0, :].cpu().detach().numpy().astype(np.float16)\n",
    "        specter2_embeddings[paper_id] = {\n",
    "            'specter2_text': txt,\n",
    "            'specter2_embedding': embeddings,\n",
    "            }\n",
    "                \n",
    "with open('/home/jovyan/chatbot/_common/datasets/deberta_retrain/specter2_embeddings_upd.pkl', 'wb') as f:\n",
    "    pickle.dump(specter2_embeddings, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11602"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(specter2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = list(specter2_embeddings.keys())\n",
    "all_values = [specter2_embeddings[id]['specter2_embedding'] for id in all_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = cosine_distances(all_values, all_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = set(['_'.join(el['id'].split('_')[0:2]) for el in valid_squad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2neig = {all_ids[i]: [all_ids[k] for k in np.argpartition(dists[i],20)[:20] if k != i and all_ids[k] not in test_ids][:3] for i in range(len(dists))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2quest = {}\n",
    "for el in train_squad:\n",
    "    id = '_'.join(el['id'].split('_')[0:2])\n",
    "    if id in id2quest:\n",
    "        id2quest[id].append(el['question'])\n",
    "    else:\n",
    "        id2quest[id] = [el['question']]\n",
    "        \n",
    "id2context = {}\n",
    "for el in train_squad:\n",
    "    id = '_'.join(el['id'].split('_')[0:2])\n",
    "    id2context[id] = (el['title'], el['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Key Principles', 'To allow PyDial to be applied to new problems easily, the PyDial architecture is designed to support three key principles: Domain Independence Wherever possible, the implementation of the dialogue modules is kept separate from the domain specification. Thus, the main functionality is domain independent, i.e., by simply using a different domain specification, simulated dialogues using belief tracker and policy are possible. To achieve this, the Ontology handles all domain-related functionality and is accessible system-wide. While this is completely true for the belief tracker, the policy, and the user simulator, the semantic decoder and the language generator inevitably have some domain-dependency and each needs domain-specific models to be loaded. To use PyDial, all relevant functionality can be controlled via a configuration file. This specifies the domains of the conversation, the variant of each domain module, which is used in the pipeline, and its parameters. For example, to use a hand-crafted policy in the domain CamRestaurants, a configuration section [policy CamRestaurants] with the entry policytype = hdc is used. The configuration file is then loaded by Pydial and the resulting configuration object is globally accessible. Extensibility One additional benefit of introducing the manager concept described in Sec. 2.2 is to allow for easy extensibility. As shown with the example in Figure 4, each manager contains a set of D domain instances. The class of each domain instance inherits from the interface class and must implement all of its interface methods. To add a new module, the respective class simply needs to adhere to the required interface definition. To use it in the running system, the configuration parameter may simply point to the new class, e.g., policytype = policy.HDCPolicy.HDCPolicy. The following modules and components support this functionality: Topic Tracker, Semantic Decoder, Belief Tracker, Policy, Language Generator, and Evaluation. Since the configuration file is a simple text file, new entries can be added easily using a convenient text editor and any special configuration options can easily be added. To add a new domain, a simulated interaction is already possible simply by defining the ontology along with the database. For text-based interaction, an additional understanding and generation component is necessary.')\n",
      "\n",
      "\n",
      "\n",
      "('Overview', \"First, the participants will get a hands-on introduction to one of the Operational Language and Culture courses. Under supervision of a presenter, Figure 2. Screen shot of a Mission Game dialog in Operational Dari TM the participants will learn to say a few phrases in the Skill Builder and use the phrases that they have learned in the Mission Game. This portion can be tailored on the fly to the interests of participants, and can take from 5 to 30 minutes to complete. Depending on time and interest, participants may also have an opportunity to work with an OLCTS course in more depth. They can be called upon to learn some basic communication skills in Dari and apply them in the Mission Game. This will give participants a firsthand understanding of how each component of OLCTS supports learning, how the components support each other, and how artificial intelligence technology is applied in the learning experience. Finally, the presenter will demo some of the authoring tools used to create OLCTS content. The participants will propose modifications or extensions to an existing OLCTS course. The presenter will use the authoring tools in real time to make the modifications, following the recommendations of the participants. For a video summary of the demonstration, please visit http://www.alelo.com/movie_tlt-6min.html. The user experience in the Mission Game is one engaging component of this demonstration. An example script for a Mission Game interaction in Alelo's Operational Dari TM course is given in the following sections. A sample of a Mission Game screen is shown in Figure 2. The player controls the figure in the center-left. At this point in the demonstration, the player has received a briefing that describes a communication task that he or she should accomplish in this exercise. To complete the task, the player must engage the virtual human, or nonplayer character (NPC) shown on the right. Organizing rebuilding operations is one example of such a task. The NPC is a host-national character in Afghanistan. The player should check on the status of their shared plan for rebuilding and give constructive feedback. This type of communication task can require finesse and delicacy on the part of the player in order to be culturally appropriate. It draws on the learner's understanding and skill with face-saving, a prominent feature of many cultures worldwide. The learner must initiate the conversation by speaking into a headset-mounted microphone. He or she clicks on the microphone icon, shown in Figure 3, speaks, then clicks on the icon again to indicate the end of the turn. Figure 2. Push the microphone button to speak during a dialog, push again to stop. Recognized player speech is posted to a dialog history window that appears near the top of the virtual scene, as shown in Figure 1. The NPC responds using spoken output, creating a realistic and engaging practice environment. During the dialog, the player may view hints that display key phrases in Dari. Once the player has discussed all of the host national's training mistakes, the dialog ends in success.\")\n",
      "\n",
      "('abstract', 'How obliged can we be to AI, and how much danger does it pose us? A surprising proportion of our society holds exaggerated fears or hopes for AI, such as the fear of robot world conquest, or the hope that AI will indefinitely perpetuate our culture. These misapprehensions are symptomatic of a larger problem-a confusion about the nature and origins of ethics and its role in society. While AI technologies do pose promises and threats, these are not qualitatively different from those posed by other artifacts of our culture which are largely ignored: from factories to advertising, weapons to political systems. Ethical systems are based on notions of identity, and the exaggerated hopes and fears of AI derive from our cultures having not yet accommodated the fact that language and reasoning are no longer uniquely human. The experience of AI may improve our ethical intuitions and self-understanding, potentially helping our societies make better-informed decisions on serious ethical dilemmas. * An earlier version of this work was partially published in Proceedings of the 15 th International Congress on Cybernetics. \"Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended. Can the Singularity be avoided? If not to be avoided, can events be guided so that we may survive? What does survival even mean in a Post-Human Era?\"-Vernor Vinge, The Coming Technological Singularity: How to Survive in the Post-Human Era (1995). \"Technologists are providing almost religious visions, and their ideas are resonating in some ways with the same idea of the Rapture.\" -Eric Horvitz, Scientists Worry Machines May Outsmart Man [Markoff, 2009]. Not all computer scientists consider world conquest by machines probable, or even possible. However, such fears have been a persistent part of our culture, not only in fiction but also in scientific writings [ de Garis, 1990]. What can lead even computer scientists to believe that AI endangers our society? Computer programs, including those classified as Artificial Intelligence (AI), are purpose-built artifacts designed, commissioned and operated by human beings. Computers can accelerate and magnify our mistakes to do more damage than an unaided individual, yet the same could be said of levers, pulleys and organised government. We believe exaggerated fears of, and hopes for, AI are symptomatic of a larger problem-a general confusion about the nature of humanity and the role of ethics in society. To the category of exaggerated fear we assign the notions of ambitious or machine-loyal AIs that make selfish decisions about the relative importance of their own intelligence, growth or energy. The category of exaggerated hopes includes the expectation that machine intelligence will perpetuate either individual or planetary experience and culture past the normal life expectancy of a human individual or the human species. Our thesis is that these are false concerns, which can distract us from the real dangers of AI technologies. The real dangers of AI are no different from those of other artifacts in our culture: from factories to advertising, weapons to political systems. The danger of these systems is the potential for misuse, either through carelessness or malevolence, by the people who control them. Social ethics is derived from each individuals\\' personal sense of obligation. The proximate (but not ultimate [West et al., 2011]) mechanism of that obligation is an individual\\'s identification with its object. This explains the misplaced hopes and fears for AI if they come from individuals\\' inappropriate identification with machine intelligence. Yet AI, properly understood, might be used to help us rationalise our ethical systems, leaving us time to address the real threats to our societies and cultures, including those stemming from the misuse of advanced technology such as AI.')\n",
      "\n",
      "('Introduction', \"There is a tremendous amount of information available on the Web, but the access to this information is largely dependent on the information providers. The individual web sites decide what information to provide to the visitors to their site and how a visitor will access that information. There is little in the way of capabilities to combine information across sites and only the most primitive capabilities to monitor information on individual sites. Of course, sites may provide integrated access to specific data sources or sophisticated monitoring capabilities on their data, but users of the Web are dependent on a site to make these capabilities available. In contrast, imagine a world where access to information is driven by the consumers of the data. Where it is not only possible, but simple to task your own personal information agents to gather specific information, monitor for changes in the gathered data, and to notify you of important information or changes. The challenges are that the required information is often embedded in HTML pages, the data is organized in different ways on different sites, there are a huge number of ways the information could be combined, and it can be slow and cumbersome to combine the information. cated task with many possible forms of failure ranging from flight cancellations and schedule changes to hotel rooms being given away when a traveler arrives late at night, b) there are a large number of online resources that can be exploited to anticipate problems and keep a traveler informed, and c) these tasks would be tedious and impractical for a human to perform with the same level of attention that could be provided by a set of software agents. To deploy a set of agents for monitoring a planned trip, the user first enters the travel itinerary and then specifies which aspects of the trip she would like to have the agents monitor. A set of information agents are then spawned to perform the requested monitoring activities. For the travel planning application, we developed the following set of agents to monitor a trip: price, address, phone number, latitude, longitude, and distance from the user's location. These agents are scheduled to run at regular intervals, where the agents are woken up to perform their task. The agents can cancel their own task once it is complete and can change the interval in which they are run based on the information from other agents. The agents often invoke other agents to help them perform their tasks For example, the flight-status agent calls another agent that extracts the flight status information directly from a web site and it invokes the hotel notification agent, which in turn sends a message to the fax agent. Figure 1 shows the messages that various agents generated during actual use of the system. The original set of agents were in use for about a year and then based on feedback and requests from the users we recently developed a new set of agents that provide improved capabilities.\")\n"
     ]
    }
   ],
   "source": [
    "key = list(id2quest.keys())[2300]\n",
    "print(id2context[key])\n",
    "print()\n",
    "print()\n",
    "for el in id2neig[key]:\n",
    "    print()\n",
    "    print(id2context[el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What kind of models were trained?',\n",
       " 'What is the article about?',\n",
       " 'What is the baseline method?',\n",
       " 'What is the paper about?',\n",
       " 'What is the purpose of the paper?',\n",
       " 'What is the mechanism that is being discussed in this paper?',\n",
       " 'What are the datasets discussed in the paper?',\n",
       " 'What is the main topic of the article?',\n",
       " 'What is the focus of the paper?',\n",
       " 'What is the purpose of the paper?',\n",
       " 'What is the focus of the study mentioned in the summary?',\n",
       " 'What is the model presented in the paper?',\n",
       " 'What is the paper about?',\n",
       " 'What is the purpose of the paper?',\n",
       " 'What is the context of the article?',\n",
       " 'What is the purpose of the algorithm introduced in the paper?',\n",
       " 'what is this algorithm used for?',\n",
       " 'What is the context or topic of the article?',\n",
       " 'What is the article about?',\n",
       " 'What is the paper discussing?',\n",
       " 'What is the paper about?',\n",
       " 'What is the article about?',\n",
       " 'What is the focus of the paper?',\n",
       " 'What is the paper examining?',\n",
       " 'What is the proof about?',\n",
       " 'What was the purpose of the experiments?',\n",
       " 'What is the purpose of the paper?',\n",
       " 'What is the topic of the article?',\n",
       " 'What is the proposed method in this paper?',\n",
       " 'What is the paper discussing?',\n",
       " 'What is the topic of the paper?',\n",
       " 'What is the method presented in the paper?',\n",
       " 'What is the approach being examined in the paper?',\n",
       " 'What is the article discussing?',\n",
       " 'What is the method proposed in the paper?',\n",
       " 'What is the main focus of the paper?',\n",
       " 'What is the main topic of the article?',\n",
       " 'What is the paper evaluating?',\n",
       " 'What is the main focus of the article?',\n",
       " 'What is the main topic of the paper?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of general questions to remove\n",
    "manual_selected = '''\n",
    "What kind of models were trained?\n",
    "What is the article about?\n",
    "What is the baseline method?\n",
    "What is the paper about?\n",
    "What is the purpose of the paper?\n",
    "What is the mechanism that is being discussed in this paper?\n",
    "What are the datasets discussed in the paper?\n",
    "What is the main topic of the article?\n",
    "What is the focus of the paper?\n",
    "What is the purpose of the paper?\n",
    "What is the focus of the study mentioned in the summary?\n",
    "What is the model presented in the paper?\n",
    "What is the paper about?\n",
    "What is the purpose of the paper?\n",
    "What is the context of the article?\n",
    "What is the purpose of the algorithm introduced in the paper?\n",
    "what is this algorithm used for? \n",
    "What is the context or topic of the article?\n",
    "What is the article about?\n",
    "What is the paper discussing?\n",
    "What is the paper about?\n",
    "What is the article about?\n",
    "What is the focus of the paper? \n",
    "What is the paper examining?  \n",
    "What is the proof about? \n",
    "What was the purpose of the experiments?\n",
    "What is the purpose of the paper?\n",
    "What is the topic of the article?\n",
    "What is the proposed method in this paper? \n",
    "What is the paper discussing?\n",
    "What is the topic of the paper?\n",
    "What is the method presented in the paper?\n",
    "What is the approach being examined in the paper?\n",
    "What is the article discussing?\n",
    "What is the method proposed in the paper?\n",
    "What is the main focus of the paper?\n",
    "What is the main topic of the article?\n",
    "What is the paper evaluating?\n",
    "What is the main focus of the article?\n",
    "What is the main topic of the paper?\n",
    "Thank you!\n",
    "'''.split('?')\n",
    "manual_selected = [m.strip()+'?' for m in manual_selected][:-1]\n",
    "manual_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hard_negatives(ids, initial_set):\n",
    "    random.seed(5757)\n",
    "    set_with_hard = initial_set.copy()\n",
    "    for id in tqdm(ids):\n",
    "        try:\n",
    "            closest_quests = sum([id2quest[idx][2:] for idx in id2neig[id]], [])  # first 2 are common\n",
    "            closest_quests = [q for q in closest_quests if q not in manual_selected]\n",
    "            closest_quests_sample = random.sample(closest_quests, 2)  # 1 closest in all cases\n",
    "\n",
    "            if random.random() < 0.75:\n",
    "                closest_quests_sample = closest_quests_sample[:1]\n",
    "\n",
    "            for el in initial_set:\n",
    "                if '_'.join(el['id'].split('_')[0:2]) == id:\n",
    "                    new_item = el.copy()\n",
    "                    break\n",
    "            new_item['answers'] = {'text': [], 'answer_start': []}\n",
    "            new_item['chat_gpt_answer'] = ''\n",
    "\n",
    "            for q in closest_quests_sample:\n",
    "                assert q not in manual_selected\n",
    "                new_item_add = new_item.copy()\n",
    "                new_item_add['question'] = q\n",
    "                set_with_hard.append(new_item_add)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for entry in set_with_hard:\n",
    "        entry['title'] = '' if type(entry['title']) != str else entry['title']\n",
    "    return set_with_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_non_negative(data):\n",
    "    return [el for el in data if not (el['question'] in manual_selected and len(el['answers']['answer_start']) == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11202/11202 [01:31<00:00, 121.97it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ids = set(['_'.join(el['id'].split('_')[0:2]) for el in train_squad])\n",
    "train_with_hard = add_hard_negatives(train_ids, train_squad)\n",
    "train_with_hard = filt_non_negative(train_with_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43615, 57306)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_squad), len(train_with_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '02c3a1d660128b694ce0aa97ab857eea1851193c_0_0_1',\n",
       " 'title': 'abstract',\n",
       " 'context': 'Temporal irradiance variations are useful for finding dense stereo correspondences. These variations can be created artificially using structured light. They also occur naturally underwater. We introduce a variational optimization formulation for finding a dense stereo correspondence field. It is based on multi-frame optical flow, adapted to stereo. The formulation uses a sequence of stereo frames, and yields dense and robust results. The inherent aperture problem of optical flow is resolved using a temporal sequence of stereo frame-pairs. The results are achieved even without considering epi-polar geometry. The method has the ability to handle dynamic stereo underwater, in harsh conditions of flickering illumination. The method is demonstrated experimentally both outdoors and indoors. We use the L 1 norm, as described in Sec. 2. Therefore, Ψ is determined according to Eq. (3) in our scheme. Stereo correspondence is well-studied in computer vision [25]. An important set of methods for establishing correspondence uses optimization methods: graph cuts [5], belief propagation [30] and methods based on an optical flow [6,14,29] formulation. Dense correspondence achievable using optical flow formulation for stereo systems is useful to refine [1] or calibrate [19,37] the epipolar geometry. Furthermore, an optical flow formulation partly compensates for lack of local constraints, using flow smoothness terms. However, the accuracy and reliability of methods seeking dense correspondence fields eventually depend on the scene texture. Irrespective of optimization formulation for stereo, Refs. [9,40] show that spatiotemporal information can be useful for finding dense correspondence, using structured light. Spatiotemporal variations also occur naturally underwater (Fig. 1.a). There, an effect called sunlight flicker 1 [12,28] exists. Submerged objects are illuminated * This work relates to Department of the Navy Grant N62909-10-1-4056 issued by the Office of Naval Research Global. The United States Government has a royalty-free license throughout the world in all copyrightable material contained herein. 1 Interestingly, in many marine animals, vision is adapted to spatiotem- by a natural random pattern (caustics) [10,38]. The refraction of sunlight through a wavy water surface creates inhomogeneous lighting. This has implications for underwater computer vision. The domain of underwater computer vision [4,17,27,36] focuses on oceanic engineering, which includes automated vehicle control [7,15], inspection of pipelines [11], communication cables, ports and ship hulls [23]. Computer vision is also used inside swimming pools [18]. Refs. [31,32,33] show that sunlight flicker can be useful for finding dense correspondence in underwater stereo, using spatiotemporal correlation. However, a relatively large number of frames is needed in order to acquire enough local constraints to disambiguate correspondence. poral frequencies of flicker [20]. This paper combines optical flow for stereo and spatiotemporal stereo into a unified formulation. The formulation uses variational expressions for dense correspondence in stereoscopic video, where spatiotemporal irradiance variations exist. The formulation uses a sequence of frames. A scene is acquired from two viewpoints, in video. If the scene irradiance changes temporally, each stereo pair of frames adds information to a data term of an optimization cost function. This yields an accurate and dense correspondence map, with a relatively small number of frames and fast computations. This, in turn, enables dynamic stereo, where camera or scene motions exist. We demonstrate this feature in an underwater experiment.',\n",
       " 'question': 'Does the proposed algorithm produce good results?',\n",
       " 'chat_gpt_answer': '',\n",
       " 'answers': {'text': [], 'answer_start': []}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_hard[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_train_withhn_filt_upd.pkl', 'wb') as f:\n",
    "    pickle.dump(train_with_hard, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 3506.27it/s]\n"
     ]
    }
   ],
   "source": [
    "test_ids = set(['_'.join(el['id'].split('_')[0:2]) for el in valid_squad])\n",
    "valid_with_hard = add_hard_negatives(test_ids, valid_squad)\n",
    "valid_with_hard = filt_non_negative(valid_with_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1559, 2039)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_squad), len(valid_with_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1cb4e99c30e602be6af42b559d7b667307d1a853_7_0_1',\n",
       " 'title': 'Experiments',\n",
       " 'context': 'For all experiments, we use a radial basis-function (RBF) kernel as in [15] , i.e., k(x, x ) = exp(− 1 h x − x 2 2 ) , where the bandwidth, h, is the median of pairwise distances between current samples. q 0 (θ) and q 0 (ξ) are set to isotropic Gaussian distributions. We share the samples of ξ across data points, i.e., ξ jn = ξ j , for n = 1, . . . , N (this is not necessary, but it saves computation). The samples of θ and z, and parameters of the recognition model, η, are optimized via Adam [9] with learning rate 0.0002. We do not perform any dataset-specific tuning or regularization other than dropout [32] and early stopping on validation sets. We set M = 100 and k = 50, and use minibatches of size 64 for all experiments, unless otherwise specified. 1 −2 and σ = 0.1. The recognition model f η (x n , ξ j ) is specified as a multi-layer perceptron (MLP) with 100 hidden units, by first concatenating ξ j and x n into a long vector. The dimension of ξ j is set to 2. The recognition model for standard VAE is also an MLP with 100 hidden units, and with the assumption of a Gaussian distribution for the latent codes [11]. Poisson Factor Analysis Given a discrete vector x n ∈ Z P + , Poisson factor analysis [36] assumes x n is a weighted combination of V latent factors x n ∼ Pois(θz n ), where θ ∈ R P ×V + is the factor loadings matrix and z n ∈ R V + is the vector of factor scores. We consider topic modeling with Dirichlet priors on θ v (v-th column of θ) and gamma priors on each component of z n . We evaluate our model on the 20 Newsgroups dataset containing N = 18, 845 documents with a vocabulary of P = 2, 000. The data are partitioned into 10,314 training, 1,000 validation and 7,531 test documents. The number of factors (topics) is set to V = 128. θ is first learned by Markov chain Monte Carlo (MCMC) [4]. We then fix θ at its MAP value, and only learn the recognition model η using standard VAE and Stein VAE; this is done, as in the previous example, to examine the accuracy of the recognition model to estimate the posterior of the latent factors, isolated from estimation of θ. The recognition model is an MLP with 100 hidden units. An analytic form of the true posterior distribution p(z n |x n ) is intractable for this problem. Consequently, we employ samples collected from MCMC as ground truth. With θ fixed, we sample z n via Gibbs sampling, using 2,000 burn-in iterations followed by 2,500 collection draws, retaining every 10th collection sample. We show the marginal and pairwise posterior of one test data point in Figure 2. Additional results are provided in Appendix F. Stein VAE leads to a more accurate approximation than standard VAE, compared to the MCMC samples. Considering Figure 2, note that VAE significantly underestimates the variance of the posterior (examining the marginals), a well-known problem of variational Bayesian analysis [7]. In sharp contrast, Stein VAE yields highly accurate approximations to the true posterior. ) is the entropy. The expectation is approximated with samples {θ j } M j=1 and {z * j } M j=1 with z * j = f η (x * , ξ j ), ξ j ∼ q 0 (ξ). Directly evaluating q(z * ) is intractable, thus it is estimated via density transformation q(z) = q 0 (ξ) det ∂f η (x,ξ) ∂ξ −1 . Model For MNIST, we train the model with one stochastic layer, z n , with 50 hidden units and two deterministic layers, each with 200 units. The nonlinearity is set as tanh. The visible layer, x n , follows a Bernoulli distribution. For the text corpora, we build a three-layer deep Poisson network [24]. The sizes of hidden units are 200, 200 and 50 for the first, second and third layer, respectively (see [24] for detailed architectures).',\n",
       " 'question': 'How many different parameterizations of the network were evaluated?',\n",
       " 'chat_gpt_answer': '',\n",
       " 'answers': {'text': [], 'answer_start': []}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_with_hard[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_valid_withhn_filt_upd.pkl', 'wb') as f:\n",
    "    pickle.dump(valid_with_hard, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
