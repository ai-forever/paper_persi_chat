{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5089a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = '' # data path; dowload from https://drive.google.com/drive/folders/1vSo9pr10h9lF3DHeWwUNaTYsyXBw7P06?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07551f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gds/anaconda3/envs/py36gosus/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "segments = pd.read_csv(data_path + 'SEGMENTS.csv')\n",
    "papers = pd.read_csv(data_path + 'PAPERS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8dfac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73986, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300ca703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>in_links</th>\n",
       "      <th>out_links</th>\n",
       "      <th>conference</th>\n",
       "      <th>authors_ids</th>\n",
       "      <th>conf_pub_date</th>\n",
       "      <th>arxiv_pub_date</th>\n",
       "      <th>min_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>756eac52299df7c1e525e72bb53a6e0ececdcf01</td>\n",
       "      <td>You are caught stealing my winning lottery tic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55da1c1b1c5fd16c91a26dce223f94c2592122e9,26384...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2118253285</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>2021-10-30</td>\n",
       "      <td>2021-10-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f9234ce567d234d1438939dd30a3cc72ea3bf741</td>\n",
       "      <td>Counting Maximal Satisfiable Subsets</td>\n",
       "      <td>e84cdfbf968e0651d5736d322c6fcdc3f5671331,f4ad1...</td>\n",
       "      <td>7f99a6830adb3c3d7083c1695437538082092740,e033d...</td>\n",
       "      <td>AAAI</td>\n",
       "      <td>3418385,3207775</td>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26380f0bf00ccffa32170838dc22f53ae096dfc4</td>\n",
       "      <td>Unsupervised Concept Representation Learning f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a5f25ffd6943672d92675c3e2a044830314a7a85,fcbca...</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>2048981220,2991105,145859270,2090567,3215702,1...</td>\n",
       "      <td>2021-08-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187014ec532e7f593da1a24cd744ce6e17695a3d</td>\n",
       "      <td>Challenges and Opportunities of Building Fast ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5fd035907cf17542631feb891babe3235e56c198,d8d3f...</td>\n",
       "      <td>IJCAI</td>\n",
       "      <td>2108107022,2068228300</td>\n",
       "      <td>2021-08-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18140e61c2242369c7f960d4f1344110911fe498</td>\n",
       "      <td>Communication-Efficient Frank-Wolfe Algorithm ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>566d1de360661c27d3477a324cd78a03c5a9cae4,4da06...</td>\n",
       "      <td>AAAI</td>\n",
       "      <td>151487059,3057688,145114933</td>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  756eac52299df7c1e525e72bb53a6e0ececdcf01   \n",
       "1  f9234ce567d234d1438939dd30a3cc72ea3bf741   \n",
       "2  26380f0bf00ccffa32170838dc22f53ae096dfc4   \n",
       "3  187014ec532e7f593da1a24cd744ce6e17695a3d   \n",
       "4  18140e61c2242369c7f960d4f1344110911fe498   \n",
       "\n",
       "                                               title  \\\n",
       "0  You are caught stealing my winning lottery tic...   \n",
       "1               Counting Maximal Satisfiable Subsets   \n",
       "2  Unsupervised Concept Representation Learning f...   \n",
       "3  Challenges and Opportunities of Building Fast ...   \n",
       "4  Communication-Efficient Frank-Wolfe Algorithm ...   \n",
       "\n",
       "                                            in_links  \\\n",
       "0                                                NaN   \n",
       "1  e84cdfbf968e0651d5736d322c6fcdc3f5671331,f4ad1...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                           out_links conference  \\\n",
       "0  55da1c1b1c5fd16c91a26dce223f94c2592122e9,26384...       NIPS   \n",
       "1  7f99a6830adb3c3d7083c1695437538082092740,e033d...       AAAI   \n",
       "2  a5f25ffd6943672d92675c3e2a044830314a7a85,fcbca...      NAACL   \n",
       "3  5fd035907cf17542631feb891babe3235e56c198,d8d3f...      IJCAI   \n",
       "4  566d1de360661c27d3477a324cd78a03c5a9cae4,4da06...       AAAI   \n",
       "\n",
       "                                         authors_ids conf_pub_date  \\\n",
       "0                                         2118253285    2021-12-07   \n",
       "1                                    3418385,3207775    2021-02-02   \n",
       "2  2048981220,2991105,145859270,2090567,3215702,1...    2021-08-02   \n",
       "3                              2108107022,2068228300    2021-08-21   \n",
       "4                        151487059,3057688,145114933    2021-02-02   \n",
       "\n",
       "  arxiv_pub_date    min_date  \n",
       "0     2021-10-30  2021-10-30  \n",
       "1            NaN  2021-02-02  \n",
       "2            NaN  2021-08-02  \n",
       "3            NaN  2021-08-21  \n",
       "4            NaN  2021-02-02  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd139aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1252953, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb754d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>parsed_title</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>segment_type</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>order_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a306396d9135260e58fa776b2b1e4f4754eb48bc_0</td>\n",
       "      <td>full_text</td>\n",
       "      <td>Two Sides of Meta-Learning Evaluation:\\nIn vs....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a306396d9135260e58fa776b2b1e4f4754eb48bc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4c3dca70c4b3091498f9608f7b0ccebc8854dfb5_0</td>\n",
       "      <td>full_text</td>\n",
       "      <td>LOHO: Latent Optimization of Hairstyles via Or...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4c3dca70c4b3091498f9608f7b0ccebc8854dfb5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fd51da088c5fe89eba0e363edad746bb3c2407d1_0</td>\n",
       "      <td>full_text</td>\n",
       "      <td>End-to-End Semi-Supervised Object Detection wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fd51da088c5fe89eba0e363edad746bb3c2407d1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34c736300613e68321b133aff9a1ab49b5aab1ba_0</td>\n",
       "      <td>full_text</td>\n",
       "      <td>SPEC: Seeing People in the Wild with an Estima...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34c736300613e68321b133aff9a1ab49b5aab1ba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5841743b05a5cbab84c81194620eeb0321493cd2_0</td>\n",
       "      <td>full_text</td>\n",
       "      <td>End-to-End Conversational Search for Online Sh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5841743b05a5cbab84c81194620eeb0321493cd2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   segment_id parsed_title  \\\n",
       "0  a306396d9135260e58fa776b2b1e4f4754eb48bc_0    full_text   \n",
       "1  4c3dca70c4b3091498f9608f7b0ccebc8854dfb5_0    full_text   \n",
       "2  fd51da088c5fe89eba0e363edad746bb3c2407d1_0    full_text   \n",
       "3  34c736300613e68321b133aff9a1ab49b5aab1ba_0    full_text   \n",
       "4  5841743b05a5cbab84c81194620eeb0321493cd2_0    full_text   \n",
       "\n",
       "                                            raw_text segment_type  \\\n",
       "0  Two Sides of Meta-Learning Evaluation:\\nIn vs....          NaN   \n",
       "1  LOHO: Latent Optimization of Hairstyles via Or...          NaN   \n",
       "2  End-to-End Semi-Supervised Object Detection wi...          NaN   \n",
       "3  SPEC: Seeing People in the Wild with an Estima...          NaN   \n",
       "4  End-to-End Conversational Search for Online Sh...          NaN   \n",
       "\n",
       "                                   paper_id  order_number  \n",
       "0  a306396d9135260e58fa776b2b1e4f4754eb48bc             0  \n",
       "1  4c3dca70c4b3091498f9608f7b0ccebc8854dfb5             0  \n",
       "2  fd51da088c5fe89eba0e363edad746bb3c2407d1             0  \n",
       "3  34c736300613e68321b133aff9a1ab49b5aab1ba             0  \n",
       "4  5841743b05a5cbab84c81194620eeb0321493cd2             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5009138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d32140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73831"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_ids = segments.paper_id.unique()\n",
    "len(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60fdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac8d9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f02fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5757)\n",
    "random_papers = random.sample(list(paper_ids), 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c0021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 13434/17068 [2:36:46<52:40,  1.15it/s]  "
     ]
    }
   ],
   "source": [
    "for paper_id in tqdm(random_papers):\n",
    "    try:\n",
    "        result = {'title': papers[papers.paper_id == paper_id].title.values[0], 'paper_id': paper_id}\n",
    "        cur_segments = []\n",
    "        for segment_id in  segments_papers[segments_papers.paper_id == paper_id].segment_id.values:\n",
    "            seg_id, text, title, seg_type = segments[segments.segment_id == segment_id][['segment_id', 'raw_text', 'parsed_title', 'segment_type']].values[0]\n",
    "            cur_segments.append((seg_id, text, title, seg_type))\n",
    "        result['segments'] = cur_segments\n",
    "        if len(cur_segments) > 4:\n",
    "            full_res.append(result)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6add04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmented_papers.pkl', 'wb') as f:\n",
    "    pickle.dump(full_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32ce9845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24875"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "099a0f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Category-specific object reconstruction from a single image',\n",
       " 'paper_id': '200c256c7f3253a365423b0a72efb37b0a692e4b',\n",
       " 'segments': [('200c256c7f3253a365423b0a72efb37b0a692e4b_0',\n",
       "   'Object reconstruction from a single image -in the wild -is a problem where we can make progress and get meaningful results today. This is the main message of this paper, which introduces an automated pipeline with pixels as inputs and 3D surfaces of various rigid categories as outputs in images of realistic scenes. At the core of our approach are deformable 3D models that can be learned from 2D annotations available in existing object detection datasets, that can be driven by noisy automatic object segmentations and which we complement with a bottom-up module for recovering high-frequency shape details. We perform a comprehensive quantitative analysis and ablation study of our approach using the recently introduced PASCAL 3D+ dataset and show very encouraging automatic reconstructions on PASCAL VOC.',\n",
       "   'abstract',\n",
       "   'abstract'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_1',\n",
       "   'Consider the car in Figure 1. As humans, not only can we infer at a glance that the image contains a car, we also construct a rich internal representation of it such as its location and 3D pose. Moreover, we have a guess of its 3D shape, even though we might never have have seen this particular car. We can do this because we don\\'t experience the image of this car tabula rasa, but in the context of our \"remembrance of things past\". Previously seen cars enable us to develop a notion of the 3D shape of cars, which we can project to this particular instance. We also specialize our representation to this particular instance (e.g. any custom decorations it might have), signalling that both top-down and bottom-up cues influence our percept [26].\\nA key component in such a process would be a mechanism to build 3D shape models from past visual experiences. We have developed an algorithm that can build category-specific shape models from just images with 2D annotations (segmentation masks and a small set of keypoints) present in modern computer vision datasets (e.g. * Authors contributed equally Figure 1: Automatic object reconstruction from a single image obtained by our system. Our method leverages estimated instance segmentations and predicted viewpoints to generate a full 3D mesh and high frequency 2.5D depth maps.\\nPASCAL VOC [15]). These models are then used to guide the top down 3D shape reconstruction of novel 2D car images. We complement our top-down shape inference algorithm with a bottom-up module that further refines our shape estimate for a particular instance. Finally, building upon the rapid recent progress in recognition modules [2,11,17,20,34] (object detection, segmentation and pose estimation), we demonstrate that our learnt models are robust when applied \"in the wild\" enabling fully automatic reconstructions with just images as inputs.\\nThe recent method of Vicente et al. [36] reconstructs 3D models from similar annotations as we do but it has a different focus: it aims to reconstruct a fully annotated image set while making strong assumptions about the quality of the segmentations it fits to and is hence inappropriate for reconstruction in an unconstrained setting. Our approach can work in such settings, partly because it uses explicit 3D shape models. Our work also has connections to that of Kemelmacher-Shlizerman et al. [23,32] which aims to learn morphable models for faces from 2D images, but we focus on richer shapes in unconstrained settings, at the expense of lower resolution reconstructions.\\nIn the history of computer vision, model-based object Figure 2: Overview of our training pipeline. We use an annotated image collection to estimate camera viewpoints which we then use alongwith object silhouettes to learn 3D shape models. Our learnt shape models, as illustrated in the rightmost figure are capable of deforming to capture intra-class shape variation.\\nreconstruction from a single image has reflected varying preferences on model representations. Generalized cylinders [27] resulted in very compact descriptions for certain classes of shapes, and can be used for category level descriptions, but the fitting problem for general shapes in challenging. Polyhedral models [18,40], which trace back to the early work of Roberts [29], and CAD models [25,31] provide crude approximations of shape and given a set of point correspondences can be quite effective for determining instance viewpoints. Here we pursue more expressive basis shape models [1,7,42] which establish a balance between the two extremes as they can deform but only along class-specific modes of variation. In contrast to previous work (e.g. [42]), we fit them to automatic figure-ground object segmentations.\\nOur paper is organized as follows: in Section 2 we describe our model learning pipeline where we estimate camera viewpoints for all training objects (Section 2.1) followed by our shape model formulation (Section 2.2) to learn 3D models. Section 3 describes our testing pipeline where we use our learnt models to reconstruct novel instances without assuming any annotations. We evaluate our reconstructions under various settings in Section 4 and provide sample reconstructions in the wild.',\n",
       "   'Introduction',\n",
       "   'introduction'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_2',\n",
       "   'We are interested in 3D shape models that can be robustly aligned to noisy object segmentations by incorporating top-down class-specific knowledge of how shapes from the class typically project into the image. We want to learn such models from just 2D training images, aided by ground truth segmentations and a few keypoints, similar to [36]. Our approach operates by first estimating the viewpoints of all objects in a class using a structure-from-motion approach, followed by optimizing over a deformation basis of representative 3D shapes that best explain all silhouettes, conditioned on the viewpoints. We describe these two stages of model learning in the following subsections. ',\n",
       "   'Learning Deformable 3D Models',\n",
       "   'introduction'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_3',\n",
       "   'We use the framework of NRSfM [10] to jointly estimate the camera viewpoints (rotation, translation and scale) for all training instances in each class. Originally proposed for recovering shape and deformations from video [6,33,16,10], NRSfM is a natural choice for viewpoint estimation from sparse correspondences as intra-class variation may become a confounding factor if not modeled explicitly. However, the performance of such algorithms has only been explored on simple categories, such as SUV\\'s [41] or flower petal and clown fish [28]. Closer to our work, Hejrati and Ramanan [21] used NRSfM on a larger class (cars) but need a predictive detector to fill-in missing data (occluded keypoints) which we do not assume to have here.\\nWe closely follow the EM-PPCA formulation of Torresani et al. [33] and propose a simple extension to the algorithm that incorporates silhouette information in addition to keypoint correspondences to robustly recover cameras and shape bases. Energies similar to ours have been proposed in the shape-from-silhouette literature [37] and with rigid structure-from-motion [36] but, to the best of our knowledge, not in conjunction with NRSfM. NRSfM Model. Given K keypoint correspondences per instance n ∈ {1, • • • , N }, our adaptation of the NRSfM algorithm in [33] corresponds to maximizing the likelihood of the following model:\\nP n = (I K ⊗ c n R n )S n + T n + N n S n =S + V z n z n ∼ N (0, I), N n ∼ N (0, σ 2 I) (1) subject to: R n R T n = I 2 K k=1 C mask n (p k,n ) = 0, ∀n ∈ {1, • • • , N } (2)\\nHere, P n is the 2D projection of the 3D shape S n with white noise N n and the rigid transformation given by the orthographic projection matrix R n , scale c n and 2D translation T n . The shape is parameterized as a factored Gaussian with a mean shapeS, m basis vectors\\n[V 1 , V 2 , • • • , V m ] = V and latent deformation parameters z n .\\nOur key modification is constraint (2) where C mask n denotes the Chamfer distance field of the n th instance\\'s binary mask and says that all keypoints p k,n of instance n should lie inside its binary mask. We observed that this results in more accurate viewpoints as well as more meaningful shape bases learnt from the data.\\nLearning. The likelihood of the above model is maximized using the EM algorithm. Missing data (occluded keypoints) is dealt with by \"filling-in\" the values using the forward equations after the E-step. The algorithm computes shape parameters {S, V }, rigid body transformations {c n , R n , T n } as well as the deformation parameters {z n } for each training instance n. In practice, we augment the data using horizontally mirrored images to exploit bilateral symmetry in the object classes considered. We also precompute the Chamfer distance fields for the whole set to speed up computation. As shown in Figure 3, NRSfM allows us to reliably predict viewpoint while being robust to intraclass variations. ',\n",
       "   'Viewpoint Estimation',\n",
       "   'introduction'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_4',\n",
       "   'Equipped with camera projection parameters and keypoint correspondences (lifted to 3D by NRSfM) on the whole training set, we proceed to build deformable 3D shape models from object silhouettes within a class. 3D shape reconstruction from multiple silhouettes projected from a single object in calibrated settings has been widely studied. Two prominent approaches are visual hulls [24] and variational methods derived from snakes e.g [14,30] which deform a surface mesh iteratively until convergence. Some interesting recent papers have extended variational approaches to handle categories [12,13] but typically require some form of 3D annotations to bootstrap models. A recently proposed visual-hull based approach [36] requires only 2D annotations as we do for class-based reconstruction and it was successfully demonstrated on PASCAL VOC but does not serve our purposes as it makes strong assumptions about the accuracy of the segmentation and will in fact fill entirely any segmentation with a voxel layer.\\nShape Model Formulation. We model our category shapes as deformable point clouds -one for each subcategory of the class. The underlying intuition is the following: some types of shape variation may be well explained by a parametric model e.g. a Toyota sedan and a Lexus sedan, but it is unreasonable to expect them to model the variations between sail boats and cruise liners. Such models typically require knowledge of object parts, their spatial arrangements etc. [22] and involve complicated formulations that are difficult to optimize. We instead train separate linear shape models for different subcategories of a class. As in the NRSfM model, we use a linear combination of bases to model these deformations. Note that we learn such models from silhouettes and this is what enables us to learn deformable models without relying on point correspondences between scanned 3D exemplars [8].\\nOur shape model M = (S, V ) comprises of a mean shape S and deformation bases\\nV = {V 1 , ., V K } learnt from a training set T : {(O i , P i )} N i=1\\n, where O i is the instance silhouette and P i is the projection function from world to image coordinates. Note that the P i we obtain using NRSfM corresponds to orthographic projection but our algorithm could handle perspective projection as well.\\nEnergy Formulation. We formulate our objective function primarily based on image silhouettes. For example, the shape for an instance should always project within its silhouette and should agree with the keypoints (lifted to 3D by NRSfM ). We capture these by defining corresponding energy terms as follows: (here P (S) corresponds to the 2D projection of shape S, C mask refers to the Chamfer distance field of the binary mask of silhouette O and ∆ k (p; Q) is defined as the squared average distance of point p to its k nearest neighbors in set Q) Silhouette Consistency. Silhouette consistency simply enforces the predicted shape for an instance to project inside its silhouette. This can be achieved by penalizing the points projected outside the instance mask by their distance from the silhouette. In our ∆ notation it can be written as follows:\\nE s (S, O, P ) = C mask (p)>0 ∆ 1 (p; O)(3)\\nSilhouette Coverage. Using silhouette consistency alone would just drive points projected outside in towards the silhouette. This wouldn\\'t ensure though that the object silhouette is \"filled\" -i.e. there might be overcarving. We deal with it by having an energy term that encourages points on the silhouette to pull nearby projected points towards them. Formally, this can be expressed as:\\nE c (S, O, P ) = p∈O ∆ m (p; P (S))(4)\\nKeypoint Consistency. Our NRSfM algorithm provides us with sparse 3D keypoints along with camera viewpoints.\\nWe use these sparse correspondences on the training set to deform the shape to explain these 3D points. The corresponding energy term penalizes deviation of the shape from the 3D keypoints KP for each instance. Specifically, this can be written as:\\nE kp (S, O, P ) = κ∈KP ∆ m (κ; S)(5)\\nLocal Consistency. In addition to the above data terms, we use a simple shape regularizer to restrict arbitrary deformations by imposing a quadratic deformation penalty between every point and its neighbors. We also impose a similar penalty on deformations to ensure local smoothness. The δ parameter represents the mean squared displacement between neighboring points and it encourages all faces to have similar size. Here V ki is the i th point in the k th basis.\\nE l (S, V ) = i j∈N (i) (( S i −S j − δ) 2 + k V ki − V kj 2 ) (6)\\nNormal Smoothness. Shapes occurring in the natural world tend to be locally smooth. We capture this prior on shapes by placing a cost on the variation of normal directions in a local neighborhood in the shape. Our normal smoothness energy is formulated as\\nE n (S) = i j∈N (i) (1 − N i • N j )(7)\\nHere, N i represents the normal for the i th point in shape S which is computed by fitting planes to local point neighborhoods. Our prior essentially states that local point neighborhoods should be flat. Note that this, in conjunction with our previous energies automatically enforces the commonly used prior that normals should be perpendicular to the viewing direction at the occluding contour [4]. Our total energy is given in equation 8. In addition to the above smoothness priors we also penalize the L 2 norm of the deformation parameters α i to prevent unnaturally large deformations.\\nE tot (S, V, α) = E l (S, V )+ i (E i s + E i kp + E i c + E i n + k ( α ik V k 2 F )) (8)\\nLearning. We solve the optimization problem in equation 9 to obtain our shape model M = (S, V ). The mean shape and deformation basis are inferred via block-coordinate descent on (S, V ) and α using sub-gradient computations over the training set. We restrict V k F to be a constant to address the scale ambiguity between V and α in our formulation. In order to deal with imperfect segmentations and wrongly estimated keypoints, we use truncated versions of the above energies that reduce the impact of outliers. The mean shapes learnt using our algorithm for 9 rigid categories in PASCAL VOC are shown in Figure 4. Note that in addition to representing the coarse shape details of a category, the model also learns finer structures like chair legs and bicycle handles, which become more prominent with deformations.\\nmin S,V,α E tot (S, V, α)\\nsubject to:\\nS i =S + k α ik V k (9)\\nOur training objective is highly non-convex and nonsmooth and is susceptible to initialization. We follow the suggestion of [14] and initialize our mean shape with a soft visual hull computed using all training instances. The deformation bases and deformation weights are initialized randomly.',\n",
       "   '3D Basis Shape Model Learning',\n",
       "   'introduction'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_5',\n",
       "   \"We approach object reconstruction from the big picture downward -like a sculptor first hammering out the big chunks and then chiseling out the details. After detecting and segmenting objects in the scene, we infer their coarse 3D poses and use them to fit our top-down shape models to the noisy segmentation masks. Finally, we recover high frequency shape details from shading cues. We will now explain these components one at a time.\\nInitialization. During inference, we first detect and segment the object in the image [20] and then predict viewpoint (rotation matrix) and subcategory for the object using a CNN based system similar to [34] (augmented to predict subcategories). Our learnt models are at a canonical bounding box scale -all objects are first resized to a particular width during training. Given the predicted bounding box, we scale the learnt mean shape of the predicted subcategory Figure 4: Mean shapes learnt for rigid classes in PASCAL VOC obtained using our basis shape formulation. Color encodes depth when viewed frontally. accordingly. Finally, the mean shape is rotated as per the predicted viewpoint and translated to the center of the predicted bounding box. Shape Inference. After initialization, we solve for the deformation weights α(initialized to 0) as well as all the camera projection parameters (scale, translation and rotation) by optimizing equation ( 9) for fixedS, V . Note that we do not have access to annotated keypoint locations at test time, the 'Keypoint Consistency' energy E kp is ignored during the optimization.\\nBottom-up Shape Refinement. The above optimization results in a top-down 3D reconstruction based on the category-level models, inferred object silhouette, viewpoint and our shape priors. We propose an additional processing step to recover high frequency shape information by adapting the intrinsic images algorithm of Barron and Malik [5,4], SIRFS, which exploits statistical regularities between shapes, reflectance and illumination Formally, SIRFS is formulated as the following optimization problem:\\nminimize Z,L g(I − S(Z, L)) + f (Z) + h(L)\\nwhere R = I − S(Z, L) is a log-reflectance image, Z is a depth map and L is a spherical-harmonic model of illumination. S(Z, L) is a rendering engine which produces a log shading image with the illumination L. g, f and h are the loss functions corresponding to reflectance, shape and illumination respectively.\\nWe incorporate our current coarse estimate of shape into SIRFS through an additional loss term:\\nf o (Z, Z ) = i ((Z i − Z i ) 2 + 2 ) γo\\nwhere Z is the initial coarse shape and a parameter added to make the loss differentiable everywhere. We obtain Z for an object by rendering a depth map of our fitted 3D shape model which guides the optimization of this highly non-convex cost function. The outputs from this bottom-up refinement are reflectance, shape and illumination maps of which we retain the shape.\\nImplementation Details. The gradients involved in our optimization for shape and projection parameters are extremely efficient to compute. We use approximate nearest neighbors computed using k-d tree to implement the 'Silhouette Coverage' gradients and leverage Chamfer distance fields for obtaining 'Silhouette Consistency' gradients. Our overall computation takes only about 2 sec to reconstruct a novel instance using a single CPU core. Our training pipeline is also equally efficient -taking only a few minutes to learn a shape model for a given object category.\",\n",
       "   'Reconstruction in the Wild',\n",
       "   'introduction'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_6',\n",
       "   'Experiments were performed to assess two things: 1) how expressive our learned 3D models are by evaluating how well they matched the underlying 3D shapes of the training data 2) study their sensitivity when fit to images using noisy automatic segmentations and pose predictions.\\nDatasets. For all our experiments, we consider images from the challenging PASCAL VOC 2012 dataset [15] which contain objects from the 10 rigid object categories (as listed in Table 1). We use the publicly available ground truth class-specific keypoints [9] and object segmentations [19]. Since ground truth 3D shapes are unavailable for PASCAL VOC and most other detection datasets, we evaluated the expressiveness of our learned 3D models on the next best thing we managed to obtain: the PASCAL3D+ dataset [39] which has up to 10 3D CAD models for the rigid categories in PASCAL VOC. PASCAL3D+ provides between 4 different models for \"tvmonitor\" and \"train\" and 10 for \"car\" and \"chair\". The different meshes primarily distinguish between subcategories but may also be redundant (e.g., there are more than 3 meshes for sedans in \"car\"). We obtain our subcategory labels on the training data by merging some of these cases, which also helps us in tackling data sparsity for some subcategories. The subset of PASCAL we considered after filtering occluded instances, which we do not tackle in this paper, had between 70 images for \"sofa\" and 500 images for classes \"aeroplanes\" and \"cars\". We will make all our image sets available along with our implementation.\\nMetrics. We quantify the quality of our 3D models by comparing against the PASCAL 3D+ models using two metrics   -1) the Hausdorff distance normalized by the 3D bounding box size of the ground truth model [3] and 2) a depth map error to evaluate the quality of the reconstructed visible object surface, measured as the mean absolute distance between reconstructed and ground truth depth:\\nZ-MAE(Ẑ, Z * ) = 1 n • γ min β x,y |Ẑ x,y − Z * x,y − β| (10)\\nwhereẐ and Z * represent predicted and ground truth depth maps respectively. Analytically, β can be computed as the median ofẐ −Z * and γ is a normalization factor to account for absolute object size for which we use the bounding box diagonal. Note that our depth map error is translation and scale invariant.',\n",
       "   'Experiments',\n",
       "   'experiments'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_7',\n",
       "   'We learn and fit our 3D models on the same whole dataset (no train/test split), following the setup of Vicente et al [36]. Table 1 compares our reconstructions on PASCAL VOC with those of this recently proposed method which is specialized for this task (e.g. it is not designed for fitting to noisy data), as well as to a state of the art class-agnostic shape inflation method that reconstructs also from a single silhouette. We demonstrate competitive performance on both benchmarks with our models showing greater robustnes to perspective foreshortening effects on \"trains\" and \"buses\". Category-agnostic methods -Puffball [35] and SIRFS [4] -consistently perform worse on the benchmark by themselves. Certain classes like \"boat\" and \"tvmonitor\" are especially hard because of large intraclass variance and data sparsity respectively.',\n",
       "   'Expressiveness of Learned 3D Models',\n",
       "   'experiments'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_8',\n",
       "   'In order to analyze sensitivity of our models to noisy inputs we reconstructed held-out test instances using our models given just ground truth bounding boxes. We compare various versions of our method using ground truth(Mask)/imperfect segmentations(SDS) and keypoints(KP)/our pose predictor(PP) for viewpoint estimation respectively. For pose prediction, we use the CNNbased system of [34] and augment it to predict subtypes at test time. This is achieved by training the system as described in [34] with additional subcategory labels obtained from PASCAL 3D+ as described above. To obtain an approximate segmentation from the bounding box, we use the refinement stage of the state-of-the-art joint detection and segmentation system proposed in [20].\\nHere, we use a train/test setting where our models are trained on only a subset of the data and used to reconstruct the held out data from bounding boxes. Table 2 shows that our results degrade gracefully from the fully annotated to the fully automatic setting. Our method is robust to some mis-segmentation owing to our shape model that prevents shapes from bending unnaturally to explain noisy silhouettes. Our reconstructions degrade slightly with imperfect pose initializations even though our projection parameter optimization deals with it to some extent. With predicted poses, we observe that sometimes even when our reconstructions look plausible, the errors can be high as the metrics are sensitive to bad alignment. The data sparsity issue is especially visible in the case of sofas where in a train/test setting in Table 2 the numbers drop significantly with less training data (only 34 instances). Note we do not evaluate our bottom-up component as the PASCAL 3D+ meshes provided do not share the same high frequency shape details as the instance. We will show qualitative results in the next subsection.',\n",
       "   'Sensitivity Analysis',\n",
       "   'experiments'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_9',\n",
       "   'We qualitatively demonstrate reconstructions on automatically detected and segmented instances with 0.5 IoU overlap with the ground truth in whole images in PASCAL VOC using [20] in Figure 5. We can see that our method is able to deal with some degree of mis-segmentation. Some of our major failure modes include not being able to capture the correct scale and pose of the object and thus badly fitting to the silhouette in some cases. Our subtype prediction also fails on some instances (e.g. CRT vs flat screen \"tvmonitors\") leading to incorrect reconstructions. We include more such images in the supplementary material for the reader to peruse.',\n",
       "   'Fully Automatic Reconstruction',\n",
       "   'experiments'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_10',\n",
       "   'We have proposed what may be the first approach to perform fully automatic object reconstruction from a single image on a large and realistic dataset. Critically, our deformable 3D shape model can be bootstrapped from easily acquired ground-truth 2D annotations thereby bypassing the need for a-priori manual mesh design or 3D scanning and making it possible for convenient use of these types of models on large real-world datasets (e.g. PASCAL VOC). We report an extensive evaluation of the quality of the learned 3D models on a recent 3D benchmarking dataset for PAS-CAL VOC [39] showing competitive results with models that specialize in shape reconstruction from ground truth segmentations inputs while demonstrating that our method is equally capable in the wild, on top of automatic object detectors.\\nMuch research lies ahead, both in terms of improving the quality and the robustness of reconstruction at test time (both bottom-up and top-down components), developing benchmarks for joint recognition and reconstruction and relaxing the need for annotations during training: all of these constitute interesting and important directions for future work. More expressive non-linear shape models [38] may prove helpful, as well as a tighter integration between segmentation and reconstruction. Figure 5: Fully automatic reconstructions on detected instances (0.5 IoU with ground truth) using our models on rigid categories in PASCAL VOC. We show our instance segmentation input, the inferred shape overlaid on the image, a 2.5D depth map (after the bottom-up refinement stage), the mesh in the image viewpoint and two other views. It can be seen that our method produces plausible reconstructions which is a remarkable achievement given just a single image and noisy instance segmentations. Color encodes depth in the image co-ordinate frame (blue is closer). More results can be found at http://goo.gl/lmALxQ.',\n",
       "   'Conclusion',\n",
       "   'results'),\n",
       "  ('200c256c7f3253a365423b0a72efb37b0a692e4b_11',\n",
       "   'This work was supported in part by NSF Award IIS-1212798 and ONR MURI-N00014-10-1-0933. Shubham Tulsiani was supported by the Berkeley fellowship and João Carreira was supported by the Portuguese Science Foundation, FCT, under grant SFRH/BPD/84194/2012.\\nWe gratefully acknowledge NVIDIA corporation for the donation of Tesla GPUs for this research.',\n",
       "   'Acknowledgements',\n",
       "   'acknowledgement')]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113bb445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (gosus)",
   "language": "python",
   "name": "py36gosus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
