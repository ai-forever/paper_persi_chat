{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"papers_segmented_data/davinci_dialogues_full_postproc.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'dialogue', 'meta_segments', 'meta_paper', 'parsed_dialogue'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3588"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc82b48c4c44ce89f4c212b4de661e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3588 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = list()\n",
    "summary = list()\n",
    "\n",
    "for row in tqdm(data):\n",
    "    text.append(row[\"text\"].replace(\"\\n\", \" \"))\n",
    "    summary.append(row[\"parsed_dialogue\"][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"text\": text, \"summary\": summary})\n",
    "df = df.loc[df.summary != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3204, 2), (356, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, validation = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train.shape, validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"datasets/sum_train.csv\", index=False)\n",
    "validation.to_csv(\"datasets/sum_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num tokens estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/jovyan/.cache/huggingface/datasets/csv/default-80e54394b1bd255d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f8fa4ebec04fc890a0aa737ef932e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fcb0a856874ea89140fe6c729c0d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/jovyan/.cache/huggingface/datasets/csv/default-80e54394b1bd255d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94440fcefa2d4d3393627d489e688a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_hf = load_dataset('csv', data_files={'train': 'datasets/sum_train.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name_or_path = \"sshleifer/distilbart-cnn-12-6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e63b8388ab4936bba7516e0dbeb6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1042 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "num_tokens_text = []\n",
    "num_tokens_summ = []\n",
    "\n",
    "for text, title in tqdm(zip(train_hf[\"train\"][\"text\"], train_hf[\"train\"][\"summary\"]), total=len(train_hf[\"train\"][\"summary\"])):\n",
    "    num_tokens_text.append(len(tokenizer.encode(text)))\n",
    "    num_tokens_summ.append(len(tokenizer.encode(title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(626.3027465667915, 588.0, 1242.2499999999995)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens_text), np.median(num_tokens_text), np.quantile(num_tokens_text, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60.16729088639201, 58.0, 98.84999999999991)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens_summ), np.median(num_tokens_summ), np.quantile(num_tokens_summ, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python custom_bart_scripts/run_summarization.py \\\n",
    "    --model_name_or_path=\"facebook/bart-large-cnn\" \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --evaluation_strategy=\"steps\" \\\n",
    "    --weight_decay=0.01 \\\n",
    "    --logging_steps=500 \\\n",
    "    --run_name=\"bart-large-cnn_2e-5_final\" \\\n",
    "    --train_file=\"datasets/sum_train.csv\" \\\n",
    "    --validation_file=\"datasets/sum_val.csv\" \\\n",
    "    --output_dir=\"bart_summarization/bart-large-cnn\" \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --max_target_length=228 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --num_train_epochs=3 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python custom_bart_scripts/run_summarization.py \\\n",
    "    --model_name_or_path=\"sshleifer/distilbart-cnn-12-6\" \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --evaluation_strategy=\"steps\" \\\n",
    "    --weight_decay=0.01 \\\n",
    "    --logging_steps=500 \\\n",
    "    --run_name=\"distilbart-cnn-12-6_1e-5\" \\\n",
    "    --train_file=\"datasets/sum_train.csv\" \\\n",
    "    --validation_file=\"datasets/sum_val.csv\" \\\n",
    "    --output_dir=\"bart_summarization/distilbart-cnn-12-6_1e-5\" \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --max_target_length=228 \\\n",
    "    --learning_rate=1e-5 \\\n",
    "    --num_train_epochs=3 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example():\n",
    "    sample = validation.sample()\n",
    "    text, title = sample.values[0]\n",
    "    \n",
    "    inputs = tokenizer([text], max_length=256, return_tensors=\"pt\").to(device)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "    pred = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    inputs1 = tokenizer1([text], max_length=256, return_tensors=\"pt\").to(device)\n",
    "    summary_ids1 = model1.generate(inputs1[\"input_ids\"])\n",
    "    pred1 = tokenizer1.batch_decode(summary_ids1, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    print(\"INDEX:\", sample.index[0])\n",
    "    print(text)\n",
    "    print(\"----\\n\")\n",
    "    print(\"BART-LARGE-CNN:\", pred)\n",
    "    print(\"--\" * 10)\n",
    "    print(\"DISTILBART-CNN-12-6:\", pred1)\n",
    "    print(\"--\" * 10)\n",
    "    print(\"GROUND TRUTH:\", title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.read_csv(\"datasets/sum_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There has been some work on feature optimizati...</td>\n",
       "      <td>Previous work in feature optimization for depe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To generate scale-sensitive features, we need ...</td>\n",
       "      <td>We need to find filters that are active to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We now describe the fitting process in our sys...</td>\n",
       "      <td>Our system fits a rendering network to point c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We presented global contrast based saliency co...</td>\n",
       "      <td>We have presented global contrast based salien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This paper addresses the challenging black-box...</td>\n",
       "      <td>This paper proposes a simple baseline approach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>The following definitions and notations are us...</td>\n",
       "      <td>This paper presents a minimization model with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>Our capturing method consists of sequentially ...</td>\n",
       "      <td>Our paper presents a method for capturing imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Baselines. We benchmark our talking-head model...</td>\n",
       "      <td>This paper evaluates the face redirection capa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>To evaluate the generalization performance of ...</td>\n",
       "      <td>This paper evaluates the generalization perfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Given a set of N data points x k ∈ R d×1 |k = ...</td>\n",
       "      <td>This paper examines the use of a selective mat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    There has been some work on feature optimizati...   \n",
       "1    To generate scale-sensitive features, we need ...   \n",
       "2    We now describe the fitting process in our sys...   \n",
       "3    We presented global contrast based saliency co...   \n",
       "4    This paper addresses the challenging black-box...   \n",
       "..                                                 ...   \n",
       "351  The following definitions and notations are us...   \n",
       "352  Our capturing method consists of sequentially ...   \n",
       "353  Baselines. We benchmark our talking-head model...   \n",
       "354  To evaluate the generalization performance of ...   \n",
       "355  Given a set of N data points x k ∈ R d×1 |k = ...   \n",
       "\n",
       "                                               summary  \n",
       "0    Previous work in feature optimization for depe...  \n",
       "1    We need to find filters that are active to the...  \n",
       "2    Our system fits a rendering network to point c...  \n",
       "3    We have presented global contrast based salien...  \n",
       "4    This paper proposes a simple baseline approach...  \n",
       "..                                                 ...  \n",
       "351  This paper presents a minimization model with ...  \n",
       "352  Our paper presents a method for capturing imag...  \n",
       "353  This paper evaluates the face redirection capa...  \n",
       "354  This paper evaluates the generalization perfor...  \n",
       "355  This paper examines the use of a selective mat...  \n",
       "\n",
       "[356 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name_or_path = \"bart_summarization/bart-large-cnn/checkpoint-1500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(model_name_or_path)\n",
    "model =  BartForConditionalGeneration.from_pretrained(model_name_or_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path1 = \"bart_summarization/distilbart-cnn-12-6_1e-5/checkpoint-1500\"\n",
    "\n",
    "tokenizer1 = BartTokenizer.from_pretrained(model_name_or_path1)\n",
    "model1 =  BartForConditionalGeneration.from_pretrained(model_name_or_path1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR. The two CIFAR datasets (Krizhevsky 2009) consist of colored natural images with a size of 32×32. CIFAR-10 is drawn from 10 and CIFAR-100 is drawn from 100 classes. In each dataset, the train and test sets contain 50,000 and 10,000 images, respectively. A standard data augmentation scheme 3 (Lee et al. 2015;Romero et al. 2015;Larsson, Maire, and Shakhnarovich 2016;Huang et al. 2017a;Liu et al. 2017) (Netzer et al. 2011) consists of 32×32 colored digit images, with one class for each digit. The train and test sets contain 604,388 and 26,032 images, respectively. Following previous works (Goodfellow et al. 2013;Huang et al. 2016;2017a;Liu et al. 2017), we split a subset of 6,000 images for validation, and train on the remaining images without data augmentation. ImageNet. The ILSVRC 2012 classification dataset (Deng et al. 2009) consists of 1000 classes, with a number of 1.2 million training images and 50,000 validation images. We adopt the the data augmentation scheme following (Krizhevsky, Sutskever, and Hinton 2012) and apply the same operation as (Huang et al. 2017a) at test time. We adopt several popular network architectures as our teacher model zoo, including VGGNet (Simonyan and Zisserman 2015), ResNet (He et al. 2016), DenseNet (Huang et al. 2017b), MobileNet (Howard et al. 2017), shakeshake (Gastaldi 2017), etc. For VGGNet, we use 19-layer with Batch Normalization (Ioffe and Szegedy 2015). For ResNet, we use 18-layer network for CIFAR and SVHN and 50-layer for ImagNet. For DenseNet, we use the BC structure with depth L=100, and growth rate k=24. For shakeshake, we use 26-layer 2×96d version. Note that due to the high computing costs, we use shake-shake as a teacher only when the student is shake-shake network.\n",
      "----\n",
      "\n",
      "BART-LARGE-CNN: The two CIFAR datasets (CIFAR and ImageNet) consist of 32x32 colored digit images, with one class for each digit. The ILSVRC 2012 classification dataset consists of 1000 classes, with a number of 1.2 million training images and 50,000 test images.\n",
      "--------------------\n",
      "DISTILBART-CNN-12-6: This paper discusses the CIFAR and ImageNet datasets, which consist of 32x32 colored digit images, with one class for each digit. The ILSVRC 2012 classification dataset consists of 1000 classes, with a number of 1.2 million training images and 50,000 classes.\n",
      "--------------------\n",
      "GROUND TRUTH: This paper discusses popular network architectures such as VGGNet, ResNet, DenseNet, MobileNet, and shakeshake and the datasets CIFAR-10 and CIFAR-100, and ImageNet. It also discusses the data augmentation schemes used for the different datasets.\n"
     ]
    }
   ],
   "source": [
    "generate_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We present and develop novel moments-based permutation tests where the permutation distributions are accurately approximated through Pearson distributions for considerably reduced computation cost. Comparing with regular random permutation, the proposed method considerably reduces computation cost without loss of accuracy. General and analytical formulations for the moments of permutation distribution are derived for weighted v-test statistics. The proposed strategy takes advantage of nonparametric permutation tests and parametric Pearson distribution approximation to achieve both accuracy/flexibility and efficiency. [(1,1) , (1,2), (1,2), (1,3), (2,3), (1,4)] , , , #( ) ( , ) ( , ) ( , ) ( , ) ( , ) ( , ) i j k l w w i i w i j w i j w i k w j k w i l . The permutation equivalent index subset is represented by an undirected graph. Every node denotes an index number. We connect two different nodes if these two corresponding index numbers are in the same index element, i.e., in the same small bracket. In figure 2, the number 2 on the edge ij denotes that the pair (i, j) is used twice. The self-connected node is also allowed. We assume there is no isolated subgraph in the following discussion. If any isolated subgraph exists, we only need to repeat the same procedure for all isolated subgraphs. Now we shall discuss the steps to compute the * w l = . Firstly, we get rid of the weights of edges and self-connections, i.e., , ( , ) ( , ) a i j w i i w i j w i j = . Then we search a node with the lowest degree and do summation for all indices connected with respect to the chosen node, i.e., , . The chosen nodes and connected edges are deleted after the above computation. We repeat the same step until a symmetric graph occurs. Since every node in the symmetric graph has the same degree, we randomly choose any node; for example, k for summation, then , , , ( , ) ( , ) ( , ) ( , ) ( , ) . Finally, we clear the whole graph and obtain the relaxed index partition sum. The most computational-expensive case is the complete graph in which every pair of nodes is connected. Hence, the computational cost of * c l is determined by the subtotal that has the largest symmetric subgraph in its graph representation. For example, the most expensive relaxed index partition sum for d=2 and r=3 is ( , ) ( , ) ( , ) w i j w i k w j k , which is a triangle in the graph representation. , where r is the order of moment and m is an integer. For a d-th order test statistic, the computational cost of the partition sum for the r-th moment is bounded by O(n m ). When d = 1 the computational complexity of the partition sum is O(n).\n",
      "----\n",
      "\n",
      "BART-LARGE-CNN: We present novel moments-based permutation tests where the permutation distributions are accurately approximated through Pearson distributions for considerably reduced computation cost. The proposed strategy takes advantage of nonparametric permutation testing and parametric Pearson distribution approximation to achieve both accuracy/flexibility and efficiency.\n",
      "--------------------\n",
      "DISTILBART-CNN-12-6: We present and develop novel moments-based permutation tests where the permutation distributions are accurately approximated through Pearson distributions for considerably reduced computation cost. The proposed strategy takes advantage of nonparametric permutations tests and parametric Pearson distribution approximation to achieve both accuracy/flexibility and efficiency.\n",
      "--------------------\n",
      "GROUND TRUTH: In this paper, we present and develop a novel moments-based permutation test where the permutation distributions are accurately approximated through Pearson distributions, leading to considerably reduced computation cost.\n"
     ]
    }
   ],
   "source": [
    "generate_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-chatbot-0/lib/python3.7/site-packages/transformers/generation/utils.py:1292: UserWarning: Using `max_length`'s default (142) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX: 334\n",
      "Datasets. We use 4 datasets arising from different domains: (a) Incart-ECG (Goldberger et al., 2000): Dataset of ECG timeseries from PhysioNet bank, annotated with heartbeat arrhythmias. We use one ECG lead. The task is to classify atrial (positive) vs. ventricular premature contractions (negative). Both are common arrhythmias that co-occur in   Results. Table 1 presents our main experimental results. We make the following observations: (1) Short-circuiting matters: The comparison of TLP to SWLP directly evaluates the effect of summarizing the stream by the star-mesh transform, as they are otherwise identical. As noticed in Table 1, it yields a substantial improvement in the accuracy on the temporallyordered datasets Incart-ECG, Daphnet-Gait, and CamVid-Car, with almost no effect on the running time. This corroborates the presumption that TLP is well suited for streams that adhere to a temporal vicinity structure as per Section 5.1. However, when there is no natural temporal ordering (such as with Caltech 10 -101 data), we did not observe an advantage over the other methods. (2) Small amount of labeled data suffice: Notice that we use a very small amount of labeled data in each experiment. For example, on the Incart-ECG dataset, TLP can get to a 95% classification accuracy given only two labeled examples of each type of arrhythmia. (3) Computational speedup: Notice that on the timeseries datasets, even with shingling, which increases the dimensionality of the data by a factor of shingle size, TLP takes few milliseconds per point. We remark that QLP is slower than the other methods because of the iterative loop in the k-center quantization step. In Appendix E, we present additional experiments that show how τ and labeled data size effects the performance of TLP. We also present some visualizations of our approach on the tested datasets.\n",
      "----\n",
      "\n",
      "BART-LARGE-CNN: We used 4 datasets from different domains to evaluate the effect of summarizing the stream by the star-mesh transform. We observed a substantial improvement in accuracy on the temporally-ordered datasets Incart-ECG, Daphnet-Gait, and CamVid-Car, with almost no effect on the running time. However, when there is no natural temporal ordering, we did not observe an advantage over SWLP.\n",
      "--------------------\n",
      "DISTILBART-CNN-12-6: We used 4 datasets, Incart-ECG, Daphnet-Gait, and CamVid-Car, to evaluate the effect of summarizing the stream by the star-mesh transform. We observed a substantial improvement in accuracy on the temporally-ordered datasets, but did not observe an advantage over SWLP when there was no natural temporal ordering.\n",
      "--------------------\n",
      "GROUND TRUTH: We present Table 1 which demonstrates the efficacy of a technique called TLP which is well suited for streams that adhere to a temporal vicinity structure. We are able to get 95% accuracy on the Incart-ECG dataset with only two labeled examples of each type of arrhythmia and TLP takes few milliseconds per point.\n"
     ]
    }
   ],
   "source": [
    "generate_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
