{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2994c199",
   "metadata": {},
   "source": [
    "**Note:** ChatGPT-based dialogue should constructed after Davinci dialogues since it uses trained summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1b2aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721df6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07db9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dialogue(text, summary, num_turns=3):\n",
    "    messages_bot = [\n",
    "        {\"role\": \"system\", \"content\": \"You should briefly answer the questions on the following text. If there is no answer in the given text, then you must answer that there is not enough information. Your answers should be brief. \\n\" + text},\n",
    "    ]\n",
    "    \n",
    "    messages_person = [\n",
    "        {\"role\": \"system\", \"content\": \"You should be asking short questions about an article you can't see. You only see the following summary. Your task is to ask clarifying dependent questions in order to understand the source text. You can ask only single short question at each turn. \\n\" + summary},\n",
    "    ]\n",
    "    \n",
    "    dialogue = []\n",
    "    for turn in range(num_turns):\n",
    "        \n",
    "        try:\n",
    "    \n",
    "            question = openai.ChatCompletion.create(\n",
    "              model=\"gpt-3.5-turbo\",\n",
    "              messages = messages_person,\n",
    "              temperature=0.7,\n",
    "              max_tokens=512,\n",
    "              top_p=1,\n",
    "              frequency_penalty=0,\n",
    "              presence_penalty=0\n",
    "            )\n",
    "            question = question['choices'][0]['message']\n",
    "\n",
    "            messages_person.append(question)\n",
    "            messages_bot.append({'role': 'user', 'content': question['content'].strip()})\n",
    "\n",
    "\n",
    "            response = openai.ChatCompletion.create(\n",
    "              model=\"gpt-3.5-turbo\",\n",
    "              messages = messages_bot,\n",
    "              temperature=0.7,\n",
    "              max_tokens=512,\n",
    "              top_p=1,\n",
    "              frequency_penalty=0,\n",
    "              presence_penalty=0\n",
    "            )\n",
    "\n",
    "            response = response['choices'][0]['message']\n",
    "\n",
    "            messages_bot.append(response)\n",
    "            messages_person.append({'role': 'user', 'content': response['content'].strip()})\n",
    "\n",
    "            dialogue.append((question['content'].strip(), response['content'].strip()))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if len(dialogue) >= 2:\n",
    "                return dialogue\n",
    "            \n",
    "            raise Exception('short dialogue') from e\n",
    "        \n",
    "    return dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca33fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ddbcd73",
   "metadata": {},
   "source": [
    "ChatGPT generates dialogues using summaries. Model weights can be downloaded from https://huggingface.co/ai-forever/paper_persi_chat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b2031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_scripts.summary_generation_inference import BartGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae550782",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = BartGenerator(\"paper_persi_chat/distilbart_summarizer\",\n",
    "                            device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922e927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c671279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_segments(raw_segments, max_len=4000):\n",
    "    '''\n",
    "    Join paper segments (by section_type) up to max length\n",
    "    '''\n",
    "    \n",
    "    indeces_sections = [[0,1]]\n",
    "    \n",
    "    prev_title = raw_segments[2][-1]\n",
    "    i = 2\n",
    "    cur_section = []\n",
    "    while i < len(raw_segments):\n",
    "        cur_title = raw_segments[i][-1]\n",
    "        if cur_title == prev_title:\n",
    "            cur_section.append(i)\n",
    "        else:\n",
    "            indeces_sections.append(cur_section)\n",
    "            cur_section = [i]\n",
    "            prev_title = cur_title\n",
    "        i += 1\n",
    "    \n",
    "    if len(cur_section) > 0:\n",
    "        indeces_sections.append(cur_section)\n",
    "    \n",
    "    joined_segments = []\n",
    "    for sec in indeces_sections:\n",
    "        cur_text = ''\n",
    "        cur_split = []\n",
    "        \n",
    "        for idx in sec:\n",
    "            if len(cur_text + '\\n' + raw_segments[idx][1]) < max_len or len(cur_text) == 0:\n",
    "                cur_split.append({'id': raw_segments[idx][0],\n",
    "                                  'title': raw_segments[idx][-2], 'section_type': raw_segments[idx][-1]})\n",
    "                cur_text = cur_text + '\\n' + raw_segments[idx][1]\n",
    "                cur_text = cur_text.strip()\n",
    "            \n",
    "            else:\n",
    "                joined_segments.append((cur_text, cur_split))\n",
    "                cur_text = ''\n",
    "                cur_split = []\n",
    "        \n",
    "        if len(cur_text) > 0:\n",
    "            joined_segments.append((cur_text, cur_split))\n",
    "                \n",
    "    return joined_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ff519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdba45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ee7645d",
   "metadata": {},
   "source": [
    "### Dialogue generation cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ddf682",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmented_papers.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41a95b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('davinci_dialogues.pkl', 'rb') as f:\n",
    "    davinci_dialogues = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc1af3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_dialogues = [] # or from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac5d62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_papers = set([d['meta_paper']['paper_id'] for d in davinci_dialogues]) | \\\n",
    "                     set([d['meta_paper']['paper_id'] for d in chatgpt_dialogues])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30e2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 2254/5000 [9:26:14<10:44:02, 14.07s/it]"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(15000, 20000)):\n",
    "    if data[i]['paper_id'] in processed_papers:\n",
    "        continue\n",
    "        \n",
    "    processed_papers.add(data[i]['paper_id'])\n",
    "    \n",
    "    segmented_paper = join_segments(data[i]['segments'])\n",
    "    \n",
    "    #  select random segment\n",
    "    j = random.randint(0, len(segmented_paper) - 1)\n",
    "    random_segment = segmented_paper[j]\n",
    "    \n",
    "    # skip acknowledgements\n",
    "    if random_segment[1][-1]['section_type'].startswith('acknowledgement'):\n",
    "        continue\n",
    "    \n",
    "    # filter short\n",
    "    if len(random_segment[0]) < 1500:\n",
    "        continue\n",
    "        \n",
    "    with open('logs/processed.txt', 'a') as f:\n",
    "        f.write(f'Processing {i}\\n')\n",
    "        \n",
    "    try:\n",
    "        text = random_segment[0]\n",
    "        summary = summarizer(text)[0]\n",
    "        with open('logs/processed.txt', 'a') as f:\n",
    "            f.write(f'Generating dialog for {i}...\\n')\n",
    "        result = generate_dialogue(text, summary, num_turns=4)\n",
    "    except Exception as e:\n",
    "        with open('logs/processed.txt', 'a') as f:\n",
    "            f.write(f'Error for {i}: {str(e)}\\n')\n",
    "        continue\n",
    "    \n",
    "    chatgpt_dialogues.append({\n",
    "        'text': random_segment[0],\n",
    "        'dialogue': result,\n",
    "        'meta_segments': random_segment[1],\n",
    "        'meta_paper': {'title': data[i]['title'], 'paper_id': data[i]['paper_id']},\n",
    "        'used_summary': summary,\n",
    "    })\n",
    "    \n",
    "    with open('chatgpt_dialogues.pkl', 'wb') as f:\n",
    "        pickle.dump(chatgpt_dialogues, f)\n",
    "        \n",
    "    with open('logs/processed.txt', 'a') as f:\n",
    "        f.write(f'Saved for {i}, segments {j}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fb036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c306d488",
   "metadata": {},
   "source": [
    "#### Postprocessing to the needed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8179cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dialogue in chatgpt_dialogues:\n",
    "    turns = []\n",
    "    for turn in dialogue['dialogue']:\n",
    "        turns.append({'speaker': 'person', 'text': turn[0]})\n",
    "        turns.append({'speaker': 'bot', 'text': turn[1]})\n",
    "    dialogue['parsed_dialogue'] = {\n",
    "        'summary': dialogue['used_summary'],\n",
    "        'turns': turns\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3148805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chatgpt_dialogues.pkl', 'wb') as f:\n",
    "    pickle.dump(chatgpt_dialogues, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80719401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2a83b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
