{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "import re\n",
    "import numpy as np\n",
    "from chat_scripts.deberta_qa import DebertaQA\n",
    "import pandas as pd\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "#from datasets import load_dataset, load_metric\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data construction\n",
    "\n",
    "Gold answers are constructed using sentence_bleu scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class for preproc dialogue data generated by OpenAI API\n",
    "'''\n",
    "class MyCorpus():\n",
    "    def __init__(self, min_tresh=0.05, norm_tresh=0.75, deberta_max_tokens=500, deberta_min_tokens_diff=200, deberta_sents_around=0): # constructor\n",
    "        # hyper-params\n",
    "        self.windows_shift = 2\n",
    "        self.min_tresh = min_tresh\n",
    "        self.norm_tresh = norm_tresh\n",
    "        self.deberta_max_tokens = deberta_max_tokens\n",
    "        self.deberta_min_tokens_diff = deberta_min_tokens_diff\n",
    "        self.deberta_sents_around = deberta_sents_around\n",
    "       \n",
    "        self.splitter = SentenceSplitter(language='en')\n",
    "        #self.deberta_qa = DebertaQA(device='cuda')\n",
    "        \n",
    "    def reinit(self, text):\n",
    "        self.text = re.compile(r\"\\s+\").sub(\" \", text)\n",
    "        self.sents_ = self.splitter.split(text)\n",
    "        self.sents = []  \n",
    "        self.words = []  \n",
    "        self.bleu_windows = []\n",
    "        self.groundings = []\n",
    "        self.groundings_detailed = []\n",
    "        self.bleu_scores_debug = []\n",
    "        self.deberta_grounding = None\n",
    "        self.deberta_span = None\n",
    "        \n",
    "        self.groundings_none = []  # answers with no grounding: 'yes, this is correct', bleu=0\n",
    "        self.groundings_not_found = []  # answers with bleu below tresholds\n",
    "        \n",
    "        self._process_corpus()\n",
    "    \n",
    "    def _process_corpus(self):\n",
    "        offset = 0\n",
    "        word_id_max = 0\n",
    "        \n",
    "        for sent_id, sent in enumerate(self.sents_):\n",
    "            sent_start = self.text.find(sent, offset)\n",
    "            sent_end = sent_start + len(sent)\n",
    "            \n",
    "            if sent_start==-1:\n",
    "                print(f'sent_id={sent_id}, sent=|{sent}|, offset={offset}')\n",
    "                return -1\n",
    "            \n",
    "            assert self.text[sent_start:sent_end] == sent, f'Error:{sent_id}, {sent_start}, {sent_end}, {offset}'\n",
    "            #print(f'Found #{sent_id}, {sent_start}, {sent_end}, {offset}')\n",
    "            \n",
    "            offset = sent_end\n",
    "            \n",
    "            self.sents.append((sent_id, sent, sent_start, sent_end))\n",
    "            \n",
    "            sent_split = sent.split()\n",
    "            curr_words = [(wid + word_id_max, w, sent_id) for wid, w in enumerate(sent_split)]\n",
    "            self.words.extend(curr_words)\n",
    "            word_id_max += len(sent_split)\n",
    "            \n",
    "    def find_grounding(self, answer, answer_id):\n",
    "        '''\n",
    "        detect_sents: defines how grounding candidates convert to sentences\n",
    "            'intersect' : find sents intersection with grouding\n",
    "            'window_num_sents' : get number of grounding sents ~ number of sents in input window (answer)\n",
    "        '''\n",
    "        self.bleu_windows = []\n",
    "        answer_tokens = answer.split()\n",
    "        window_len = len(answer_tokens)\n",
    "        window_num_sents = len(self.splitter.split(answer))\n",
    "        max_score = -1\n",
    "        \n",
    "        for i in range(0, len(self.words)-window_len, self.windows_shift):\n",
    "            # print(i)\n",
    "            win_start, win_end = i, i + window_len\n",
    "            win_tokens = [w[1] for w in self.words[win_start:win_end]]\n",
    "            win_text = ' '.join(win_tokens)\n",
    "            bleu_score = sentence_bleu([win_tokens], answer_tokens, weights=(0.5, 0.5, 0., 0.))\n",
    "            bleu_score_filtered = -1.0 if bleu_score <= self.min_tresh else bleu_score # apply min theshold\n",
    "            \n",
    "            if bleu_score > max_score:\n",
    "                max_score = bleu_score\n",
    "            \n",
    "            # bleu_scores - windows: [win_start, win_end, score, score_filtered, score_norm, answer, window_text]\n",
    "            self.bleu_windows.append([win_start, win_end, bleu_score, bleu_score_filtered, None, answer, win_text])\n",
    "        \n",
    "       \n",
    "        #assert max_score != 0, f'max bleu score is equial {max_score}!' \n",
    "        if max_score == 0:\n",
    "            self.groundings_none.append(answer_id)\n",
    "            return\n",
    "        \n",
    "        # normalize scores\n",
    "        for w in self.bleu_windows:\n",
    "            w[4] = w[3]/max_score if w[3]>0 else -1.0\n",
    "            \n",
    "        # apply max norm treshold\n",
    "        self.grounding_candidates = [w for w in self.bleu_windows if w[4] >= self.norm_tresh]\n",
    "        \n",
    "        if not self.grounding_candidates:\n",
    "            # print(f'No grounding candidates found')\n",
    "            self.groundings_not_found.append(answer_id)\n",
    "            return\n",
    "\n",
    "        # Convert grounding candidates to output sentences\n",
    "        grounding_sents_= []\n",
    "\n",
    "        for g in self.grounding_candidates:\n",
    "            sent_start = self.words[g[0]][2]\n",
    "            sent_end = self.words[g[1]][2] # each is (wid, w, sentid)\n",
    "            grounding_sents_.append([sent_start, sent_end, g[4]])\n",
    "        \n",
    "        grounding_sents_.sort(key=lambda x: x[2], reverse=True)\n",
    "        self.debug_grounding_sents = grounding_sents_\n",
    "        \n",
    "        # Get number of grounding sents ~ number of sents in input window (answer)\n",
    "        gss = set()\n",
    "        \n",
    "        \n",
    "        for gs in grounding_sents_:\n",
    "            start_sent, end_sent = gs[0], gs[1]\n",
    "            gss.update(list(range(gs[0], gs[1]+1)))  # add all sent from range [start, end] (not pythonic, all included)\n",
    "            \n",
    "            if len(gss) > window_num_sents:\n",
    "                break\n",
    "                \n",
    "        # set of continuous groundings \n",
    "        self.groundings = self.split_to_intervals(sorted(gss))\n",
    "        \n",
    "        for g in self.groundings:\n",
    "            self.groundings_detailed.append([self.sents[sid] for sid in g])\n",
    "         \n",
    "    \n",
    "    def deberta_extract_grounding(self, question):\n",
    "        # Extract DEBERTA grounding\n",
    "        grounding, span = self.deberta_qa.extract_grounding(question=question, \n",
    "                                                            context=self.text,\n",
    "                                                            max_tokens=self.deberta_max_tokens, \n",
    "                                                            min_tokens_diff=self.deberta_min_tokens_diff, \n",
    "                                                            return_response_span=True, \n",
    "                                                            sents_around=self.deberta_sents_around)\n",
    "        \n",
    "        self.deberta_grounding = grounding \n",
    "        self.deberta_span = span\n",
    "        \n",
    "    @staticmethod\n",
    "    def split_to_intervals(s):\n",
    "        '''\n",
    "        Utility function for splitting set of integers (grounding sents ids) into intervals of consecutive numbers (id)\n",
    "        '''\n",
    "        ds = np.array(s[1:]) - np.array(s[:-1])\n",
    "        grounding_starts = [0] + [i+1 for i in list(np.where(ds>1)[0])]\n",
    "\n",
    "        grounding_intervals = [] # indexes\n",
    "        for idx in range(len(grounding_starts)-1):\n",
    "            grounding_intervals.append((\n",
    "                 grounding_starts[idx],\n",
    "                 grounding_starts[idx+1]\n",
    "            ))\n",
    "        grounding_intervals.append((grounding_starts[-1], len(s)))  # last interval\n",
    "\n",
    "        # print(f's={s}\\n grounding_starts: {grounding_starts}, grounding_intervals: {grounding_intervals}')\n",
    "\n",
    "        return [s[gi[0]:gi[1]] for gi in grounding_intervals]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(data):\n",
    "    '''\n",
    "    For Davinchi only!\n",
    "    '''\n",
    "    bad = []\n",
    "\n",
    "    for id_, i in enumerate(data):\n",
    "        ts = i['parsed_dialogue']['turns']\n",
    "        if len(ts)%2==1:\n",
    "            bad.append(id_)\n",
    "            continue\n",
    "\n",
    "        t1 = [t['speaker'] for t in ts[::2]] # person\n",
    "        t2 = [t['speaker'] for t in ts[1::2]] # bot\n",
    "\n",
    "        if set(t1) != {'person'} or set(t2) != {'bot'}:\n",
    "            bad.append(id_)\n",
    "\n",
    "    data_clean = [data[i] for i in range(len(data)) if i not in bad]\n",
    "    print(f'Total: {len(data)}, bad: {len(bad)}, clean: {len(data_clean)}')\n",
    "    \n",
    "    return data_clean\n",
    "\n",
    "\n",
    "\n",
    "def generate_squad_dataset(data, params):\n",
    "    '''\n",
    "    generates GT for bot answers and returns list of dict:\n",
    "    \n",
    "        template = {\n",
    "            'id': None, \n",
    "            'title': None,\n",
    "            'context': None,\n",
    "            'question': None,\n",
    "            'answers': {\n",
    "                'text': [], \n",
    "                'answer_start': []\n",
    "            }\n",
    "        }\n",
    "    '''\n",
    "    \n",
    "    mc = MyCorpus(**params)\n",
    "    res=[]\n",
    "    \n",
    "    for item_id, item in tqdm(enumerate(data)):\n",
    "        #print(f'start with {item_id}')\n",
    "        mc.reinit(item['text'])\n",
    "        \n",
    "        d_id = str(item['meta_segments'][0]['id'])\n",
    "        d_title = str(item['meta_segments'][0]['title'])\n",
    "        d_text = mc.text\n",
    "\n",
    "        ts = item['parsed_dialogue']['turns']\n",
    " \n",
    "        for person_turn in range(0, len(ts), 2):\n",
    "            mc.reinit(item['text'])\n",
    "            tmp = {}        \n",
    "            qa_id = f'{person_turn}_{person_turn+1}'\n",
    "            q = ts[person_turn]['text']\n",
    "            a = ts[person_turn+1]['text']\n",
    "\n",
    "            mc.find_grounding(a, f'{item_id}_{qa_id}')  # by bleu\n",
    "\n",
    "            tmp['id'] = f'{d_id}_{qa_id}'\n",
    "            tmp['title'] = d_title\n",
    "            tmp['context'] = d_text\n",
    "            tmp['question'] = q # person\n",
    "            tmp['chat_gpt_answer'] = a # GT\n",
    "            \n",
    "            # tmp['answers'] - dict from grounding \n",
    "            answers_dict = {'text': [], 'answer_start': []}\n",
    "            \n",
    "            for g in mc.groundings_detailed:\n",
    "                answers_dict['text'].append(' '.join([s[1] for s in g]))  # joined sents\n",
    "                answers_dict['answer_start'].append(g[0][2])  # start of 1st sent\n",
    "            \n",
    "            tmp['answers'] = answers_dict\n",
    "            \n",
    "            res.append(tmp)\n",
    "            \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../_common/papers_segmented_data/davinci_dialogues_full_postproc.pkl', 'rb') as f:\n",
    "    davinchi_data_gross = pickle.load(f)\n",
    "\n",
    "with open('../_common/papers_segmented_data/chatgpt_dialogues_full_postproc_upd.pkl', 'rb') as f:\n",
    "    chatgpt_data = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 3588, bad: 771, clean: 2817\n"
     ]
    }
   ],
   "source": [
    "davinchi_data = clean_dataset(davinchi_data_gross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from grid search\n",
    "best_params = {'min_tresh': 0.15,\n",
    "               'norm_tresh': 0.8,\n",
    "               'deberta_max_tokens': 500,\n",
    "               'deberta_min_tokens_diff': 200,\n",
    "               'deberta_sents_around': 1}\n",
    "\n",
    "valid_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11204, 400)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = davinchi_data + chatgpt_data\n",
    "shuffle(dataset)\n",
    "train, valid = dataset[:-valid_size], dataset[-valid_size:]\n",
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/user/conda/envs/deberta_retrain/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/user/conda/envs/deberta_retrain/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/user/conda/envs/deberta_retrain/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "11204it [35:35,  5.25it/s]\n"
     ]
    }
   ],
   "source": [
    "train_squad = generate_squad_dataset(train, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../_common/datasets/deberta_retrain/squad_format_train_upd.pkl', 'wb') as f:\n",
    "    pickle.dump(train_squad, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [01:19,  5.03it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_squad = generate_squad_dataset(valid, best_params)\n",
    "\n",
    "with open('../_common/datasets/deberta_retrain/squad_format_valid_upd.pkl', 'wb') as f:\n",
    "    pickle.dump(valid_squad, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need json lines format for hugging face Dataset loader!\n",
    "import json\n",
    "\n",
    "with open('datasets/deberta_retrain/squad_format_train.json', 'w') as outfile:\n",
    "    for entry in train_squad:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "with open('datasets/deberta_retrain/squad_format_valid.json', 'w') as outfile:\n",
    "    for entry in valid_squad:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with hard negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_train_withhn_filt_upd.pkl', 'rb') as f:\n",
    "    train_with_hard = pickle.load(f)\n",
    "\n",
    "with open('/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_valid_withhn_filt_upd.pkl', 'rb') as f:\n",
    "    valid_with_hard = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57306, 2039)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_with_hard), len(valid_with_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix None in titles and key error: answers_start -> answer_start\n",
    "def fix_entry(entry):\n",
    "    entry['title'] = '' if type(entry['title']) != str else entry['title']\n",
    "    if 'answers_start' in entry['answers']:\n",
    "        entry['answers']['answer_start'] = entry['answers']['answers_start']\n",
    "        del entry['answers']['answers_start']\n",
    "        \n",
    "    return entry\n",
    "    \n",
    "\n",
    "with open('/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_train_withhn_filt_upd.json', 'w') as outfile:\n",
    "    for entry in train_with_hard:\n",
    "        json.dump(fix_entry(entry), outfile)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "with open('/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_valid_withhn_filt_upd.json', 'w') as outfile:\n",
    "    for entry in valid_with_hard:\n",
    "        json.dump(fix_entry(entry), outfile)\n",
    "        outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deberta train\n",
    "https://github.com/huggingface/transformers/tree/v4.26.0/examples/pytorch/question-answering\n",
    "\n",
    "model: deepset/deberta-v3-base-squad2\n",
    "\n",
    "Note that if your dataset contains samples with no possible answers (like SQuAD version 2), you need to pass along the flag --version_2_with_negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/deberta_retrain\n",
      "/home/user/conda/envs/deberta_retrain\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "04/24/2023 18:35:12 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/24/2023 18:35:12 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/runs/Apr24_18-35-11_chatbot-0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use,\n",
      "save_on_each_node=False,\n",
      "save_steps=20000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "========================\n",
      " hit! extension=json, extension={'train': '/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_train_withhn_filt_upd_use.json', 'validation': '/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_valid_withhn_filt_upd_use.json'} \n",
      " ========================\n",
      "04/24/2023 18:35:12 - INFO - datasets.builder - Using custom data configuration default-9cc1d0e13347b8e4\n",
      "04/24/2023 18:35:12 - INFO - datasets.info - Loading Dataset Infos from /home/user/conda/envs/deberta_retrain/lib/python3.8/site-packages/datasets/packaged_modules/json\n",
      "04/24/2023 18:35:12 - INFO - datasets.builder - Generating dataset json (/home/jovyan/.cache/huggingface/datasets/json/default-9cc1d0e13347b8e4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Downloading and preparing dataset json/default to /home/jovyan/.cache/huggingface/datasets/json/default-9cc1d0e13347b8e4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 15335.66it/s]\n",
      "04/24/2023 18:35:12 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "04/24/2023 18:35:12 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|█████████████████████| 2/2 [00:00<00:00, 597.48it/s]\n",
      "04/24/2023 18:35:12 - INFO - datasets.builder - Generating train split\n",
      "04/24/2023 18:35:25 - INFO - datasets.builder - Generating validation split\n",
      "04/24/2023 18:35:41 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /home/jovyan/.cache/huggingface/datasets/json/default-9cc1d0e13347b8e4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 26.87it/s]\n",
      "[INFO|configuration_utils.py:660] 2023-04-24 18:35:42,042 >> loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--deberta-v3-base-squad2/snapshots/937a9e842670a7d23f8dfd4be71bbb62d5a5636a/config.json\n",
      "[INFO|configuration_utils.py:712] 2023-04-24 18:35:42,046 >> Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"deepset/deberta-v3-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"name\": \"DebertaV2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-24 18:35:42,277 >> loading file spm.model from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--deberta-v3-base-squad2/snapshots/937a9e842670a7d23f8dfd4be71bbb62d5a5636a/spm.model\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-24 18:35:42,278 >> loading file tokenizer.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--deberta-v3-base-squad2/snapshots/937a9e842670a7d23f8dfd4be71bbb62d5a5636a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-24 18:35:42,278 >> loading file added_tokens.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--deberta-v3-base-squad2/snapshots/937a9e842670a7d23f8dfd4be71bbb62d5a5636a/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-24 18:35:42,278 >> loading file special_tokens_map.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--deberta-v3-base-squad2/snapshots/937a9e842670a7d23f8dfd4be71bbb62d5a5636a/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-24 18:35:42,278 >> loading file tokenizer_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--deberta-v3-base-squad2/snapshots/937a9e842670a7d23f8dfd4be71bbb62d5a5636a/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2275] 2023-04-24 18:35:42,488 >> loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--deberta-v3-base-squad2/snapshots/937a9e842670a7d23f8dfd4be71bbb62d5a5636a/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2857] 2023-04-24 18:35:44,186 >> All model checkpoint weights were used when initializing DebertaV2ForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:2865] 2023-04-24 18:35:44,186 >> All the weights of DebertaV2ForQuestionAnswering were initialized from the model checkpoint at deepset/deberta-v3-base-squad2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset:   0%|      | 0/57306 [00:00<?, ? examples/s]04/24/2023 18:35:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jovyan/.cache/huggingface/datasets/json/default-9cc1d0e13347b8e4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e1a2c8f4247762c6.arrow\n",
      "Running tokenizer on validation dataset:   0%|  | 0/2039 [00:00<?, ? examples/s]04/24/2023 18:36:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jovyan/.cache/huggingface/datasets/json/default-9cc1d0e13347b8e4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-05af7f3dbf197559.arrow\n",
      "/home/user/conda/envs/deberta_retrain/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1650] 2023-04-24 18:37:09,964 >> ***** Running training *****\n",
      "[INFO|trainer.py:1651] 2023-04-24 18:37:09,964 >>   Num examples = 164635\n",
      "[INFO|trainer.py:1652] 2023-04-24 18:37:09,964 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:1653] 2023-04-24 18:37:09,964 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1654] 2023-04-24 18:37:09,964 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1655] 2023-04-24 18:37:09,964 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1656] 2023-04-24 18:37:09,964 >>   Total optimization steps = 82316\n",
      "[INFO|trainer.py:1657] 2023-04-24 18:37:09,964 >>   Number of trainable parameters = 183833090\n",
      "{'loss': 0.9457, 'learning_rate': 1.98785169347393e-05, 'epoch': 0.02}          \n",
      "{'loss': 0.7969, 'learning_rate': 1.9757033869478596e-05, 'epoch': 0.05}        \n",
      "{'loss': 0.7755, 'learning_rate': 1.9635550804217894e-05, 'epoch': 0.07}        \n",
      "{'loss': 0.7533, 'learning_rate': 1.951406773895719e-05, 'epoch': 0.1}          \n",
      "{'loss': 0.7438, 'learning_rate': 1.939258467369649e-05, 'epoch': 0.12}         \n",
      "{'loss': 0.6857, 'learning_rate': 1.9271101608435786e-05, 'epoch': 0.15}        \n",
      "{'loss': 0.7479, 'learning_rate': 1.9149618543175083e-05, 'epoch': 0.17}        \n",
      "{'loss': 0.7317, 'learning_rate': 1.902813547791438e-05, 'epoch': 0.19}         \n",
      "{'loss': 0.6994, 'learning_rate': 1.8906652412653678e-05, 'epoch': 0.22}        \n",
      "{'loss': 0.6817, 'learning_rate': 1.8785169347392975e-05, 'epoch': 0.24}        \n",
      "{'loss': 0.6674, 'learning_rate': 1.8663686282132272e-05, 'epoch': 0.27}        \n",
      "{'loss': 0.7167, 'learning_rate': 1.854220321687157e-05, 'epoch': 0.29}         \n",
      "{'loss': 0.6899, 'learning_rate': 1.8420720151610867e-05, 'epoch': 0.32}        \n",
      "{'loss': 0.6693, 'learning_rate': 1.8299237086350164e-05, 'epoch': 0.34}        \n",
      "{'loss': 0.6308, 'learning_rate': 1.8177754021089462e-05, 'epoch': 0.36}        \n",
      "{'loss': 0.6717, 'learning_rate': 1.805627095582876e-05, 'epoch': 0.39}         \n",
      "{'loss': 0.6845, 'learning_rate': 1.7934787890568056e-05, 'epoch': 0.41}        \n",
      "{'loss': 0.6878, 'learning_rate': 1.7813304825307354e-05, 'epoch': 0.44}        \n",
      "{'loss': 0.658, 'learning_rate': 1.769182176004665e-05, 'epoch': 0.46}          \n",
      "{'loss': 0.6409, 'learning_rate': 1.757033869478595e-05, 'epoch': 0.49}         \n",
      "{'loss': 0.6857, 'learning_rate': 1.7448855629525246e-05, 'epoch': 0.51}        \n",
      "{'loss': 0.6836, 'learning_rate': 1.7327372564264543e-05, 'epoch': 0.53}        \n",
      "{'loss': 0.652, 'learning_rate': 1.720588949900384e-05, 'epoch': 0.56}          \n",
      "{'loss': 0.6621, 'learning_rate': 1.7084406433743138e-05, 'epoch': 0.58}        \n",
      "{'loss': 0.6648, 'learning_rate': 1.6962923368482435e-05, 'epoch': 0.61}        \n",
      "{'loss': 0.668, 'learning_rate': 1.6841440303221732e-05, 'epoch': 0.63}         \n",
      "{'loss': 0.6511, 'learning_rate': 1.671995723796103e-05, 'epoch': 0.66}         \n",
      "{'loss': 0.6655, 'learning_rate': 1.6598474172700327e-05, 'epoch': 0.68}        \n",
      "{'loss': 0.6318, 'learning_rate': 1.6476991107439624e-05, 'epoch': 0.7}         \n",
      "{'loss': 0.6435, 'learning_rate': 1.6355508042178922e-05, 'epoch': 0.73}        \n",
      "{'loss': 0.6386, 'learning_rate': 1.623402497691822e-05, 'epoch': 0.75}         \n",
      "{'loss': 0.6569, 'learning_rate': 1.6112541911657517e-05, 'epoch': 0.78}        \n",
      "{'loss': 0.642, 'learning_rate': 1.5991058846396814e-05, 'epoch': 0.8}          \n",
      "{'loss': 0.6249, 'learning_rate': 1.586957578113611e-05, 'epoch': 0.83}         \n",
      "{'loss': 0.6252, 'learning_rate': 1.574809271587541e-05, 'epoch': 0.85}         \n",
      "{'loss': 0.6463, 'learning_rate': 1.5626609650614706e-05, 'epoch': 0.87}        \n",
      "{'loss': 0.6325, 'learning_rate': 1.5505126585354003e-05, 'epoch': 0.9}         \n",
      "{'loss': 0.6367, 'learning_rate': 1.53836435200933e-05, 'epoch': 0.92}          \n",
      "{'loss': 0.6507, 'learning_rate': 1.5262160454832598e-05, 'epoch': 0.95}        \n",
      "{'loss': 0.6296, 'learning_rate': 1.5140677389571894e-05, 'epoch': 0.97}        \n",
      " 24%|████████                         | 20000/82316 [1:12:46<3:47:22,  4.57it/s][INFO|trainer.py:2709] 2023-04-24 19:49:56,236 >> Saving model checkpoint to /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-20000\n",
      "[INFO|configuration_utils.py:453] 2023-04-24 19:49:56,241 >> Configuration saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1704] 2023-04-24 19:49:57,652 >> Model weights saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2160] 2023-04-24 19:49:57,655 >> tokenizer config file saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2167] 2023-04-24 19:49:57,657 >> Special tokens file saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-20000/special_tokens_map.json\n",
      "{'loss': 0.638, 'learning_rate': 1.5019194324311193e-05, 'epoch': 1.0}          \n",
      "{'loss': 0.5864, 'learning_rate': 1.489771125905049e-05, 'epoch': 1.02}         \n",
      "{'loss': 0.5791, 'learning_rate': 1.4776228193789787e-05, 'epoch': 1.04}        \n",
      "{'loss': 0.5834, 'learning_rate': 1.4654745128529085e-05, 'epoch': 1.07}        \n",
      "{'loss': 0.5762, 'learning_rate': 1.453326206326838e-05, 'epoch': 1.09}         \n",
      "{'loss': 0.5723, 'learning_rate': 1.441177899800768e-05, 'epoch': 1.12}         \n",
      "{'loss': 0.5531, 'learning_rate': 1.4290295932746977e-05, 'epoch': 1.14}        \n",
      "{'loss': 0.5675, 'learning_rate': 1.4168812867486274e-05, 'epoch': 1.17}        \n",
      "{'loss': 0.5572, 'learning_rate': 1.4047329802225571e-05, 'epoch': 1.19}        \n",
      "{'loss': 0.539, 'learning_rate': 1.3925846736964867e-05, 'epoch': 1.21}         \n",
      "{'loss': 0.5241, 'learning_rate': 1.3804363671704166e-05, 'epoch': 1.24}        \n",
      "{'loss': 0.5782, 'learning_rate': 1.3682880606443463e-05, 'epoch': 1.26}        \n",
      "{'loss': 0.5539, 'learning_rate': 1.3561397541182759e-05, 'epoch': 1.29}        \n",
      "{'loss': 0.5666, 'learning_rate': 1.3439914475922058e-05, 'epoch': 1.31}        \n",
      "{'loss': 0.5388, 'learning_rate': 1.3318431410661354e-05, 'epoch': 1.34}        \n",
      "{'loss': 0.5345, 'learning_rate': 1.3196948345400653e-05, 'epoch': 1.36}        \n",
      "{'loss': 0.5852, 'learning_rate': 1.307546528013995e-05, 'epoch': 1.38}         \n",
      "{'loss': 0.5925, 'learning_rate': 1.2953982214879246e-05, 'epoch': 1.41}        \n",
      "{'loss': 0.5393, 'learning_rate': 1.2832499149618545e-05, 'epoch': 1.43}        \n",
      "{'loss': 0.5698, 'learning_rate': 1.271101608435784e-05, 'epoch': 1.46}         \n",
      "{'loss': 0.5239, 'learning_rate': 1.258953301909714e-05, 'epoch': 1.48}         \n",
      "{'loss': 0.5465, 'learning_rate': 1.2468049953836437e-05, 'epoch': 1.51}        \n",
      "{'loss': 0.5276, 'learning_rate': 1.2346566888575732e-05, 'epoch': 1.53}        \n",
      "{'loss': 0.5321, 'learning_rate': 1.2225083823315031e-05, 'epoch': 1.55}        \n",
      "{'loss': 0.5437, 'learning_rate': 1.2103600758054329e-05, 'epoch': 1.58}        \n",
      "{'loss': 0.5548, 'learning_rate': 1.1982117692793624e-05, 'epoch': 1.6}         \n",
      "{'loss': 0.5568, 'learning_rate': 1.1860634627532923e-05, 'epoch': 1.63}        \n",
      "{'loss': 0.5394, 'learning_rate': 1.1739151562272219e-05, 'epoch': 1.65}        \n",
      "{'loss': 0.5293, 'learning_rate': 1.1617668497011518e-05, 'epoch': 1.68}        \n",
      "{'loss': 0.5389, 'learning_rate': 1.1496185431750815e-05, 'epoch': 1.7}         \n",
      "{'loss': 0.5726, 'learning_rate': 1.1374702366490111e-05, 'epoch': 1.73}        \n",
      "{'loss': 0.547, 'learning_rate': 1.125321930122941e-05, 'epoch': 1.75}          \n",
      "{'loss': 0.5362, 'learning_rate': 1.1131736235968706e-05, 'epoch': 1.77}        \n",
      "{'loss': 0.566, 'learning_rate': 1.1010253170708005e-05, 'epoch': 1.8}          \n",
      "{'loss': 0.5813, 'learning_rate': 1.0888770105447302e-05, 'epoch': 1.82}        \n",
      "{'loss': 0.5433, 'learning_rate': 1.0767287040186598e-05, 'epoch': 1.85}        \n",
      "{'loss': 0.5173, 'learning_rate': 1.0645803974925897e-05, 'epoch': 1.87}        \n",
      "{'loss': 0.5876, 'learning_rate': 1.0524320909665192e-05, 'epoch': 1.9}         \n",
      "{'loss': 0.5587, 'learning_rate': 1.0402837844404491e-05, 'epoch': 1.92}        \n",
      "{'loss': 0.5178, 'learning_rate': 1.0281354779143789e-05, 'epoch': 1.94}        \n",
      " 49%|████████████████                 | 40000/82316 [2:25:44<2:36:22,  4.51it/s][INFO|trainer.py:2709] 2023-04-24 21:02:54,277 >> Saving model checkpoint to /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-40000\n",
      "[INFO|configuration_utils.py:453] 2023-04-24 21:02:54,284 >> Configuration saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-40000/config.json\n",
      "[INFO|modeling_utils.py:1704] 2023-04-24 21:02:55,816 >> Model weights saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-40000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2160] 2023-04-24 21:02:55,820 >> tokenizer config file saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-40000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2167] 2023-04-24 21:02:55,821 >> Special tokens file saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-40000/special_tokens_map.json\n",
      "{'loss': 0.5525, 'learning_rate': 1.0159871713883084e-05, 'epoch': 1.97}        \n",
      "{'loss': 0.5544, 'learning_rate': 1.0038388648622384e-05, 'epoch': 1.99}        \n",
      "{'loss': 0.5044, 'learning_rate': 9.916905583361681e-06, 'epoch': 2.02}         \n",
      "{'loss': 0.4586, 'learning_rate': 9.795422518100978e-06, 'epoch': 2.04}         \n",
      "{'loss': 0.4464, 'learning_rate': 9.673939452840276e-06, 'epoch': 2.07}         \n",
      "{'loss': 0.4299, 'learning_rate': 9.552456387579571e-06, 'epoch': 2.09}         \n",
      "{'loss': 0.4654, 'learning_rate': 9.430973322318869e-06, 'epoch': 2.11}         \n",
      "{'loss': 0.4273, 'learning_rate': 9.309490257058168e-06, 'epoch': 2.14}         \n",
      "{'loss': 0.443, 'learning_rate': 9.188007191797465e-06, 'epoch': 2.16}          \n",
      "{'loss': 0.4421, 'learning_rate': 9.066524126536762e-06, 'epoch': 2.19}         \n",
      "{'loss': 0.4392, 'learning_rate': 8.945041061276058e-06, 'epoch': 2.21}         \n",
      "{'loss': 0.4511, 'learning_rate': 8.823557996015357e-06, 'epoch': 2.24}         \n",
      "{'loss': 0.4836, 'learning_rate': 8.702074930754654e-06, 'epoch': 2.26}         \n",
      "{'loss': 0.448, 'learning_rate': 8.580591865493952e-06, 'epoch': 2.28}          \n",
      "{'loss': 0.4528, 'learning_rate': 8.459108800233247e-06, 'epoch': 2.31}         \n",
      "{'loss': 0.444, 'learning_rate': 8.337625734972545e-06, 'epoch': 2.33}          \n",
      "{'loss': 0.4637, 'learning_rate': 8.216142669711844e-06, 'epoch': 2.36}         \n",
      "{'loss': 0.4334, 'learning_rate': 8.094659604451141e-06, 'epoch': 2.38}         \n",
      "{'loss': 0.4514, 'learning_rate': 7.973176539190437e-06, 'epoch': 2.41}         \n",
      "{'loss': 0.4448, 'learning_rate': 7.851693473929734e-06, 'epoch': 2.43}         \n",
      "{'loss': 0.4368, 'learning_rate': 7.730210408669033e-06, 'epoch': 2.45}         \n",
      "{'loss': 0.439, 'learning_rate': 7.6087273434083295e-06, 'epoch': 2.48}         \n",
      "{'loss': 0.4424, 'learning_rate': 7.487244278147628e-06, 'epoch': 2.5}          \n",
      "{'loss': 0.4579, 'learning_rate': 7.365761212886924e-06, 'epoch': 2.53}         \n",
      "{'loss': 0.469, 'learning_rate': 7.2442781476262215e-06, 'epoch': 2.55}         \n",
      "{'loss': 0.4235, 'learning_rate': 7.122795082365519e-06, 'epoch': 2.58}         \n",
      "{'loss': 0.4205, 'learning_rate': 7.001312017104816e-06, 'epoch': 2.6}          \n",
      "{'loss': 0.422, 'learning_rate': 6.879828951844113e-06, 'epoch': 2.62}          \n",
      "{'loss': 0.463, 'learning_rate': 6.758345886583411e-06, 'epoch': 2.65}          \n",
      "{'loss': 0.4313, 'learning_rate': 6.636862821322708e-06, 'epoch': 2.67}         \n",
      "{'loss': 0.4062, 'learning_rate': 6.5153797560620055e-06, 'epoch': 2.7}         \n",
      "{'loss': 0.4285, 'learning_rate': 6.393896690801304e-06, 'epoch': 2.72}         \n",
      "{'loss': 0.4392, 'learning_rate': 6.2724136255406e-06, 'epoch': 2.75}           \n",
      "{'loss': 0.4449, 'learning_rate': 6.1509305602798975e-06, 'epoch': 2.77}        \n",
      "{'loss': 0.4235, 'learning_rate': 6.029447495019195e-06, 'epoch': 2.79}         \n",
      "{'loss': 0.4595, 'learning_rate': 5.907964429758492e-06, 'epoch': 2.82}         \n",
      "{'loss': 0.4474, 'learning_rate': 5.786481364497789e-06, 'epoch': 2.84}         \n",
      "{'loss': 0.4367, 'learning_rate': 5.664998299237087e-06, 'epoch': 2.87}         \n",
      "{'loss': 0.4365, 'learning_rate': 5.543515233976384e-06, 'epoch': 2.89}         \n",
      "{'loss': 0.4204, 'learning_rate': 5.422032168715682e-06, 'epoch': 2.92}         \n",
      " 73%|████████████████████████         | 60000/82316 [3:38:43<1:21:11,  4.58it/s][INFO|trainer.py:2709] 2023-04-24 22:15:53,605 >> Saving model checkpoint to /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-60000\n",
      "[INFO|configuration_utils.py:453] 2023-04-24 22:15:53,609 >> Configuration saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-60000/config.json\n",
      "[INFO|modeling_utils.py:1704] 2023-04-24 22:15:55,034 >> Model weights saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-60000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2160] 2023-04-24 22:15:55,038 >> tokenizer config file saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-60000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2167] 2023-04-24 22:15:55,039 >> Special tokens file saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-60000/special_tokens_map.json\n",
      "{'loss': 0.4255, 'learning_rate': 5.300549103454978e-06, 'epoch': 2.94}         \n",
      "{'loss': 0.4502, 'learning_rate': 5.179066038194276e-06, 'epoch': 2.96}         \n",
      "{'loss': 0.4491, 'learning_rate': 5.057582972933574e-06, 'epoch': 2.99}         \n",
      "{'loss': 0.3993, 'learning_rate': 4.936099907672871e-06, 'epoch': 3.01}         \n",
      "{'loss': 0.3264, 'learning_rate': 4.814616842412168e-06, 'epoch': 3.04}         \n",
      "{'loss': 0.3535, 'learning_rate': 4.693133777151466e-06, 'epoch': 3.06}         \n",
      "{'loss': 0.3587, 'learning_rate': 4.571650711890763e-06, 'epoch': 3.09}         \n",
      "{'loss': 0.3635, 'learning_rate': 4.45016764663006e-06, 'epoch': 3.11}          \n",
      "{'loss': 0.3404, 'learning_rate': 4.328684581369358e-06, 'epoch': 3.13}         \n",
      "{'loss': 0.3387, 'learning_rate': 4.207201516108655e-06, 'epoch': 3.16}         \n",
      "{'loss': 0.315, 'learning_rate': 4.085718450847952e-06, 'epoch': 3.18}          \n",
      "{'loss': 0.3499, 'learning_rate': 3.96423538558725e-06, 'epoch': 3.21}          \n",
      "{'loss': 0.3651, 'learning_rate': 3.842752320326546e-06, 'epoch': 3.23}         \n",
      "{'loss': 0.3468, 'learning_rate': 3.7212692550658443e-06, 'epoch': 3.26}        \n",
      "{'loss': 0.3359, 'learning_rate': 3.5997861898051417e-06, 'epoch': 3.28}        \n",
      "{'loss': 0.3425, 'learning_rate': 3.4783031245444386e-06, 'epoch': 3.3}         \n",
      "{'loss': 0.3456, 'learning_rate': 3.3568200592837364e-06, 'epoch': 3.33}        \n",
      "{'loss': 0.3495, 'learning_rate': 3.2353369940230333e-06, 'epoch': 3.35}        \n",
      "{'loss': 0.3325, 'learning_rate': 3.113853928762331e-06, 'epoch': 3.38}         \n",
      "{'loss': 0.3608, 'learning_rate': 2.992370863501628e-06, 'epoch': 3.4}          \n",
      "{'loss': 0.3766, 'learning_rate': 2.8708877982409257e-06, 'epoch': 3.43}        \n",
      "{'loss': 0.333, 'learning_rate': 2.7494047329802226e-06, 'epoch': 3.45}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3676, 'learning_rate': 2.62792166771952e-06, 'epoch': 3.47}          \n",
      "{'loss': 0.3619, 'learning_rate': 2.5064386024588173e-06, 'epoch': 3.5}         \n",
      "{'loss': 0.3268, 'learning_rate': 2.3849555371981147e-06, 'epoch': 3.52}        \n",
      "{'loss': 0.3241, 'learning_rate': 2.263472471937412e-06, 'epoch': 3.55}         \n",
      "{'loss': 0.3289, 'learning_rate': 2.1419894066767093e-06, 'epoch': 3.57}        \n",
      "{'loss': 0.3571, 'learning_rate': 2.0205063414160067e-06, 'epoch': 3.6}         \n",
      "{'loss': 0.325, 'learning_rate': 1.899023276155304e-06, 'epoch': 3.62}          \n",
      "{'loss': 0.3497, 'learning_rate': 1.7775402108946014e-06, 'epoch': 3.64}        \n",
      "{'loss': 0.3434, 'learning_rate': 1.6560571456338987e-06, 'epoch': 3.67}        \n",
      "{'loss': 0.3315, 'learning_rate': 1.5345740803731962e-06, 'epoch': 3.69}        \n",
      "{'loss': 0.3632, 'learning_rate': 1.4130910151124936e-06, 'epoch': 3.72}        \n",
      "{'loss': 0.3459, 'learning_rate': 1.291607949851791e-06, 'epoch': 3.74}         \n",
      "{'loss': 0.3509, 'learning_rate': 1.170124884591088e-06, 'epoch': 3.77}         \n",
      "{'loss': 0.3569, 'learning_rate': 1.0486418193303854e-06, 'epoch': 3.79}        \n",
      "{'loss': 0.3333, 'learning_rate': 9.271587540696827e-07, 'epoch': 3.81}         \n",
      "{'loss': 0.3364, 'learning_rate': 8.056756888089801e-07, 'epoch': 3.84}         \n",
      "{'loss': 0.3206, 'learning_rate': 6.841926235482774e-07, 'epoch': 3.86}         \n",
      "{'loss': 0.3421, 'learning_rate': 5.627095582875748e-07, 'epoch': 3.89}         \n",
      " 97%|██████████████████████████████████ | 80000/82316 [4:51:42<08:25,  4.58it/s][INFO|trainer.py:2709] 2023-04-24 23:28:52,560 >> Saving model checkpoint to /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-80000\n",
      "[INFO|configuration_utils.py:453] 2023-04-24 23:28:52,564 >> Configuration saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-80000/config.json\n",
      "[INFO|modeling_utils.py:1704] 2023-04-24 23:28:53,991 >> Model weights saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-80000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2160] 2023-04-24 23:28:53,996 >> tokenizer config file saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-80000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2167] 2023-04-24 23:28:53,997 >> Special tokens file saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/checkpoint-80000/special_tokens_map.json\n",
      "{'loss': 0.3113, 'learning_rate': 4.4122649302687214e-07, 'epoch': 3.91}        \n",
      "{'loss': 0.3322, 'learning_rate': 3.1974342776616943e-07, 'epoch': 3.94}        \n",
      "{'loss': 0.3552, 'learning_rate': 1.9826036250546677e-07, 'epoch': 3.96}        \n",
      "{'loss': 0.3573, 'learning_rate': 7.677729724476409e-08, 'epoch': 3.98}         \n",
      "100%|███████████████████████████████████| 82316/82316 [5:00:13<00:00,  4.58it/s][INFO|trainer.py:1901] 2023-04-24 23:37:23,408 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 18013.4434, 'train_samples_per_second': 36.558, 'train_steps_per_second': 4.57, 'train_loss': 0.5054071877671085, 'epoch': 4.0}\n",
      "100%|███████████████████████████████████| 82316/82316 [5:00:13<00:00,  4.57it/s]\n",
      "[INFO|trainer.py:2709] 2023-04-24 23:37:23,412 >> Saving model checkpoint to /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use\n",
      "[INFO|configuration_utils.py:453] 2023-04-24 23:37:23,416 >> Configuration saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/config.json\n",
      "[INFO|modeling_utils.py:1704] 2023-04-24 23:37:24,778 >> Model weights saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2160] 2023-04-24 23:37:24,782 >> tokenizer config file saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2167] 2023-04-24 23:37:24,784 >> Special tokens file saved in /home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  train_loss               =     0.5054\n",
      "  train_runtime            = 5:00:13.44\n",
      "  train_samples            =     164635\n",
      "  train_samples_per_second =     36.558\n",
      "  train_steps_per_second   =       4.57\n",
      "04/24/2023 23:37:24 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:710] 2023-04-24 23:37:24,832 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `DebertaV2ForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2964] 2023-04-24 23:37:24,834 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2966] 2023-04-24 23:37:24,834 >>   Num examples = 5963\n",
      "[INFO|trainer.py:2969] 2023-04-24 23:37:24,834 >>   Batch size = 4\n",
      "100%|██████████████████████████████████████▉| 1488/1491 [00:52<00:00, 28.16it/s]04/24/2023 23:38:25 - INFO - utils_qa - Post-processing 2039 example predictions split into 5963 features.\n",
      "\n",
      "  0%|                                                  | 0/2039 [00:00<?, ?it/s]\u001b[A\n",
      "Traceback (most recent call last):\n",
      "  File \"deberta_qa/run_qa.py\", line 692, in <module>\n",
      "    main()\n",
      "  File \"deberta_qa/run_qa.py\", line 649, in main\n",
      "    metrics = trainer.evaluate()\n",
      "  File \"/home/jovyan/chatbot/achernyavskiy/deberta_qa/trainer_qa.py\", line 71, in evaluate\n",
      "    eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)\n",
      "  File \"deberta_qa/run_qa.py\", line 585, in post_processing_function\n",
      "    predictions = postprocess_qa_predictions(\n",
      "  File \"/home/jovyan/chatbot/achernyavskiy/deberta_qa/utils_qa.py\", line 209, in postprocess_qa_predictions\n",
      "    score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n",
      "UnboundLocalError: local variable 'null_score' referenced before assignment\n",
      "100%|███████████████████████████████████████| 1491/1491 [01:01<00:00, 24.42it/s]\n"
     ]
    }
   ],
   "source": [
    "!WANDB_DISABLED=True CUDA_VISIBLE_DEVICES=0 /home/user/conda/envs/deberta_retrain/bin/python deberta_qa/run_qa.py \\\n",
    "  --model_name_or_path deepset/deberta-v3-base-squad2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --train_file '/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_train_withhn_filt_upd_use.json' \\\n",
    "  --do_train \\\n",
    "  --validation_file '/home/jovyan/chatbot/_common/datasets/deberta_retrain/squad_format_valid_withhn_filt_upd_use.json' \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size=4 \\\n",
    "  --per_device_eval_batch_size=4 \\\n",
    "  --gradient_accumulation_steps=2 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 4 \\\n",
    "  --save_steps 20000 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir=\"/home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use\" \\\n",
    "  --overwrite_output_dir \\\n",
    "  --max_seq_length 384 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/deberta_retrain/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "#from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "import re\n",
    "import numpy as np\n",
    "from chat_scripts.deberta_qa import DebertaQA\n",
    "import pandas as pd\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "deberta_qa = DebertaQA(model_name='deepset/deberta-v3-base-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "deberta_qa = DebertaQA(model_name='/home/jovyan/chatbot/_common/checkpoint/deberta_retrain_withhn_filt_upd_use/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''As shown in Table 2, our model outperforms baseline models and other lexicon-based models on four Chinese NER datasets. Our model outperforms TENER (Yan et al., 2019) by 1.72 in average F1 score. For lattice LSTM, our model has an average F1 improvement of 1.51 over it. When using another lexicon (Li et al., 2018), our model also outperforms CGN by 0.73 in average F1 score. Maybe due to the characteristic of Transformer, the improvement of FLAT over other lexicon-based models on small datasets is not so significant like that on large datasets. To verify the computation efficiency of our model, we compare the inference-speed of different lexicon-based models on Ontonotes. The result is shown in Figure 3. GNN-based models outperform lattice LSTM and LR-CNN. But the RNN encoder of GNN-based models also degrades their speed. Because our model has no recurrent module and can fully leverage parallel computation of GPU, it outperforms other methods in running efficiency. In terms of leveraging batch-parallelism, the speedup ratio brought by batch-parallelism is 4.97 for FLAT, 2.1 for lattice LSTM, when batch size = 16. Due to the simplicity of our model, it can benefit from batch-parallelism more significantly.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''The flat-lattice structure consists of spans with different lengths. To encode the interactions among spans, we propose the relative position encoding of spans. For two spans x i and x j in the lattice, there are three kinds of relations between them: intersection, inclusion and separation, determined by their heads and tails. Instead of directly encoding these three kinds of relations, we use a dense vector to model their relations. It is calculated by continuous transformation of the head and tail information. Thus, we think it can not only represent the relation between two tokens, but also indicate more detailed information, such as the distance between a character and a word.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is the speedup ratio brought by batch-parallelism?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9848097562789917,\n",
       " 'start_pos': 978,\n",
       " 'end_pos': 1129,\n",
       " 'text': ' In terms of leveraging batch-parallelism, the speedup ratio brought by batch-parallelism is 4.97 for FLAT, 2.1 for lattice LSTM, when batch size = 16.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta_qa.predict(question, context, max_tokens=384, min_tokens_diff=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deberta_retrain",
   "language": "python",
   "name": "deberta_retrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
