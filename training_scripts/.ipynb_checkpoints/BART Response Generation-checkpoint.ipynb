{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../_common/datasets/dialogpt/train_v2.txt\") as file:\n",
    "    train = file.read().splitlines()\n",
    "    \n",
    "with open(\"../_common/datasets/dialogpt/val_v2.txt\") as file:\n",
    "    valid = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28092"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, text) in tqdm(enumerate(train), total=len(train)):\n",
    "    if len(re.findall(\" <SEP> \", text)) != 2:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, text) in tqdm(enumerate(valid), total=len(valid)):\n",
    "    if len(re.findall(\" <SEP> \", text)) != 2:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate optimal input lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'theojolliffe/bart-cnn-science'\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "# model = BartModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    'sep_token': '<SEP>',\n",
    "    'additional_special_tokens': ['<UTTERSEP>']\n",
    "}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb85e7c03ee4b8ba0cefefef1057c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28092 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_tokens_input = []\n",
    "num_tokens_output = []\n",
    "\n",
    "toomuchtokens = []\n",
    "\n",
    "\n",
    "for i, text in tqdm(enumerate(train), total=len(train)):\n",
    "    input_text, output_text = text.split(\" <INPUTEND> \")\n",
    "    input_text = input_text.replace(\"<BOS> \", \"\").replace(\" <EOS>\", \"\")\n",
    "    if len(tokenizer.encode(text)) > 1024:\n",
    "        toomuchtokens.append(i)\n",
    "    output_text = output_text.replace(\"<BOS> \", \"\").replace(\" <EOS>\", \"\")\n",
    "    num_tokens_input.append(len(tokenizer.encode(input_text)))\n",
    "    num_tokens_output.append(len(tokenizer.encode(output_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148.88996867435569, 136.0, 276.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens_input), np.median(num_tokens_input), np.quantile(num_tokens_input, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53.6047273245052, 43.0, 123.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens_output), np.median(num_tokens_output), np.quantile(num_tokens_output, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "for text in tqdm(train):\n",
    "    input_text, output_text = text.split(\" <INPUTEND> \")\n",
    "    input_text = input_text[6:]\n",
    "    output_text = output_text[:-6]\n",
    "    \n",
    "    if len(tokenizer.encode(input_text)) > 1800:\n",
    "        continue\n",
    "    \n",
    "    questions.append(input_text)\n",
    "    answers.append(output_text)\n",
    "\n",
    "train_df = pd.DataFrame({\"text\": questions, \"summary\": answers})\n",
    "train_df.to_csv(\"datasets/bart_response_data/train_ft_deberta.csv\", index=False)\n",
    "        \n",
    "        \n",
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "for text in tqdm(valid):\n",
    "    input_text, output_text = text.split(\" <INPUTEND> \")\n",
    "    questions.append(input_text[6:])\n",
    "    answers.append(output_text[:-6])\n",
    "\n",
    "valid_df = pd.DataFrame({\"text\": questions, \"summary\": answers})\n",
    "valid_df.to_csv(\"datasets/bart_response_data/valid_ft_deberta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"datasets/bart_response_data/train_ft_deberta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-14 12:07:11.995887: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 12:07:13.837836: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-14 12:07:13.837918: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-14 12:07:13.837927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "04/14/2023 12:07:17 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/14/2023 12:07:17 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=1000,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../_common/bart_response_generation/bart_cnn_science_4ep_2e05/runs/Apr14_12-07-16_chatbot-1gpu-2-0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=../_common/bart_response_generation/bart_cnn_science_4ep_2e05,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=20,\n",
      "per_device_train_batch_size=20,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=bart_cnn_science_4ep_2e05,\n",
      "save_on_each_node=False,\n",
      "save_steps=1000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/14/2023 12:07:17 - INFO - datasets.builder - Using custom data configuration default-538fc555142246f6\n",
      "04/14/2023 12:07:17 - INFO - datasets.info - Loading Dataset Infos from /home/jovyan/.imgenv-chatbot-1gpu-2-0/lib/python3.7/site-packages/datasets/packaged_modules/csv\n",
      "04/14/2023 12:07:17 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "04/14/2023 12:07:17 - INFO - datasets.info - Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/csv/default-538fc555142246f6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\n",
      "04/14/2023 12:07:17 - WARNING - datasets.builder - Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-538fc555142246f6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "04/14/2023 12:07:17 - INFO - datasets.info - Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/csv/default-538fc555142246f6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 639.18it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-04-14 12:07:18,046 >> loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--theojolliffe--bart-cnn-science/snapshots/2b5c0e689642ef19663935c01d19a6881777c0d2/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-14 12:07:18,058 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"theojolliffe/bart-cnn-science\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-14 12:07:18,301 >> loading file vocab.json from cache at /home/jovyan/.cache/huggingface/hub/models--theojolliffe--bart-cnn-science/snapshots/2b5c0e689642ef19663935c01d19a6881777c0d2/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-14 12:07:18,301 >> loading file merges.txt from cache at /home/jovyan/.cache/huggingface/hub/models--theojolliffe--bart-cnn-science/snapshots/2b5c0e689642ef19663935c01d19a6881777c0d2/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-14 12:07:18,301 >> loading file tokenizer.json from cache at /home/jovyan/.cache/huggingface/hub/models--theojolliffe--bart-cnn-science/snapshots/2b5c0e689642ef19663935c01d19a6881777c0d2/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-14 12:07:18,301 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-14 12:07:18,301 >> loading file special_tokens_map.json from cache at /home/jovyan/.cache/huggingface/hub/models--theojolliffe--bart-cnn-science/snapshots/2b5c0e689642ef19663935c01d19a6881777c0d2/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-04-14 12:07:18,301 >> loading file tokenizer_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--theojolliffe--bart-cnn-science/snapshots/2b5c0e689642ef19663935c01d19a6881777c0d2/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:907] 2023-04-14 12:07:18,346 >> Assigning <SEP> to the sep_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:907] 2023-04-14 12:07:18,346 >> Assigning ['<INPUTEND>', '<UTTERSEP>'] to the additional_special_tokens key of the tokenizer\n",
      "[INFO|modeling_utils.py:2403] 2023-04-14 12:07:18,393 >> loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--theojolliffe--bart-cnn-science/snapshots/2b5c0e689642ef19663935c01d19a6881777c0d2/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:575] 2023-04-14 12:07:18,889 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3034] 2023-04-14 12:07:21,954 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3043] 2023-04-14 12:07:21,954 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at theojolliffe/bart-cnn-science.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "[INFO|modeling_utils.py:2693] 2023-04-14 12:07:22,193 >> Generation config file not found, using a generation config created from the model config.\n",
      "04/14/2023 12:07:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-538fc555142246f6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-caf17a714d05048d.arrow\n",
      "Running tokenizer on validation dataset:   0%|  | 0/1473 [00:00<?, ? examples/s]04/14/2023 12:07:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-538fc555142246f6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eff08a81dfaf8a1c.arrow\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/jovyan/.imgenv-chatbot-1gpu-2-0/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1740] 2023-04-14 12:07:27,841 >> ***** Running training *****\n",
      "[INFO|trainer.py:1741] 2023-04-14 12:07:27,842 >>   Num examples = 28089\n",
      "[INFO|trainer.py:1742] 2023-04-14 12:07:27,842 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1743] 2023-04-14 12:07:27,842 >>   Instantaneous batch size per device = 20\n",
      "[INFO|trainer.py:1744] 2023-04-14 12:07:27,842 >>   Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "[INFO|trainer.py:1745] 2023-04-14 12:07:27,842 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1746] 2023-04-14 12:07:27,842 >>   Total optimization steps = 7025\n",
      "[INFO|trainer.py:1748] 2023-04-14 12:07:27,842 >>   Number of trainable parameters = 406294528\n",
      "[INFO|integrations.py:710] 2023-04-14 12:07:27,844 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcruel-no\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.14.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jovyan/chatbot/nikiforova/wandb/run-20230414_120729-kroz5pbl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbart_cnn_science_4ep_2e05\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cruel-no/bart_responces\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cruel-no/bart_responces/runs/kroz5pbl\u001b[0m\n",
      "  0%|                                                  | 0/7025 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-04-14 12:07:34,790 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 1.9929, 'learning_rate': 1.715302491103203e-05, 'epoch': 0.71}         \n",
      " 14%|█████▌                                 | 1000/7025 [04:18<25:34,  3.93it/s][INFO|trainer.py:3068] 2023-04-14 12:11:53,242 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3070] 2023-04-14 12:11:53,242 >>   Num examples = 1473\n",
      "[INFO|trainer.py:3073] 2023-04-14 12:11:53,242 >>   Batch size = 20\n",
      "[INFO|configuration_utils.py:575] 2023-04-14 12:11:53,250 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/74 [00:02<01:47,  1.49s/it]\u001b[A\n",
      "  4%|█▊                                          | 3/74 [00:05<02:17,  1.93s/it]\u001b[A\n",
      "  5%|██▍                                         | 4/74 [00:08<02:50,  2.44s/it]\u001b[A\n",
      "  7%|██▉                                         | 5/74 [00:12<03:09,  2.75s/it]\u001b[A\n",
      "  8%|███▌                                        | 6/74 [00:14<03:06,  2.74s/it]\u001b[A\n",
      "  9%|████▏                                       | 7/74 [00:17<03:07,  2.79s/it]\u001b[A\n",
      " 11%|████▊                                       | 8/74 [00:22<03:38,  3.31s/it]\u001b[A\n",
      " 12%|█████▎                                      | 9/74 [00:25<03:31,  3.26s/it]\u001b[A\n",
      " 14%|█████▊                                     | 10/74 [00:28<03:23,  3.18s/it]\u001b[A\n",
      " 15%|██████▍                                    | 11/74 [00:31<03:25,  3.27s/it]\u001b[A\n",
      " 16%|██████▉                                    | 12/74 [00:34<03:19,  3.22s/it]\u001b[A\n",
      " 18%|███████▌                                   | 13/74 [00:37<03:10,  3.12s/it]\u001b[A\n",
      " 19%|████████▏                                  | 14/74 [00:41<03:22,  3.37s/it]\u001b[A\n",
      " 20%|████████▋                                  | 15/74 [00:44<03:13,  3.28s/it]\u001b[A\n",
      " 22%|█████████▎                                 | 16/74 [00:47<03:02,  3.14s/it]\u001b[A\n",
      " 23%|█████████▉                                 | 17/74 [00:51<03:14,  3.41s/it]\u001b[A\n",
      " 24%|██████████▍                                | 18/74 [00:53<02:47,  2.99s/it]\u001b[A\n",
      " 26%|███████████                                | 19/74 [00:58<03:06,  3.39s/it]\u001b[A\n",
      " 27%|███████████▌                               | 20/74 [01:01<02:59,  3.32s/it]\u001b[A\n",
      " 28%|████████████▏                              | 21/74 [01:04<03:00,  3.41s/it]\u001b[A\n",
      " 30%|████████████▊                              | 22/74 [01:07<02:49,  3.26s/it]\u001b[A\n",
      " 31%|█████████████▎                             | 23/74 [01:11<02:48,  3.30s/it]\u001b[A\n",
      " 32%|█████████████▉                             | 24/74 [01:14<02:42,  3.25s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 25/74 [01:16<02:25,  2.97s/it]\u001b[A\n",
      " 35%|███████████████                            | 26/74 [01:21<02:56,  3.67s/it]\u001b[A\n",
      " 36%|███████████████▋                           | 27/74 [01:25<02:46,  3.53s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 28/74 [01:28<02:34,  3.36s/it]\u001b[A\n",
      " 39%|████████████████▊                          | 29/74 [01:30<02:20,  3.12s/it]\u001b[A\n",
      " 41%|█████████████████▍                         | 30/74 [01:34<02:33,  3.48s/it]\u001b[A\n",
      " 42%|██████████████████                         | 31/74 [01:37<02:19,  3.25s/it]\u001b[A\n",
      " 43%|██████████████████▌                        | 32/74 [01:40<02:17,  3.28s/it]\u001b[A\n",
      " 45%|███████████████████▏                       | 33/74 [01:47<02:54,  4.26s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 34/74 [01:49<02:26,  3.65s/it]\u001b[A\n",
      " 47%|████████████████████▎                      | 35/74 [01:52<02:09,  3.32s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 36/74 [01:54<01:55,  3.04s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 37/74 [01:58<02:01,  3.28s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 38/74 [02:02<02:06,  3.52s/it]\u001b[A\n",
      " 53%|██████████████████████▋                    | 39/74 [02:04<01:50,  3.15s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 40/74 [02:08<01:47,  3.17s/it]\u001b[A\n",
      " 55%|███████████████████████▊                   | 41/74 [02:10<01:40,  3.05s/it]\u001b[A\n",
      " 57%|████████████████████████▍                  | 42/74 [02:13<01:29,  2.80s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 43/74 [02:16<01:36,  3.11s/it]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 44/74 [02:20<01:33,  3.10s/it]\u001b[A\n",
      " 61%|██████████████████████████▏                | 45/74 [02:23<01:36,  3.31s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 46/74 [02:26<01:29,  3.19s/it]\u001b[A\n",
      " 64%|███████████████████████████▎               | 47/74 [02:28<01:18,  2.91s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 48/74 [02:32<01:20,  3.08s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 49/74 [02:37<01:32,  3.71s/it]\u001b[A\n",
      " 68%|█████████████████████████████              | 50/74 [02:42<01:38,  4.11s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 51/74 [02:49<01:53,  4.96s/it]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 52/74 [02:54<01:51,  5.05s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 53/74 [02:57<01:29,  4.24s/it]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 54/74 [03:00<01:20,  4.01s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 55/74 [03:03<01:10,  3.72s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 56/74 [03:07<01:06,  3.70s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 57/74 [03:11<01:05,  3.84s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 58/74 [03:15<01:02,  3.89s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 59/74 [03:18<00:55,  3.67s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 60/74 [03:22<00:53,  3.84s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 61/74 [03:26<00:48,  3.76s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 62/74 [03:30<00:47,  3.96s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 63/74 [03:33<00:39,  3.62s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████▏     | 64/74 [03:38<00:38,  3.88s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 65/74 [03:41<00:33,  3.76s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████▎    | 66/74 [03:44<00:26,  3.33s/it]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 67/74 [03:47<00:23,  3.30s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 68/74 [03:50<00:19,  3.27s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 69/74 [03:53<00:15,  3.15s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [04:12<00:31,  7.86s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 71/74 [04:15<00:19,  6.46s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 72/74 [04:18<00:10,  5.48s/it]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 73/74 [04:22<00:04,  4.93s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.862419605255127, 'eval_rouge1': 48.1439, 'eval_rouge2': 31.6734, 'eval_rougeL': 39.4342, 'eval_rougeLsum': 43.8398, 'eval_gen_len': 78.28173794976239, 'eval_runtime': 275.0793, 'eval_samples_per_second': 5.355, 'eval_steps_per_second': 0.269, 'epoch': 0.71}\n",
      " 14%|█████▌                                 | 1000/7025 [08:53<25:34,  3.93it/s]\n",
      "100%|███████████████████████████████████████████| 74/74 [04:31<00:00,  4.38s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2814] 2023-04-14 12:16:28,327 >> Saving model checkpoint to ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-1000\n",
      "[INFO|configuration_utils.py:457] 2023-04-14 12:16:28,357 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-14 12:16:28,387 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-1000/generation_config.json\n",
      "[INFO|modeling_utils.py:1762] 2023-04-14 12:16:31,333 >> Model weights saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-04-14 12:16:31,364 >> tokenizer config file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-14 12:16:31,371 >> Special tokens file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.7638, 'learning_rate': 1.4306049822064058e-05, 'epoch': 1.42}        \n",
      " 28%|███████████                            | 2000/7025 [13:24<21:28,  3.90it/s][INFO|trainer.py:3068] 2023-04-14 12:20:59,278 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3070] 2023-04-14 12:20:59,278 >>   Num examples = 1473\n",
      "[INFO|trainer.py:3073] 2023-04-14 12:20:59,278 >>   Batch size = 20\n",
      "\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/74 [00:03<02:18,  1.93s/it]\u001b[A\n",
      "  4%|█▊                                          | 3/74 [00:09<04:04,  3.44s/it]\u001b[A\n",
      "  5%|██▍                                         | 4/74 [00:14<04:46,  4.09s/it]\u001b[A\n",
      "  7%|██▉                                         | 5/74 [00:18<04:36,  4.00s/it]\u001b[A\n",
      "  8%|███▌                                        | 6/74 [00:21<04:01,  3.56s/it]\u001b[A\n",
      "  9%|████▏                                       | 7/74 [00:25<04:07,  3.69s/it]\u001b[A\n",
      " 11%|████▊                                       | 8/74 [00:31<05:08,  4.68s/it]\u001b[A\n",
      " 12%|█████▎                                      | 9/74 [00:34<04:27,  4.11s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14%|█████▊                                     | 10/74 [00:38<04:13,  3.96s/it]\u001b[A\n",
      " 15%|██████▍                                    | 11/74 [00:44<04:59,  4.75s/it]\u001b[A\n",
      " 16%|██████▉                                    | 12/74 [00:48<04:26,  4.30s/it]\u001b[A\n",
      " 18%|███████▌                                   | 13/74 [00:52<04:22,  4.30s/it]\u001b[A\n",
      " 19%|████████▏                                  | 14/74 [00:57<04:24,  4.41s/it]\u001b[A\n",
      " 20%|████████▋                                  | 15/74 [01:00<04:08,  4.22s/it]\u001b[A\n",
      " 22%|█████████▎                                 | 16/74 [01:05<04:07,  4.27s/it]\u001b[A\n",
      " 23%|█████████▉                                 | 17/74 [01:09<04:06,  4.33s/it]\u001b[A\n",
      " 24%|██████████▍                                | 18/74 [01:12<03:40,  3.94s/it]\u001b[A\n",
      " 26%|███████████                                | 19/74 [01:17<03:53,  4.25s/it]\u001b[A\n",
      " 27%|███████████▌                               | 20/74 [01:21<03:36,  4.00s/it]\u001b[A\n",
      " 28%|████████████▏                              | 21/74 [01:25<03:40,  4.16s/it]\u001b[A\n",
      " 30%|████████████▊                              | 22/74 [01:30<03:44,  4.31s/it]\u001b[A\n",
      " 31%|█████████████▎                             | 23/74 [01:34<03:35,  4.22s/it]\u001b[A\n",
      " 32%|█████████████▉                             | 24/74 [01:38<03:22,  4.05s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 25/74 [01:40<02:52,  3.52s/it]\u001b[A\n",
      " 35%|███████████████                            | 26/74 [01:44<03:02,  3.80s/it]\u001b[A\n",
      " 36%|███████████████▋                           | 27/74 [01:48<02:55,  3.73s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 28/74 [01:51<02:43,  3.56s/it]\u001b[A\n",
      " 39%|████████████████▊                          | 29/74 [01:55<02:50,  3.80s/it]\u001b[A\n",
      " 41%|█████████████████▍                         | 30/74 [02:00<03:03,  4.18s/it]\u001b[A\n",
      " 42%|██████████████████                         | 31/74 [02:03<02:39,  3.71s/it]\u001b[A\n",
      " 43%|██████████████████▌                        | 32/74 [02:07<02:37,  3.76s/it]\u001b[A\n",
      " 45%|███████████████████▏                       | 33/74 [02:12<02:48,  4.12s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 34/74 [02:14<02:23,  3.58s/it]\u001b[A\n",
      " 47%|████████████████████▎                      | 35/74 [02:20<02:42,  4.18s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 36/74 [02:22<02:21,  3.72s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 37/74 [02:26<02:18,  3.73s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 38/74 [02:32<02:32,  4.23s/it]\u001b[A\n",
      " 53%|██████████████████████▋                    | 39/74 [02:34<02:14,  3.83s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 40/74 [02:40<02:23,  4.21s/it]\u001b[A\n",
      " 55%|███████████████████████▊                   | 41/74 [02:44<02:22,  4.32s/it]\u001b[A\n",
      " 57%|████████████████████████▍                  | 42/74 [02:47<02:01,  3.81s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 43/74 [02:52<02:10,  4.22s/it]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 44/74 [02:56<02:00,  4.03s/it]\u001b[A\n",
      " 61%|██████████████████████████▏                | 45/74 [02:59<01:55,  3.98s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 46/74 [03:04<01:52,  4.01s/it]\u001b[A\n",
      " 64%|███████████████████████████▎               | 47/74 [03:07<01:42,  3.81s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 48/74 [03:10<01:33,  3.59s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 49/74 [03:16<01:48,  4.33s/it]\u001b[A\n",
      " 68%|█████████████████████████████              | 50/74 [03:23<02:04,  5.20s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 51/74 [03:27<01:52,  4.91s/it]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 54/74 [03:39<01:20,  4.02s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 55/74 [03:42<01:09,  3.67s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 56/74 [03:46<01:10,  3.90s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 57/74 [03:52<01:14,  4.36s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 58/74 [03:55<01:06,  4.14s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 59/74 [04:00<01:03,  4.22s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 60/74 [04:04<01:00,  4.30s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 61/74 [04:07<00:50,  3.92s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 62/74 [04:12<00:50,  4.22s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 63/74 [04:15<00:42,  3.86s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████▏     | 64/74 [04:21<00:44,  4.44s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 65/74 [04:25<00:38,  4.23s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████▎    | 66/74 [04:29<00:33,  4.16s/it]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 67/74 [04:33<00:28,  4.08s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 68/74 [04:37<00:24,  4.07s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 69/74 [04:40<00:19,  3.95s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [04:45<00:16,  4.15s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 71/74 [04:48<00:11,  3.94s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 72/74 [04:52<00:07,  3.83s/it]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 73/74 [04:56<00:03,  3.85s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8397542238235474, 'eval_rouge1': 48.1467, 'eval_rouge2': 31.7288, 'eval_rougeL': 39.163, 'eval_rougeLsum': 43.8131, 'eval_gen_len': 80.17990495587237, 'eval_runtime': 309.3903, 'eval_samples_per_second': 4.761, 'eval_steps_per_second': 0.239, 'epoch': 1.42}\n",
      " 28%|███████████                            | 2000/7025 [18:33<21:28,  3.90it/s]\n",
      "100%|███████████████████████████████████████████| 74/74 [05:06<00:00,  3.73s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2814] 2023-04-14 12:26:08,673 >> Saving model checkpoint to ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-2000\n",
      "[INFO|configuration_utils.py:457] 2023-04-14 12:26:08,678 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-2000/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-14 12:26:08,683 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-2000/generation_config.json\n",
      "[INFO|modeling_utils.py:1762] 2023-04-14 12:26:11,907 >> Model weights saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-04-14 12:26:11,917 >> tokenizer config file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-14 12:26:11,944 >> Special tokens file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.6468, 'learning_rate': 1.1459074733096086e-05, 'epoch': 2.14}        \n",
      " 43%|████████████████▋                      | 3000/7025 [23:02<16:54,  3.97it/s][INFO|trainer.py:3068] 2023-04-14 12:30:37,540 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3070] 2023-04-14 12:30:37,541 >>   Num examples = 1473\n",
      "[INFO|trainer.py:3073] 2023-04-14 12:30:37,541 >>   Batch size = 20\n",
      "\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/74 [00:03<01:48,  1.51s/it]\u001b[A\n",
      "  4%|█▊                                          | 3/74 [00:06<02:53,  2.44s/it]\u001b[A\n",
      "  5%|██▍                                         | 4/74 [00:10<03:14,  2.78s/it]\u001b[A\n",
      "  7%|██▉                                         | 5/74 [00:13<03:37,  3.15s/it]\u001b[A\n",
      "  8%|███▌                                        | 6/74 [00:16<03:22,  2.98s/it]\u001b[A\n",
      "  9%|████▏                                       | 7/74 [00:20<03:39,  3.28s/it]\u001b[A\n",
      " 11%|████▊                                       | 8/74 [00:25<04:12,  3.83s/it]\u001b[A\n",
      " 12%|█████▎                                      | 9/74 [00:28<03:58,  3.68s/it]\u001b[A\n",
      " 14%|█████▊                                     | 10/74 [00:31<03:40,  3.45s/it]\u001b[A\n",
      " 15%|██████▍                                    | 11/74 [00:37<04:28,  4.26s/it]\u001b[A\n",
      " 16%|██████▉                                    | 12/74 [00:41<04:11,  4.05s/it]\u001b[A\n",
      " 18%|███████▌                                   | 13/74 [00:44<03:56,  3.87s/it]\u001b[A\n",
      " 19%|████████▏                                  | 14/74 [00:48<03:50,  3.84s/it]\u001b[A\n",
      " 20%|████████▋                                  | 15/74 [00:52<03:54,  3.97s/it]\u001b[A\n",
      " 22%|█████████▎                                 | 16/74 [00:56<03:47,  3.92s/it]\u001b[A\n",
      " 23%|█████████▉                                 | 17/74 [01:00<03:46,  3.98s/it]\u001b[A\n",
      " 24%|██████████▍                                | 18/74 [01:04<03:30,  3.75s/it]\u001b[A\n",
      " 26%|███████████                                | 19/74 [01:07<03:15,  3.55s/it]\u001b[A\n",
      " 27%|███████████▌                               | 20/74 [01:10<03:10,  3.54s/it]\u001b[A\n",
      " 28%|████████████▏                              | 21/74 [01:14<03:07,  3.53s/it]\u001b[A\n",
      " 30%|████████████▊                              | 22/74 [01:16<02:48,  3.23s/it]\u001b[A\n",
      " 31%|█████████████▎                             | 23/74 [01:20<02:55,  3.45s/it]\u001b[A\n",
      " 32%|█████████████▉                             | 24/74 [01:24<03:01,  3.63s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 25/74 [01:27<02:50,  3.47s/it]\u001b[A\n",
      " 35%|███████████████                            | 26/74 [01:33<03:22,  4.22s/it]\u001b[A\n",
      " 36%|███████████████▋                           | 27/74 [01:38<03:21,  4.28s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 28/74 [01:42<03:16,  4.28s/it]\u001b[A\n",
      " 39%|████████████████▊                          | 29/74 [01:46<03:01,  4.04s/it]\u001b[A\n",
      " 41%|█████████████████▍                         | 30/74 [01:50<03:02,  4.14s/it]\u001b[A\n",
      " 42%|██████████████████                         | 31/74 [01:53<02:39,  3.72s/it]\u001b[A\n",
      " 43%|██████████████████▌                        | 32/74 [01:57<02:43,  3.89s/it]\u001b[A\n",
      " 45%|███████████████████▏                       | 33/74 [02:01<02:44,  4.00s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 34/74 [02:04<02:26,  3.66s/it]\u001b[A\n",
      " 47%|████████████████████▎                      | 35/74 [02:07<02:18,  3.54s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 36/74 [02:10<02:02,  3.22s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 37/74 [02:14<02:09,  3.49s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 38/74 [02:17<02:02,  3.41s/it]\u001b[A\n",
      " 53%|██████████████████████▋                    | 39/74 [02:20<01:57,  3.37s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 40/74 [02:26<02:12,  3.89s/it]\u001b[A\n",
      " 55%|███████████████████████▊                   | 41/74 [02:28<01:58,  3.60s/it]\u001b[A\n",
      " 57%|████████████████████████▍                  | 42/74 [02:32<01:52,  3.51s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 43/74 [02:38<02:10,  4.21s/it]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 44/74 [02:41<01:56,  3.90s/it]\u001b[A\n",
      " 61%|██████████████████████████▏                | 45/74 [02:48<02:22,  4.92s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 46/74 [02:52<02:07,  4.54s/it]\u001b[A\n",
      " 64%|███████████████████████████▎               | 47/74 [02:55<01:55,  4.27s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 48/74 [02:59<01:42,  3.94s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 49/74 [03:04<01:48,  4.34s/it]\u001b[A\n",
      " 68%|█████████████████████████████              | 50/74 [03:09<01:49,  4.57s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 51/74 [03:14<01:47,  4.68s/it]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 52/74 [03:23<02:10,  5.94s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 53/74 [03:25<01:43,  4.95s/it]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 54/74 [03:28<01:25,  4.28s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 55/74 [03:32<01:18,  4.12s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 56/74 [03:36<01:14,  4.11s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 57/74 [03:41<01:15,  4.43s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 58/74 [03:45<01:08,  4.26s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 59/74 [03:49<01:03,  4.21s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 60/74 [03:54<01:01,  4.36s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 61/74 [03:56<00:49,  3.78s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 62/74 [04:00<00:46,  3.90s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 63/74 [04:03<00:39,  3.55s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████▏     | 64/74 [04:07<00:37,  3.79s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 65/74 [04:11<00:34,  3.84s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████▎    | 66/74 [04:15<00:30,  3.75s/it]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 67/74 [04:19<00:25,  3.70s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 68/74 [04:22<00:22,  3.77s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 69/74 [04:26<00:18,  3.65s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [04:29<00:13,  3.50s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 71/74 [04:32<00:10,  3.34s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 72/74 [04:35<00:06,  3.33s/it]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 73/74 [04:39<00:03,  3.46s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.865185022354126, 'eval_rouge1': 48.6522, 'eval_rouge2': 31.9941, 'eval_rougeL': 39.5605, 'eval_rougeLsum': 44.1349, 'eval_gen_len': 80.52274270196877, 'eval_runtime': 292.715, 'eval_samples_per_second': 5.032, 'eval_steps_per_second': 0.253, 'epoch': 2.14}\n",
      " 43%|████████████████▋                      | 3000/7025 [27:55<16:54,  3.97it/s]\n",
      "100%|███████████████████████████████████████████| 74/74 [04:48<00:00,  3.39s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2814] 2023-04-14 12:35:30,260 >> Saving model checkpoint to ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-3000\n",
      "[INFO|configuration_utils.py:457] 2023-04-14 12:35:30,266 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-3000/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-14 12:35:30,271 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-3000/generation_config.json\n",
      "[INFO|modeling_utils.py:1762] 2023-04-14 12:35:33,302 >> Model weights saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-04-14 12:35:33,307 >> tokenizer config file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-14 12:35:33,312 >> Special tokens file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.522, 'learning_rate': 8.612099644128115e-06, 'epoch': 2.85}          \n",
      " 57%|██████████████████████▏                | 4000/7025 [32:24<13:45,  3.66it/s][INFO|trainer.py:3068] 2023-04-14 12:39:59,216 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3070] 2023-04-14 12:39:59,216 >>   Num examples = 1473\n",
      "[INFO|trainer.py:3073] 2023-04-14 12:39:59,217 >>   Batch size = 20\n",
      "\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/74 [00:03<01:51,  1.55s/it]\u001b[A\n",
      "  4%|█▊                                          | 3/74 [00:07<03:18,  2.80s/it]\u001b[A\n",
      "  5%|██▍                                         | 4/74 [00:11<03:53,  3.34s/it]\u001b[A\n",
      "  7%|██▉                                         | 5/74 [00:14<03:40,  3.19s/it]\u001b[A\n",
      "  8%|███▌                                        | 6/74 [00:17<03:25,  3.02s/it]\u001b[A\n",
      "  9%|████▏                                       | 7/74 [00:20<03:29,  3.12s/it]\u001b[A\n",
      " 11%|████▊                                       | 8/74 [00:28<04:58,  4.53s/it]\u001b[A\n",
      " 12%|█████▎                                      | 9/74 [00:31<04:24,  4.07s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14%|█████▊                                     | 10/74 [00:35<04:12,  3.95s/it]\u001b[A\n",
      " 15%|██████▍                                    | 11/74 [00:44<05:53,  5.61s/it]\u001b[A\n",
      " 16%|██████▉                                    | 12/74 [00:49<05:39,  5.47s/it]\u001b[A\n",
      " 18%|███████▌                                   | 13/74 [00:53<04:57,  4.88s/it]\u001b[A\n",
      " 19%|████████▏                                  | 14/74 [00:57<04:43,  4.72s/it]\u001b[A\n",
      " 20%|████████▋                                  | 15/74 [01:01<04:30,  4.58s/it]\u001b[A\n",
      " 22%|█████████▎                                 | 16/74 [01:06<04:25,  4.58s/it]\u001b[A\n",
      " 23%|█████████▉                                 | 17/74 [01:12<04:47,  5.04s/it]\u001b[A\n",
      " 24%|██████████▍                                | 18/74 [01:15<04:02,  4.33s/it]\u001b[A\n",
      " 26%|███████████                                | 19/74 [01:19<03:54,  4.27s/it]\u001b[A\n",
      " 27%|███████████▌                               | 20/74 [01:23<03:43,  4.13s/it]\u001b[A\n",
      " 28%|████████████▏                              | 21/74 [01:27<03:36,  4.09s/it]\u001b[A\n",
      " 30%|████████████▊                              | 22/74 [01:33<04:00,  4.63s/it]\u001b[A\n",
      " 31%|█████████████▎                             | 23/74 [01:37<03:51,  4.55s/it]\u001b[A\n",
      " 32%|█████████████▉                             | 24/74 [01:40<03:29,  4.19s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 25/74 [01:43<03:00,  3.69s/it]\u001b[A\n",
      " 35%|███████████████                            | 26/74 [01:49<03:28,  4.35s/it]\u001b[A\n",
      " 36%|███████████████▋                           | 27/74 [01:54<03:32,  4.52s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 28/74 [01:58<03:20,  4.36s/it]\u001b[A\n",
      " 39%|████████████████▊                          | 29/74 [02:03<03:28,  4.64s/it]\u001b[A\n",
      " 41%|█████████████████▍                         | 30/74 [02:08<03:34,  4.87s/it]\u001b[A\n",
      " 42%|██████████████████                         | 31/74 [02:12<03:10,  4.43s/it]\u001b[A\n",
      " 43%|██████████████████▌                        | 32/74 [02:16<03:01,  4.31s/it]\u001b[A\n",
      " 45%|███████████████████▏                       | 33/74 [02:19<02:50,  4.15s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 34/74 [02:23<02:33,  3.84s/it]\u001b[A\n",
      " 47%|████████████████████▎                      | 35/74 [02:28<02:47,  4.29s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 36/74 [02:30<02:23,  3.77s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 37/74 [02:34<02:16,  3.69s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 38/74 [02:39<02:30,  4.18s/it]\u001b[A\n",
      " 53%|██████████████████████▋                    | 39/74 [02:42<02:14,  3.85s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 40/74 [02:48<02:32,  4.47s/it]\u001b[A\n",
      " 55%|███████████████████████▊                   | 41/74 [02:52<02:16,  4.13s/it]\u001b[A\n",
      " 57%|████████████████████████▍                  | 42/74 [02:55<02:01,  3.81s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 43/74 [03:04<02:48,  5.42s/it]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 44/74 [03:07<02:23,  4.77s/it]\u001b[A\n",
      " 61%|██████████████████████████▏                | 45/74 [03:14<02:39,  5.51s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 46/74 [03:19<02:23,  5.12s/it]\u001b[A\n",
      " 64%|███████████████████████████▎               | 47/74 [03:22<02:02,  4.55s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 48/74 [03:25<01:48,  4.18s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 49/74 [03:31<01:54,  4.57s/it]\u001b[A\n",
      " 68%|█████████████████████████████              | 50/74 [03:36<01:57,  4.91s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 51/74 [03:41<01:51,  4.84s/it]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 52/74 [03:48<02:02,  5.57s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 53/74 [03:51<01:42,  4.87s/it]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 54/74 [03:54<01:25,  4.29s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 55/74 [03:59<01:25,  4.48s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 56/74 [04:05<01:29,  4.97s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 57/74 [04:11<01:26,  5.11s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 58/74 [04:18<01:29,  5.62s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 59/74 [04:21<01:14,  4.94s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 60/74 [04:28<01:16,  5.49s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 61/74 [04:31<01:02,  4.80s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 62/74 [04:36<00:58,  4.84s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 63/74 [04:39<00:46,  4.23s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████▏     | 64/74 [04:44<00:46,  4.63s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 65/74 [04:50<00:43,  4.81s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████▎    | 66/74 [04:53<00:36,  4.54s/it]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 67/74 [04:57<00:30,  4.33s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 68/74 [05:03<00:27,  4.65s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 69/74 [05:07<00:23,  4.64s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [05:11<00:17,  4.40s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 71/74 [05:15<00:12,  4.12s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 72/74 [05:18<00:07,  3.99s/it]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 73/74 [05:22<00:03,  3.92s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.826321005821228, 'eval_rouge1': 48.4829, 'eval_rouge2': 31.9076, 'eval_rougeL': 39.42, 'eval_rougeLsum': 44.1494, 'eval_gen_len': 83.05838424983028, 'eval_runtime': 336.168, 'eval_samples_per_second': 4.382, 'eval_steps_per_second': 0.22, 'epoch': 2.85}\n",
      " 57%|██████████████████████▏                | 4000/7025 [38:00<13:45,  3.66it/s]\n",
      "100%|███████████████████████████████████████████| 74/74 [05:31<00:00,  3.64s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2814] 2023-04-14 12:45:35,389 >> Saving model checkpoint to ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-4000\n",
      "[INFO|configuration_utils.py:457] 2023-04-14 12:45:35,394 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-4000/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-14 12:45:35,403 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-4000/generation_config.json\n",
      "[INFO|modeling_utils.py:1762] 2023-04-14 12:45:38,274 >> Model weights saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-04-14 12:45:38,283 >> tokenizer config file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-14 12:45:38,289 >> Special tokens file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.4271, 'learning_rate': 5.765124555160143e-06, 'epoch': 3.56}         \n",
      " 71%|███████████████████████████▊           | 5000/7025 [42:30<08:38,  3.91it/s][INFO|trainer.py:3068] 2023-04-14 12:50:05,201 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3070] 2023-04-14 12:50:05,201 >>   Num examples = 1473\n",
      "[INFO|trainer.py:3073] 2023-04-14 12:50:05,201 >>   Batch size = 20\n",
      "\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/74 [00:03<01:51,  1.55s/it]\u001b[A\n",
      "  4%|█▊                                          | 3/74 [00:07<03:09,  2.67s/it]\u001b[A\n",
      "  5%|██▍                                         | 4/74 [00:10<03:30,  3.01s/it]\u001b[A\n",
      "  7%|██▉                                         | 5/74 [00:15<04:00,  3.48s/it]\u001b[A\n",
      "  8%|███▌                                        | 6/74 [00:17<03:34,  3.16s/it]\u001b[A\n",
      "  9%|████▏                                       | 7/74 [00:20<03:16,  2.93s/it]\u001b[A\n",
      " 11%|████▊                                       | 8/74 [00:26<04:21,  3.96s/it]\u001b[A\n",
      " 12%|█████▎                                      | 9/74 [00:29<04:07,  3.81s/it]\u001b[A\n",
      " 14%|█████▊                                     | 10/74 [00:33<03:51,  3.62s/it]\u001b[A\n",
      " 15%|██████▍                                    | 11/74 [00:39<04:49,  4.59s/it]\u001b[A\n",
      " 16%|██████▉                                    | 12/74 [00:43<04:27,  4.31s/it]\u001b[A\n",
      " 18%|███████▌                                   | 13/74 [00:46<04:05,  4.03s/it]\u001b[A\n",
      " 19%|████████▏                                  | 14/74 [00:52<04:22,  4.38s/it]\u001b[A\n",
      " 20%|████████▋                                  | 15/74 [00:56<04:14,  4.31s/it]\u001b[A\n",
      " 22%|█████████▎                                 | 16/74 [01:00<04:04,  4.21s/it]\u001b[A\n",
      " 23%|█████████▉                                 | 17/74 [01:06<04:29,  4.73s/it]\u001b[A\n",
      " 24%|██████████▍                                | 18/74 [01:09<03:56,  4.21s/it]\u001b[A\n",
      " 26%|███████████                                | 19/74 [01:12<03:39,  4.00s/it]\u001b[A\n",
      " 27%|███████████▌                               | 20/74 [01:16<03:38,  4.05s/it]\u001b[A\n",
      " 28%|████████████▏                              | 21/74 [01:20<03:27,  3.91s/it]\u001b[A\n",
      " 30%|████████████▊                              | 22/74 [01:25<03:40,  4.25s/it]\u001b[A\n",
      " 31%|█████████████▎                             | 23/74 [01:29<03:32,  4.17s/it]\u001b[A\n",
      " 32%|█████████████▉                             | 24/74 [01:33<03:18,  3.97s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 25/74 [01:35<02:56,  3.59s/it]\u001b[A\n",
      " 35%|███████████████                            | 26/74 [01:40<03:05,  3.86s/it]\u001b[A\n",
      " 36%|███████████████▋                           | 27/74 [01:45<03:24,  4.36s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 28/74 [01:50<03:20,  4.37s/it]\u001b[A\n",
      " 39%|████████████████▊                          | 29/74 [01:54<03:18,  4.42s/it]\u001b[A\n",
      " 41%|█████████████████▍                         | 30/74 [01:59<03:25,  4.68s/it]\u001b[A\n",
      " 42%|██████████████████                         | 31/74 [02:03<03:05,  4.31s/it]\u001b[A\n",
      " 43%|██████████████████▌                        | 32/74 [02:08<03:11,  4.56s/it]\u001b[A\n",
      " 45%|███████████████████▏                       | 33/74 [02:13<03:09,  4.62s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 34/74 [02:15<02:40,  4.02s/it]\u001b[A\n",
      " 47%|████████████████████▎                      | 35/74 [02:20<02:41,  4.14s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 36/74 [02:24<02:36,  4.12s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 37/74 [02:29<02:44,  4.45s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 38/74 [02:34<02:42,  4.52s/it]\u001b[A\n",
      " 53%|██████████████████████▋                    | 39/74 [02:37<02:26,  4.18s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 40/74 [02:42<02:29,  4.39s/it]\u001b[A\n",
      " 55%|███████████████████████▊                   | 41/74 [02:45<02:10,  3.96s/it]\u001b[A\n",
      " 57%|████████████████████████▍                  | 42/74 [02:48<01:54,  3.59s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 43/74 [02:53<02:09,  4.17s/it]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 44/74 [02:57<02:01,  4.03s/it]\u001b[A\n",
      " 61%|██████████████████████████▏                | 45/74 [03:02<02:03,  4.25s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 46/74 [03:06<02:00,  4.30s/it]\u001b[A\n",
      " 64%|███████████████████████████▎               | 47/74 [03:10<01:48,  4.02s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 48/74 [03:13<01:41,  3.90s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 49/74 [03:19<01:49,  4.36s/it]\u001b[A\n",
      " 68%|█████████████████████████████              | 50/74 [03:23<01:47,  4.48s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 51/74 [03:29<01:50,  4.81s/it]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 52/74 [03:36<02:03,  5.60s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 53/74 [03:39<01:40,  4.77s/it]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 54/74 [03:45<01:40,  5.03s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 55/74 [03:49<01:31,  4.82s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 56/74 [03:54<01:28,  4.89s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 57/74 [04:00<01:28,  5.23s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 58/74 [04:05<01:20,  5.01s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 59/74 [04:08<01:06,  4.45s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 60/74 [04:31<02:19,  9.93s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 61/74 [04:34<01:42,  7.87s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 62/74 [04:38<01:22,  6.91s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 63/74 [04:41<01:02,  5.67s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████▏     | 64/74 [04:46<00:54,  5.48s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 65/74 [04:51<00:47,  5.26s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████▎    | 66/74 [04:54<00:36,  4.61s/it]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 67/74 [04:58<00:31,  4.48s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 68/74 [05:02<00:25,  4.22s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 69/74 [05:06<00:21,  4.29s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [05:11<00:17,  4.35s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 71/74 [05:14<00:12,  4.04s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 72/74 [05:18<00:07,  3.94s/it]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 73/74 [05:22<00:04,  4.11s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8554127216339111, 'eval_rouge1': 48.5398, 'eval_rouge2': 31.7864, 'eval_rougeL': 39.3932, 'eval_rougeLsum': 44.0806, 'eval_gen_len': 83.061099796334, 'eval_runtime': 336.6342, 'eval_samples_per_second': 4.376, 'eval_steps_per_second': 0.22, 'epoch': 3.56}\n",
      " 71%|███████████████████████████▊           | 5000/7025 [48:07<08:38,  3.91it/s]\n",
      "100%|███████████████████████████████████████████| 74/74 [05:32<00:00,  3.98s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2814] 2023-04-14 12:55:41,840 >> Saving model checkpoint to ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-5000\n",
      "[INFO|configuration_utils.py:457] 2023-04-14 12:55:41,847 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-5000/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-14 12:55:41,858 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-5000/generation_config.json\n",
      "[INFO|modeling_utils.py:1762] 2023-04-14 12:55:44,719 >> Model weights saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-04-14 12:55:44,727 >> tokenizer config file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-14 12:55:44,733 >> Special tokens file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.3848, 'learning_rate': 2.918149466192171e-06, 'epoch': 4.27}         \n",
      " 85%|█████████████████████████████████▎     | 6000/7025 [52:35<04:24,  3.88it/s][INFO|trainer.py:3068] 2023-04-14 13:00:10,132 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3070] 2023-04-14 13:00:10,132 >>   Num examples = 1473\n",
      "[INFO|trainer.py:3073] 2023-04-14 13:00:10,132 >>   Batch size = 20\n",
      "\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/74 [00:03<02:02,  1.70s/it]\u001b[A\n",
      "  4%|█▊                                          | 3/74 [00:08<03:30,  2.97s/it]\u001b[A\n",
      "  5%|██▍                                         | 4/74 [00:11<03:34,  3.06s/it]\u001b[A\n",
      "  7%|██▉                                         | 5/74 [00:16<04:17,  3.73s/it]\u001b[A\n",
      "  8%|███▌                                        | 6/74 [00:19<03:52,  3.42s/it]\u001b[A\n",
      "  9%|████▏                                       | 7/74 [00:22<03:45,  3.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11%|████▊                                       | 8/74 [00:28<04:46,  4.35s/it]\u001b[A\n",
      " 12%|█████▎                                      | 9/74 [00:33<04:40,  4.31s/it]\u001b[A\n",
      " 14%|█████▊                                     | 10/74 [00:37<04:43,  4.43s/it]\u001b[A\n",
      " 15%|██████▍                                    | 11/74 [00:43<04:58,  4.74s/it]\u001b[A\n",
      " 16%|██████▉                                    | 12/74 [00:48<04:57,  4.80s/it]\u001b[A\n",
      " 18%|███████▌                                   | 13/74 [00:53<04:53,  4.81s/it]\u001b[A\n",
      " 19%|████████▏                                  | 14/74 [00:57<04:48,  4.82s/it]\u001b[A\n",
      " 20%|████████▋                                  | 15/74 [01:01<04:27,  4.54s/it]\u001b[A\n",
      " 22%|█████████▎                                 | 16/74 [01:07<04:39,  4.82s/it]\u001b[A\n",
      " 23%|█████████▉                                 | 17/74 [01:12<04:38,  4.89s/it]\u001b[A\n",
      " 24%|██████████▍                                | 18/74 [01:15<03:57,  4.24s/it]\u001b[A\n",
      " 26%|███████████                                | 19/74 [01:18<03:44,  4.09s/it]\u001b[A\n",
      " 27%|███████████▌                               | 20/74 [01:23<03:50,  4.26s/it]\u001b[A\n",
      " 28%|████████████▏                              | 21/74 [01:28<03:57,  4.47s/it]\u001b[A\n",
      " 30%|████████████▊                              | 22/74 [01:31<03:35,  4.14s/it]\u001b[A\n",
      " 31%|█████████████▎                             | 23/74 [01:37<03:55,  4.62s/it]\u001b[A\n",
      " 32%|█████████████▉                             | 24/74 [01:41<03:36,  4.34s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 25/74 [01:43<03:06,  3.81s/it]\u001b[A\n",
      " 35%|███████████████                            | 26/74 [01:48<03:19,  4.15s/it]\u001b[A\n",
      " 36%|███████████████▋                           | 27/74 [01:54<03:36,  4.60s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 28/74 [01:58<03:28,  4.53s/it]\u001b[A\n",
      " 39%|████████████████▊                          | 29/74 [02:03<03:29,  4.66s/it]\u001b[A\n",
      " 41%|█████████████████▍                         | 30/74 [02:09<03:42,  5.05s/it]\u001b[A\n",
      " 42%|██████████████████                         | 31/74 [02:12<03:11,  4.46s/it]\u001b[A\n",
      " 43%|██████████████████▌                        | 32/74 [02:17<03:11,  4.56s/it]\u001b[A\n",
      " 45%|███████████████████▏                       | 33/74 [02:22<03:07,  4.57s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 34/74 [02:24<02:41,  4.05s/it]\u001b[A\n",
      " 47%|████████████████████▎                      | 35/74 [02:28<02:35,  3.99s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 36/74 [02:32<02:29,  3.94s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 37/74 [02:38<02:44,  4.45s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 38/74 [02:44<03:01,  5.03s/it]\u001b[A\n",
      " 53%|██████████████████████▋                    | 39/74 [02:48<02:41,  4.61s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 40/74 [02:52<02:34,  4.56s/it]\u001b[A\n",
      " 55%|███████████████████████▊                   | 41/74 [03:05<03:56,  7.17s/it]\u001b[A\n",
      " 57%|████████████████████████▍                  | 42/74 [03:09<03:12,  6.01s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 43/74 [03:15<03:04,  5.94s/it]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 44/74 [03:18<02:39,  5.31s/it]\u001b[A\n",
      " 61%|██████████████████████████▏                | 45/74 [03:25<02:49,  5.84s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 46/74 [03:29<02:21,  5.06s/it]\u001b[A\n",
      " 64%|███████████████████████████▎               | 47/74 [03:33<02:10,  4.83s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 48/74 [03:37<01:59,  4.59s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 49/74 [03:43<02:03,  4.93s/it]\u001b[A\n",
      " 68%|█████████████████████████████              | 50/74 [03:48<02:02,  5.12s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 51/74 [03:54<01:59,  5.21s/it]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 52/74 [04:01<02:11,  5.97s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 53/74 [04:05<01:49,  5.22s/it]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 54/74 [04:08<01:31,  4.59s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 55/74 [04:12<01:24,  4.45s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 56/74 [04:18<01:28,  4.89s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 57/74 [04:24<01:28,  5.20s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 58/74 [04:28<01:19,  4.96s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 59/74 [04:32<01:08,  4.54s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 60/74 [04:40<01:20,  5.74s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 61/74 [04:44<01:04,  4.95s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 62/74 [04:48<00:58,  4.84s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 63/74 [04:51<00:46,  4.19s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████▏     | 64/74 [04:56<00:44,  4.49s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 65/74 [05:00<00:39,  4.37s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████▎    | 66/74 [05:03<00:32,  4.03s/it]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 67/74 [05:08<00:29,  4.19s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 68/74 [05:13<00:26,  4.44s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 69/74 [05:18<00:22,  4.48s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [05:24<00:20,  5.09s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 71/74 [05:28<00:13,  4.62s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 72/74 [05:31<00:08,  4.28s/it]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 73/74 [05:35<00:04,  4.16s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8451822996139526, 'eval_rouge1': 48.7455, 'eval_rouge2': 31.8768, 'eval_rougeL': 39.494, 'eval_rougeLsum': 44.2642, 'eval_gen_len': 83.6795655125594, 'eval_runtime': 349.4233, 'eval_samples_per_second': 4.216, 'eval_steps_per_second': 0.212, 'epoch': 4.27}\n",
      " 85%|█████████████████████████████████▎     | 6000/7025 [58:24<04:24,  3.88it/s]\n",
      "100%|███████████████████████████████████████████| 74/74 [05:45<00:00,  3.96s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2814] 2023-04-14 13:05:59,559 >> Saving model checkpoint to ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-6000\n",
      "[INFO|configuration_utils.py:457] 2023-04-14 13:05:59,564 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-6000/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-14 13:05:59,570 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-6000/generation_config.json\n",
      "[INFO|modeling_utils.py:1762] 2023-04-14 13:06:02,898 >> Model weights saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-04-14 13:06:02,907 >> tokenizer config file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-14 13:06:02,915 >> Special tokens file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.3372, 'learning_rate': 7.117437722419929e-08, 'epoch': 4.98}         \n",
      "100%|████████████████████████████████████▊| 7000/7025 [1:02:56<00:05,  4.27it/s][INFO|trainer.py:3068] 2023-04-14 13:10:31,322 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3070] 2023-04-14 13:10:31,322 >>   Num examples = 1473\n",
      "[INFO|trainer.py:3073] 2023-04-14 13:10:31,322 >>   Batch size = 20\n",
      "\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/74 [00:03<01:49,  1.52s/it]\u001b[A\n",
      "  4%|█▊                                          | 3/74 [00:08<03:52,  3.28s/it]\u001b[A\n",
      "  5%|██▍                                         | 4/74 [00:11<03:47,  3.25s/it]\u001b[A\n",
      "  7%|██▉                                         | 5/74 [00:18<04:54,  4.27s/it]\u001b[A\n",
      "  8%|███▌                                        | 6/74 [00:20<04:09,  3.66s/it]\u001b[A\n",
      "  9%|████▏                                       | 7/74 [00:23<03:50,  3.44s/it]\u001b[A\n",
      " 11%|████▊                                       | 8/74 [00:29<04:48,  4.37s/it]\u001b[A\n",
      " 12%|█████▎                                      | 9/74 [00:33<04:29,  4.14s/it]\u001b[A\n",
      " 14%|█████▊                                     | 10/74 [00:37<04:25,  4.14s/it]\u001b[A\n",
      " 15%|██████▍                                    | 11/74 [00:46<05:43,  5.45s/it]\u001b[A\n",
      " 16%|██████▉                                    | 12/74 [00:50<05:15,  5.08s/it]\u001b[A\n",
      " 18%|███████▌                                   | 13/74 [00:54<04:55,  4.84s/it]\u001b[A\n",
      " 19%|████████▏                                  | 14/74 [00:59<04:42,  4.70s/it]\u001b[A\n",
      " 20%|████████▋                                  | 15/74 [01:03<04:33,  4.63s/it]\u001b[A\n",
      " 22%|█████████▎                                 | 16/74 [01:07<04:15,  4.41s/it]\u001b[A\n",
      " 23%|█████████▉                                 | 17/74 [01:15<05:18,  5.58s/it]\u001b[A\n",
      " 24%|██████████▍                                | 18/74 [01:18<04:28,  4.80s/it]\u001b[A\n",
      " 26%|███████████                                | 19/74 [01:22<03:59,  4.35s/it]\u001b[A\n",
      " 27%|███████████▌                               | 20/74 [01:27<04:12,  4.68s/it]\u001b[A\n",
      " 28%|████████████▏                              | 21/74 [01:32<04:19,  4.90s/it]\u001b[A\n",
      " 30%|████████████▊                              | 22/74 [01:36<03:55,  4.53s/it]\u001b[A\n",
      " 31%|█████████████▎                             | 23/74 [01:41<03:52,  4.55s/it]\u001b[A\n",
      " 32%|█████████████▉                             | 24/74 [01:44<03:29,  4.19s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 25/74 [01:46<02:57,  3.61s/it]\u001b[A\n",
      " 35%|███████████████                            | 26/74 [01:55<04:09,  5.21s/it]\u001b[A\n",
      " 36%|███████████████▋                           | 27/74 [02:00<03:52,  4.95s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 28/74 [02:03<03:25,  4.47s/it]\u001b[A\n",
      " 39%|████████████████▊                          | 29/74 [02:08<03:30,  4.69s/it]\u001b[A\n",
      " 41%|█████████████████▍                         | 30/74 [02:13<03:23,  4.62s/it]\u001b[A\n",
      " 42%|██████████████████                         | 31/74 [02:17<03:11,  4.46s/it]\u001b[A\n",
      " 43%|██████████████████▌                        | 32/74 [02:20<02:58,  4.26s/it]\u001b[A\n",
      " 45%|███████████████████▏                       | 33/74 [02:28<03:35,  5.25s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 34/74 [02:31<03:05,  4.65s/it]\u001b[A\n",
      " 47%|████████████████████▎                      | 35/74 [02:36<03:05,  4.76s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 36/74 [02:41<03:00,  4.75s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 37/74 [02:49<03:33,  5.78s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 38/74 [02:53<03:10,  5.29s/it]\u001b[A\n",
      " 53%|██████████████████████▋                    | 39/74 [02:57<02:45,  4.73s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 40/74 [03:01<02:39,  4.70s/it]\u001b[A\n",
      " 55%|███████████████████████▊                   | 41/74 [03:05<02:24,  4.38s/it]\u001b[A\n",
      " 57%|████████████████████████▍                  | 42/74 [03:08<02:06,  3.94s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 43/74 [03:14<02:20,  4.54s/it]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 44/74 [03:17<02:01,  4.03s/it]\u001b[A\n",
      " 61%|██████████████████████████▏                | 45/74 [03:24<02:22,  4.91s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 46/74 [03:29<02:19,  4.99s/it]\u001b[A\n",
      " 64%|███████████████████████████▎               | 47/74 [03:33<02:07,  4.73s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 48/74 [03:37<01:57,  4.50s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 49/74 [03:46<02:29,  5.98s/it]\u001b[A\n",
      " 68%|█████████████████████████████              | 50/74 [03:51<02:15,  5.63s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 51/74 [03:56<02:01,  5.30s/it]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 52/74 [04:06<02:26,  6.67s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 53/74 [04:10<02:02,  5.85s/it]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 54/74 [04:13<01:41,  5.07s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 55/74 [04:16<01:26,  4.57s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 56/74 [04:21<01:25,  4.77s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 57/74 [04:27<01:27,  5.14s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 58/74 [04:32<01:20,  5.06s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 59/74 [04:36<01:07,  4.53s/it]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 60/74 [04:45<01:22,  5.91s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 61/74 [04:48<01:06,  5.10s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 62/74 [04:52<00:56,  4.75s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 63/74 [04:55<00:46,  4.25s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████▏     | 64/74 [05:01<00:48,  4.87s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 65/74 [05:06<00:43,  4.80s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████▎    | 66/74 [05:10<00:35,  4.49s/it]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 67/74 [05:14<00:30,  4.32s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 68/74 [05:19<00:28,  4.73s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 69/74 [05:23<00:22,  4.57s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [05:28<00:17,  4.42s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 71/74 [05:31<00:12,  4.11s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 72/74 [05:35<00:08,  4.12s/it]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 73/74 [05:39<00:04,  4.01s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8598814010620117, 'eval_rouge1': 48.511, 'eval_rouge2': 31.7694, 'eval_rougeL': 39.3033, 'eval_rougeLsum': 44.0855, 'eval_gen_len': 84.02104548540393, 'eval_runtime': 353.0278, 'eval_samples_per_second': 4.172, 'eval_steps_per_second': 0.21, 'epoch': 4.98}\n",
      "100%|████████████████████████████████████▊| 7000/7025 [1:08:49<00:05,  4.27it/s]\n",
      "100%|███████████████████████████████████████████| 74/74 [05:49<00:00,  3.82s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2814] 2023-04-14 13:16:24,354 >> Saving model checkpoint to ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-7000\n",
      "[INFO|configuration_utils.py:457] 2023-04-14 13:16:24,363 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-7000/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-14 13:16:24,369 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-7000/generation_config.json\n",
      "[INFO|modeling_utils.py:1762] 2023-04-14 13:16:27,225 >> Model weights saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-04-14 13:16:27,230 >> tokenizer config file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-14 13:16:27,231 >> Special tokens file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/checkpoint-7000/special_tokens_map.json\n",
      "100%|█████████████████████████████████████| 7025/7025 [1:09:04<00:00,  3.87it/s][INFO|trainer.py:2012] 2023-04-14 13:16:39,684 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 4151.8418, 'train_samples_per_second': 33.827, 'train_steps_per_second': 1.692, 'train_loss': 1.5809268986712146, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████| 7025/7025 [1:09:04<00:00,  1.69it/s]\n",
      "[INFO|trainer.py:2814] 2023-04-14 13:16:39,694 >> Saving model checkpoint to ../_common/bart_response_generation/bart_cnn_science_4ep_2e05\n",
      "[INFO|configuration_utils.py:457] 2023-04-14 13:16:39,717 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-14 13:16:39,722 >> Configuration saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1762] 2023-04-14 13:16:42,960 >> Model weights saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-04-14 13:16:42,971 >> tokenizer config file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-14 13:16:42,978 >> Special tokens file saved in ../_common/bart_response_generation/bart_cnn_science_4ep_2e05/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     1.5809\n",
      "  train_runtime            = 1:09:11.84\n",
      "  train_samples            =      28089\n",
      "  train_samples_per_second =     33.827\n",
      "  train_steps_per_second   =      1.692\n",
      "04/14/2023 13:16:43 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3068] 2023-04-14 13:16:43,083 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3070] 2023-04-14 13:16:43,083 >>   Num examples = 1473\n",
      "[INFO|trainer.py:3073] 2023-04-14 13:16:43,083 >>   Batch size = 20\n",
      "100%|███████████████████████████████████████████| 74/74 [05:42<00:00,  4.62s/it]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_gen_len            =    84.0475\n",
      "  eval_loss               =       1.86\n",
      "  eval_rouge1             =    48.4735\n",
      "  eval_rouge2             =    31.7505\n",
      "  eval_rougeL             =    39.2779\n",
      "  eval_rougeLsum          =    44.0887\n",
      "  eval_runtime            = 0:05:45.93\n",
      "  eval_samples            =       1473\n",
      "  eval_samples_per_second =      4.258\n",
      "  eval_steps_per_second   =      0.214\n",
      "[INFO|modelcard.py:451] 2023-04-14 13:22:29,265 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 48.4735}]}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/gen_len ▁▃▄▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▃█▁▆▄▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge1 ▁▁▇▅▆█▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge2 ▁▂█▆▃▅▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rougeL ▆▁█▆▅▇▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeLsum ▁▁▆▆▅█▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▁▄▃▆▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second █▄▆▂▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second █▄▆▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▃▃▄▄▆▆▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▃▃▄▄▆▆▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▇▆▅▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▆▄▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/gen_len 84.04752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 1.86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge1 48.4735\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge2 31.7505\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rougeL 39.2779\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeLsum 44.0887\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 345.9321\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 4.258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.214\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 7025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 1.3372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 9.103490476341658e+16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.58093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 4151.8418\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 33.827\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbart_cnn_science_4ep_2e05\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cruel-no/bart_responces/runs/kroz5pbl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230414_120729-kroz5pbl/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python custom_bart_scripts/run_response_generation.py \\\n",
    "    --model_name_or_path=\"theojolliffe/bart-cnn-science\" \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --evaluation_strategy=\"steps\" \\\n",
    "    --weight_decay=0.01 \\\n",
    "    --logging_steps=1000 \\\n",
    "    --save_steps=1000 \\\n",
    "    --run_name=\"bart_cnn_science_4ep_2e05\" \\\n",
    "    --train_file=\"../_common/datasets/bart_response_data/train_ft_deberta.csv\" \\\n",
    "    --validation_file=\"../_common/datasets/bart_response_data/valid_ft_deberta.csv\" \\\n",
    "    --output_dir=\"../_common/bart_response_generation/bart_cnn_science_4ep_2e05\" \\\n",
    "    --per_device_train_batch_size=20 \\\n",
    "    --per_device_eval_batch_size=20 \\\n",
    "    --max_target_length=512 \\\n",
    "    --learning_rate=2e-05 \\\n",
    "    --num_train_epochs=5 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bart_cnn_science_4ep_2e05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = f\"/home/jovyan/chatbot/_common/bart_response_generation/{model_name}/checkpoint-4000\"\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "tokenizer = BartTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_top(text, num_beams=4,  max_source_len=512, max_target_length=700, top_k=50, top_p=1):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    input_tensor = inputs[\"input_ids\"]\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        input_tensor,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=512\n",
    "    )\n",
    "    pred = tokenizer.batch_decode(\n",
    "        summary_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(s):\n",
    "    for t in special_tokens_list:\n",
    "        s = s.replace(t, '')\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_list = []\n",
    "for el in tokenizer.special_tokens_map.values():\n",
    "    if isinstance(el, list):\n",
    "        special_tokens_list.extend(el)\n",
    "    else:\n",
    "        special_tokens_list.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../_common/datasets/dialogpt/val_v2.txt', 'r') as f:\n",
    "    test_data = f.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS> What is V2V-PoseNet? <SEP> First, we convert 2D depth images to 3D volumetric forms by reprojecting the points in the 3D space and discretizing the continuous space. After voxelizing the 2D depth image, the V2V-PoseNet takes the 3D voxelized data as an input and estimates the per-voxel likelihood for each keypoint. The position of the highest likelihood response for each keypoint is identified and warped to the real world coordinate, which becomes the final result of our model. <SEP> <INPUTEND> V2V-PoseNet is a model designed for 3D pose estimation, which takes voxelized 3D data as input and estimates the per-voxel likelihood for each keypoint. It includes four kinds of building blocks: volumetric basic block, volumetric residual block, volumetric downsampling block, and volumetric upsampling block. <EOS>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1473/1473 [1:01:38<00:00,  2.51s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for text in tqdm(test_data):\n",
    "    inp, target = text.split(' <INPUTEND> ')\n",
    "    inp = inp.replace(\"<BOS> \", \"\").replace(\" <EOS>\", \"\")\n",
    "    target = target.replace(\"<BOS> \", \"\").replace(\" <EOS>\", \"\")\n",
    "    pred = generate_top(inp)\n",
    "    predictions.append({'input': inp,\n",
    "                        'target': remove_special_tokens(target),\n",
    "                        'prediction': remove_special_tokens(pred)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1473"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../_common/bart_response_generation/predictions_{model_name}.pkl\", 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate LM scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_paths = sorted([f for f in os.listdir('../_common/bart_response_generation') if \".pkl\" in f and \"ft\" in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predictions_large_ft_deberta_3ep_5e5.pkl',\n",
       " 'predictions_large_ft_deberta_3ep_5e5_16bs.pkl',\n",
       " 'predictions_large_ft_deberta_5ep_1e5_20bs_wr01.pkl',\n",
       " 'predictions_large_ft_deberta_5ep_8e5_20bs_ch4000.pkl']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_large_ft_deberta_3ep_5e5.pkl\n",
      "\n",
      "ROUGE-1: 51.72\n",
      "ROUGE-2: 36.25\n",
      "ROUGE-L: 49.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/chatbot_bart_37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/user/conda/envs/chatbot_bart_37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/user/conda/envs/chatbot_bart_37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 42.33\n",
      "BLEU-2: 17.83\n",
      "BLEU-3: 9.21\n",
      "BLEU-4: 5.89\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "predictions_large_ft_deberta_3ep_5e5_16bs.pkl\n",
      "\n",
      "ROUGE-1: 52.78\n",
      "ROUGE-2: 37.49\n",
      "ROUGE-L: 51.21\n",
      "\n",
      "BLEU-1: 43.73\n",
      "BLEU-2: 19.83\n",
      "BLEU-3: 10.82\n",
      "BLEU-4: 7.16\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "predictions_large_ft_deberta_5ep_1e5_20bs_wr01.pkl\n",
      "\n",
      "ROUGE-1: 51.79\n",
      "ROUGE-2: 36.51\n",
      "ROUGE-L: 50.11\n",
      "\n",
      "BLEU-1: 42.36\n",
      "BLEU-2: 18.41\n",
      "BLEU-3: 9.64\n",
      "BLEU-4: 6.14\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "predictions_large_ft_deberta_5ep_8e5_20bs_ch4000.pkl\n",
      "\n",
      "ROUGE-1: 51.71\n",
      "ROUGE-2: 36.37\n",
      "ROUGE-L: 49.93\n",
      "\n",
      "BLEU-1: 42.52\n",
      "BLEU-2: 17.92\n",
      "BLEU-3: 9.15\n",
      "BLEU-4: 5.69\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for res_path in results_paths:\n",
    "    print(res_path)\n",
    "    with open('../_common/bart_response_generation/' + res_path, 'rb') as f:\n",
    "        preds = pickle.load(f)\n",
    "        \n",
    "    rouge = Rouge(metrics=['rouge-n', 'rouge-l'],\n",
    "                       max_n=2,\n",
    "                       limit_length=False,\n",
    "                       length_limit=3,\n",
    "                       length_limit_type='words',\n",
    "                       apply_avg=True,\n",
    "                       apply_best=False,\n",
    "                       alpha=0.5, # Default F1_score\n",
    "                       weight_factor=1.2,\n",
    "                       stemming=False)\n",
    "    \n",
    "    hyps, refs = [], []\n",
    "    for i in range(len(preds)):\n",
    "        hyps.append(preds[i]['prediction'])\n",
    "        refs.append(preds[i]['target'])\n",
    "        \n",
    "    gen_ref = zip(hyps, refs)\n",
    "    gen_ref = [_ for _ in gen_ref if not all(j in string.punctuation for j in _[1]) and not all(j in string.punctuation for j in _[0])]\n",
    "    gens, refs  = zip(*gen_ref)\n",
    "    \n",
    "    #rouge_res = rouge.get_scores(gens, refs, avg=True, ignore_empty=False) #python-rouge\n",
    "    rouge_res = rouge.get_scores(gens, refs) # py-rouge\n",
    "    print()\n",
    "    print('ROUGE-1:', round(100 * rouge_res['rouge-1']['f'], 2))\n",
    "    print('ROUGE-2:', round(100 * rouge_res['rouge-2']['f'], 2))\n",
    "    print('ROUGE-L:', round(100 * rouge_res['rouge-l']['f'], 2))\n",
    "    \n",
    "    print()\n",
    "    for j in range(1, 5):\n",
    "        weights=[0,0,0,0]\n",
    "        for k in range(j):\n",
    "            weights[k] = 1\n",
    "        mean_bleu = 0\n",
    "        for gen, ref in zip(gens, refs):\n",
    "            mean_bleu += sentence_bleu([word_tokenize(ref)], word_tokenize(gen), weights=weights)\n",
    "        mean_bleu /= len(gens)\n",
    "        print(f'BLEU-{j}:', round(100 * mean_bleu, 2))  \n",
    "    \n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
