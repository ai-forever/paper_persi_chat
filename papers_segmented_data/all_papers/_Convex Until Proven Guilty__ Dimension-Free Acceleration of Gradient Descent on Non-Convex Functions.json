{"title": "\"Convex Until Proven Guilty\": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions", "paper_id": "672977bfbb94e115cc778c32b3b0fff7d09102b8", "segments": [["672977bfbb94e115cc778c32b3b0fff7d09102b8_0", "We develop and analyze a variant of Nesterov's accelerated gradient descent (AGD) for minimization of smooth non-convex functions. We prove that one of two cases occurs: either our AGD variant converges quickly, as if the function was convex, or we produce a certificate that the function is \"guilty\" of being non-convex. This non-convexity certificate allows us to exploit negative curvature and obtain deterministic, dimension-free acceleration of convergence for non-convex functions. For a function f with Lipschitz continuous gradient and Hessian, we compute a point x with log(1/\u270f)) gradient and function evaluations. Assuming additionally that the third derivative is Lipschitz, we require only O(\u270f 5/3 log(1/\u270f)) evaluations.", "abstract", "abstract"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_1", "Nesterov's seminal 1983 accelerated gradient method has inspired substantial development of first-order methods for large-scale convex optimization. In recent years, machine learning and statistics have seen a shift toward large scale non-convex problems, including methods for matrix completion (Koren et al., 2009), phase retrieval Wang et al., 2016), dictionary learning (Mairal et al., 2008), and neural network training (LeCun et al., 2015). In practice, techniques from accelerated gradient methods-namely, momentum-can have substantial benefits for stochastic gradient methods, for example, in training neural networks (Rumelhart et al., 1986;Kingma and Ba, 2015). Yet little of the rich theory of acceleration for convex optimization is known to transfer into non-convex optimization.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nOptimization becomes more difficult without convexity, as gradients no longer provide global information about the function. Even determining if a stationary point is a local minimum is (generally) NP-hard (Murty and Kabadi, 1987;Nesterov, 2000). It is, however, possible to leverage non-convexity to improve objectives in smooth optimization: moving in directions of negative curvature can guarantee function value reduction. We explore the interplay between negative curvature, smoothness, and acceleration techniques, showing how an understanding of the three simultaneously yields a method that provably accelerates convergence of gradient descent for a broad class of non-convex functions.", "Introduction", "introduction"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_2", "We consider the unconstrained minimization problem\nminimize x f (x),(1)\nwhere f : R d ! R is smooth but potentially non-convex. We assume throughout the paper that f is bounded from below, two-times differentiable, and has Lipschitz continuous gradient and Hessian. In Section 4 we strengthen our results under the additional assumption that f has Lipschitz continuous third derivatives. Following the standard first-order oracle model (Nemirovski and Yudin, 1983), we consider optimization methods that access only values and gradients of f (and not higher order derivatives), and we measure their complexity by the total number of gradient and function evaluations.\nApproximating the global minimum of f to \u270f-accuracy is generally intractable, requiring time exponential in d log 1 \u270f (Nemirovski and Yudin, 1983, \u00a71.6). Instead, we seek a point x that is \u270f-approximately stationary, that is, krf (x)k \uf8ff \u270f.\n(2)\nFinding stationary points is a canonical problem in nonlinear optimization (Nocedal and Wright, 2006), and while saddle points and local maxima are stationary, excepting pathological cases, descent methods that converge to a stationary point converge to a local minimum (Lee et al., 2016;Nemirovski, 1999, \u00a73.2.2).\nIf we assume f is convex, gradient descent satisfies the bound (2) after O(\u270f 1 ) gradient evaluations, and AGD improves this rate to O(\u270f 1/2 log 1 \u270f ) (Nesterov, 2012). Without convexity, gradient descent is significantly worse, having worst-case complexity \u21e5(\u270f 2) (Cartis et al., 2010). More sophisticated gradient-based methods, including nonlinear conjugate gradient (Hager and Zhang, 2006) and L-BFGS (Liu and Nocedal, 1989) provide excellent practical performance, but their global convergence guarantees are no better than O(\u270f 2\n). Our work (Carmon et al., 2016) and, independently, Agarwal et al. (2016), break this O(\u270f 2 ) barrier, obtaining the rate O(\u270f 7/4 log d \u270f ). Before we discuss this line of work in Section 1.3, we overview our contributions.\n1.2. Our contributions \"Convex until proven guilty\" Underpinning our results is the observation that when we run Nesterov's accelerated gradient descent (AGD) on any smooth function f , one of two outcomes must follow:\n(a) AGD behaves as though f was -strongly convex, satisfying inequality (2\n) in O( 1/2 log 1 \u270f ) iterations. (b) There exist points u, v in the AGD trajectory that prove f is \"guilty\" of not being -strongly convex, f (u) < f(v) + rf (v) T (u v) + 2 ku vk 2 . (3)\nThe intuition behind these observations is that if inequality (3) never holds during the iterations of AGD, then f \"looks\" strongly convex, and the convergence (a) follows.\nIn Section 2 we make this observation precise, presenting an algorithm to monitor AGD and quickly find the witness pair u, v satisfying (3) whenever AGD progresses more slowly than it does on strongly convex functions. We believe there is potential to apply this strategy beyond AGD, extending additional convex gradient methods to non-convex settings.\nAn accelerated non-convex gradient method In Section 3 we propose a method that iteratively applies our monitored AGD algorithm to f augmented by a proximal regularizer. We show that both outcomes (a) and (b) above imply progress minimizing f , where in case (b) we make explicit use of the negative curvature that AGD exposes. These progress guarantees translate to an overall first-order oracle complexity of O(\u270f 7/4 log 1 \u270f ), a strict improvement over the O(\u270f 2\n) rate of gradient descent. In Section 5 we report preliminary experimental results, showing a basic implementation of our method outperforms gradient descent but not nonlinear conjugate gradient.\nImproved guarantees with third-order smoothness As we show in Section 4, assuming Lipschitz continuous third derivatives instead of Lipschitz continuous Hessian allows us to increase the step size we take when exploiting negative curvature, making more function progress. Consequently, the complexity of our method improves to O(\u270f 5/3 log 1 \u270f ). While the analysis of the third-order setting is more complex, the method remains essentially unchanged. In particular, we still use only first-order information, never computing higher-order derivatives. Nesterov and Polyak (2006) show that cubic regularization of Newton's method finds a point that satisfies the sta-", "Problem setting", "introduction"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_3", "tionarity condition (2) in O(\u270f 3/2\n) evaluations of the Hessian. Given sufficiently accurate arithmetic operations, a Lipschitz continuous Hessian is approximable to arbitrary precision using finite gradient differences, and obtaining a full Hessian requires O(d) gradient evaluations. A direct implementation of the Nesterov-Polyak method with a first-order oracle therefore has gradient evaluation complexity O(\u270f 3/2 d), improving on gradient descent only if d \u2327 \u270f 1/2 , which may fail in high-dimensions.\nIn two recent papers, we (Carmon et al., 2016) and (independently) Agarwal et al. obtain better rates for first-order methods. Agarwal et al. (2016) propose a careful implementation of the Nesterov-Polyak method, using accelerated methods for fast approximate matrix inversion. In our earlier work, we employ a combination of (regularized) accelerated gradient descent and the Lanczos method. Both find a point that satisfies the bound (2) with probability at least 1 using O \u270f 7/4 log d \u270f gradient and Hessianvector product evaluations.\nThe primary conceptual difference between our approach and those of Carmon et al. and Agarwal et al. is that we perform no eigenvector search: we automatically find directions of negative curvature whenever AGD proves f \"guilty\" of non-convexity. Qualitatively, this shows that explicit second orders information is unnecessary to improve upon gradient descent for stationary point computation. Quantitatively, this leads to the following improvements:\n(i) Our result is dimension-free and deterministic, with complexity independent of the ratio d/ , compared to the log d dependence of previous works. This is significant, as log d may be comparable to \u270f 1/4 / log 1 \u270f .\n(ii) Our method uses only gradient evaluations, and does not require Hessian-vector products. In practice, Hessian-vector products may be difficult to implement and more expensive to compute than gradients.\n(iii) Under third-order smoothness assumptions we improve our method to achieve O(\u270f 5/3 log 1 \u270f ) rate. It is unclear how to extend previous approaches to obtain similar guarantees.\nIn distinction from the methods of Carmon et al. (2016) and Agarwal et al. (2016), our method provides no guarantees on positive definiteness of r 2 f (x); if initialized at a saddle point it will terminate immediately. However, as we further explain in Section D, we may combine our method with a fast eigenvector search to recover the approximate positive definiteness guarantee r 2 f (x) \u232b p \u270fI, even improving it to r 2 f (x) \u232b \u270f 2/3 I using third-order smoothness, but at the cost of reintroducing randomization, Hessian-vector products and a log d complexity term.", "Related work", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_4", "Here we introduce notation and briefly overview definitions and results we use throughout. We index sequences by subscripts, and use x j i as shorthand for x i , x i+1 , ..., x j . We use x, y, v, u, w, p, c, q and z to denote points in R d . Additionally, \u2318 denotes step sizes, \u270f, \" denote desired accuracy, \u2713 denotes a scalar and k\u2022k denotes the Euclidean norm on R d . We denote the nth derivative of a function h : R ! R by h (n) . We let log + (t) = max{0, log t}.\nA function f : R d ! R has L n -\nLipschitz nth derivative if it is n times differentiable and for every x 0 and unit vector , the one-dimensional function h(\u2713) = f (x 0 +\u2713 ) satisfies\nh (n) (\u2713 1 ) h (n) (\u2713 2 ) \uf8ff L n |\u2713 1 \u2713 2 |.\nWe refer to this property as nth-order smoothness, or simply smoothness for n = 1, where it coincides with the Lipschitz continuity of rf . Throughout the paper, we make extensive use of the well-known consequence of Taylor's theorem, that the Lipschitz constant of the nth-order derivative controls the error in the nth order Taylor series expansion of h, i.e. for \u2713, \u2713 0 2 R we have\nh(\u2713) n X i=0 1 i! h (i) (\u2713 0 )(\u2713 \u2713 0 ) i \uf8ff L n (n + 1)! |\u2713 \u2713 0 | n+1 . (4) A function f is -strongly convex if f (u) f (v) + rf (v) T (u v) + 2 ku vk 2 for all v, u 2 R d .", "Preliminaries and notation", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_5", "We begin our development by presenting the two building blocks of our result: a monitored variation of AGD (Section 2.1) and a negative curvature descent step (Section 2.2) that we use when the monitored version of AGD certifies non-convexity. In Section 3, we combine these components to obtain an accelerated method for non-convex functions.\nAlgorithm 1 AGD-UNTIL-GUILTY(f, y 0 , \", L, )\n1: Set \uf8ff L/ , ! p \uf8ff 1 p \uf8ff+1 and x 0 y 0 2: for t = 1, 2, . . . do 3: y t x t 1 1 L rf (x t 1 ) 4: x t y t + ! (y t y t 1 ) 5: w t CERTIFY-PROGRESS(f, y 0 , y t , L, , \uf8ff) 6:\nif w t 6 = NULL then . convexity violation 7:\n(u, v) FIND-WITNESS-PAIR(f, x t 0 , y t 0 , w t , )\n8: return (x t 0 , y t 0 , u, v) 9: if krf (y t )k \uf8ff \" then return (x t 0 , y t 0 , NULL) 1: function CERTIFY-PROGRESS(f , y 0 , y t , L, , \uf8ff) 2: if f (y t ) > f(y 0 ) then 3: return y 0 . non-convex behavior 4: Set z t y t 1 L rf (y t ) 5: Set (z t ) f (y 0 ) f (z t ) + 2 kz t y 0 k 2 6: if krf (y t )k 2 > 2L (z t )e t/ p \uf8ff then 7: return z t . AGD has stalled 8: else return NULL 1: function FIND-WITNESS-PAIR(f , x t 0 , y t 0 , wt , ) 2:\nfor j = 0, 1, . . . , t 1 do\n3: for u = y j , w t do 4:\nif eq. ( 8) holds with v = x j then 5:\nreturn (u, x\n(by Corollary 1 this line is never reached)", "Algorithm components", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_6", "The main component in our approach is Alg. 1, AGD-UNTIL-GUILTY. We take as input an L-smooth function f , conjectured to be -strongly convex, and optimize it with Nesterov's accelerated gradient descent method for strongly convex functions (lines 3 and 4). At every iteration, the method invokes CERTIFY-PROGRESS to test whether the optimization is progressing as it should for strongly convex functions, and in particular that the gradient norm is decreasing exponentially quickly (line 6). If the test fails, FIND-WITNESS-PAIR produces points u, v proving that f violates -strong convexity. Otherwise, we proceed until we find a point y such that krf (y)k \uf8ff \".\nThe efficacy of our method is based on the following guarantee on the performance of AGD.\nProposition 1. Let f be L-smooth, and let y t 0 and x t 0 be the sequence of iterates generated by AGD-UNTIL-GUILTY(f , y 0 , L, \", ) for some \" > 0 and 0 < \uf8ff L. Fix w 2 R d . If for s = 0, 1, . . . , t 1 we have\nf (u) f (x s ) + rf (x s ) T (u x s ) + 2 ku x s k 2 (5)\nfor both u = w and u = y s , then\nf (y t ) f (w) \uf8ff \u2713 1 1 p \uf8ff \u25c6 t (w),(6)\nwhere \uf8ff = L and (w) = f (y 0 ) f (w) + 2 kw y 0 k 2 .\nProposition 1 is essentially a restatement of established results (Nesterov, 2004;Bubeck, 2014), where we take care to phrase the requirements on f in terms of local inequalities, rather than a global strong convexity assumption. For completeness, we provide a proof of Proposition 1 in Section A.1 in the supplementary material.\nWith Proposition 1 in hand, we summarize the guarantees of Alg. 1 as follows.\nCorollary 1. Let f : R d ! R be L-smooth, let y 0 2 R d , \" > 0 and 0 < \uf8ff L. Let (x t 0 , y t 0 , u, v) = AGD-UNTIL- GUILTY(f , y 0 , \", L, ). Then the number of iterations t satisfies t \uf8ff 1 + max ( 0, r L log \u2713 2L (z t 1 ) \" 2 \u25c6 ) ,(7)\nwhere\n(z) = f (y 0 ) f (z)+ 2 kz y 0 k 2 is as in line 5 of CERTIFY-PROGRESS. If u, v 6 = NULL (non-convexity was detected), then f (u) < f(v) + rf (v) T (u v) + 2 ku vk 2 (8)\nwhere v = x j for some 0 \uf8ff j < t and u = y j or u = w t (defined on line 5 of AGD-UNTIL-GUILTY). Moreover, max{f (y 1 ), . . . , f(y t 1 ), f(u)} \uf8ff f (y 0 ).\nProof. The bound ( 7) is clear for t = 1. For t > 1, the algorithm has not terminated at iteration t 1, and so we know that neither the condition in line 9 of AGD-UNTIL-GUILTY nor the condition in line 6 of CERTIFY-PROGRESS held at iteration t 1. Thus\n\" 2 < krf (y t 1 )k 2 \uf8ff 2L (z t 1 )e (t 1)/ p \uf8ff ,\nwhich gives the bound (7) when rearranged. Now we consider the returned vectors x t 0 , y t 0 , u, and v from AGD-UNTIL-GUILTY. Note that u, v 6 = NULL only if w t 6 = NULL. Suppose that w t = y 0 , then by line 2 of CERTIFY-PROGRESS we have,\nf (y t ) f (w t ) > 0 = \u2713 1 1 p \uf8ff \u25c6 t (w t ),\nsince (w t ) = (y 0 ) = 0. Since this contradicts the progress bound (6), we obtain the certificate (8) by the contrapositive of Proposition 1: condition (5) must not hold for some 0 \uf8ff s < t, implying FIND-WITNESS-PAIR will return for some j \uf8ff s.", "AGD as a convexity monitor", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_7", "t = z t = y t 1 L rf (y t )\nthen by line 6 of CERTIFY-PROGRESS we must have\n1 2L krf (y t )k 2 > (z t )e t/ p \uf8ff \u2713 1 1 p \uf8ff \u25c6 t (z t ).\nSince f is L-smooth we have the standard progress guarantee (c.f. Nesterov (2004) \n\u00a71.2.3) f (y t ) f (z t ) 1 2L krf (y t )k 2\n, again contradicting inequality (6).\nTo see that the bound ( 9) holds, note that f (y s ) \uf8ff f (y 0 ) for s = 0, . . . , t 1 since condition 2 of CERTIFY-PROGRESS did not hold. If u = y j for some 0 \uf8ff j < t then f (u) \uf8ff f (y 0 ) holds trivially. Alternatively, if u t = w t = z t then condition 2 did not hold at time t as well, so we have f (y\nt ) \uf8ff f (y 0 ) and also f (u) = f (z t ) \uf8ff f (y t ) 1 2L krf (y t )k 2 as noted above; therefore f (z t ) \uf8ff f (y 0 ).\nBefore continuing, we make two remarks about implementation of Alg. 1.\n(1) As stated, the algorithm requires evaluation of two function gradients per iteration (at x t and y t ). Corollary 1 holds essentially unchanged if we execute line 9 of AGD-UNTIL-GUILTY and lines 4-6 of CERTIFY-PROGRESS only once every \u2327 iterations, where \u2327 is some fixed number (say 10). This reduces the number of gradient evaluations to 1 + 1 \u2327 per iteration.\n(2) Direct implementation would require O(d \u2022 t) memory to store the sequences y t 0 , x t 0 and rf (x t 0 ) for later use by FIND-WITNESS-PAIR. Alternatively, FIND-WITNESS-PAIR can regenerate these sequences from their recursive definition while iterating over j, reducing the memory requirement to O(d) and increasing the number of gradient and function evaluations by at most a factor of 2.\nIn addition, while our emphasis is on applying AGD-UNTIL-GUILTY to non-convex problems, the algorithm has implications for convex optimization. For example, we rarely know the strong convexity parameter of a given function f ; to remedy this, O'Donoghue and  propose adaptive restart schemes. Instead, one may repeatedly apply AGD-UNTIL-GUILTY and use the witnesses to update .", "Similarly, if w", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_8", "The second component of our approach is exploitation of negative curvature to decrease function values; in Section 3\nAlgorithm 2 EXPLOIT-NC-PAIR(f , u, v, \u2318) 1: (u v)/ku vk 2: u + u + \u2318 3: u u \u2318 4: return arg min z2{u ,u+} f (z)\nwe use AGD-UNTIL-GUILTY to generate u, v such that\nf (u) < f(v) + rf (v) T (u v) \u21b5 2 ku vk 2 , (10\n)\na nontrivial violation of convexity (where \u21b5 > 0 is a parameter we control using a proximal term). By taking an appropriately sized step from u in the direction \u00b1(u v), Alg. 2 can substantially lower the function value near u whenever the convexity violation ( 10) holds. The following basic lemma shows this essential progress guarantee.\nLemma 1. Let f : R d ! R have L 2 -Lipschitz Hessian. Let \u21b5 > 0 and let u and v satisfy (10). If ku vk \uf8ff \u21b5 2L2 , then for every \u2318 \uf8ff \u21b5 L2 , EXPLOIT-NC-PAIR(f, u, v, \u2318) finds a point z such that f (z) \uf8ff f (u) \u21b5\u2318 2 12 .(11)\nWe give the proof of Lemma 1 in Section A.2, and we outline it here. The proof is split into two parts, both using the Lipschitz continuity of r 2 f . In the first part, we show using (10) that f has negative curvature of at least \u21b5/2 in the direction of at the point u. In the second part, we consider the Taylor series expansion of f . The first order term predicts, due to its anti-symmetry, that either a step size of \u2318 or \u2318 in the direction reduces the objective. Adding our knowledge of the negative curvature from the first part yields the required progress.", "Using negative curvature", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_9", "We now combine the accelerated convergence guarantee of Corollary 1 and the non-convex progress guarantee of Lemma 1 to form GUARDED-NON-CONVEX-AGD. The idea for the algorithm is as follows. Consider iterate k 1, denoted p k 1 . We create a proximal functionf by adding the proximal term \u21b5 kx p k 1 k 2 to f . Applying AGD-UNTIL-GUILTY tof yields the sequences x 0 , . . . , x t , y 0 , . . . , y t and possibly a non-convexity witnessing pair u, v (line 3). If u, v are not available, we set p k = y t and continue to the next iteration. Otherwise, by Corollary 1, u and v certify thatf is not \u21b5 strongly convex, and therefore that f has negative curvature. EXPLOIT-NC-PAIR then leverages this negative curvature, obtaining a point b (2) . The next iterate p k is the best out of y 0 , . . . , y t , u and b (2) in terms of function value.\nAlgorithm 3 GUARDED-NON-CONVEX-AGD(f , p 0 , L 1 , \u270f, \u21b5, \u2318) 1: for k = 1, 2, . . . do 2: Setf (x) := f (x) + \u21b5 kx p k 1 k 2 3: (x t 0 , y t 0 , u, v) AGD-UNTIL-GUILTY(f , p k 1 , \u270f 10 , L 1 + 2\u21b5, \u21b5) 4:\nif u, v = NULL then else . non-convexity proof available\n7: b (1) FIND-BEST-ITERATE(f, y t 0 , u, v) 8: b (2)\nEXPLOIT-NC-PAIR(f, u, v, \u2318)\n9: p k arg min z2{b (1) ,b (2) } f (z) 10: if krf (p k )k \uf8ff \u270f then 11: return p k 1: function FIND-BEST-ITERATE(f , y t 0 , u, v) 2:\nreturn arg min z2{u,y0,...,yt} f (z)\nThe following central lemma provides a progress guarantee for each of the iterations of Alg. 3.\nLemma 2. Let f : R d ! R be L 1 -smooth and have L 2 - Lipschitz continuous Hessian, let \u270f, \u21b5 > 0 and p 0 2 R d . Let p 1 , . . . , p K be the iterates GUARDED-NON-CONVEX- AGD(f , p 0 , L 1 , \u270f, \u21b5, \u21b5 L2 ) generates. Then for each k 2 {1, . . . , K 1}, f (p k ) \uf8ff f (p k 1 ) min \u21e2 \u270f 2 5\u21b5 , \u21b5 3 64L 2 2 . (12\n)\nWe defer a detailed proof of Lemma 2 to Section B.1, and instead sketch the main arguments. Fix an iteration k of GUARDED-NON-CONVEX-AGD that is not the final one (i.e. k < K). Then, iff was effectively strongly convex we must have krf (y t )k \uf8ff \u270f/10 and standard proximal point arguments show that we reduce the objective by \u270f 2 /(5\u21b5). Otherwise, a witness pair u, v is available for which (10) holds by Corollary 1, and f (u) \uf8fff (u) \uf8fff (y 0 ) = f (p k ). To apply Lemma 1 it remains to show that ku vk \uf8ff \u21b5/(2L 2 ). We note that, since f (y\ni ) + \u21b5 ky i y 0 k 2 = f (y i ) \uf8ff f (y 0 ) for every i < t, if any iterate y i is far from y 0 , f (y i\n) must be substantially lower than f (y 0 ), and therefore b (1) makes good progress. Formalizing the converse of this claim gives Lemma 3, which we prove Section B.2. Lemma 3. Let f be L 1 -smooth, and \u2327 0. At any iteration of GUARDED-NON-CONVEX-AGD, if u, v 6 = NULL and the best iterate b\n(1) satisfies f (b (1) ) f (y 0 ) \u21b5\u2327 2 then for 1 \uf8ff i < t, ky i y 0 k \uf8ff \u2327, and kx i y 0 k \uf8ff 3\u2327.\nConsequently, ku vk \uf8ff 4\u2327 .\nLemma 3 explains the role of b (1) produced by FIND-BEST-ITERATE: it is an \"insurance policy\" against ku vk being too large. To complete the proof of Lemma 2 we take \u2327 = \u21b5 8L2 , so that either\nf (b (1) ) \uf8ff f (y 0 ) \u21b5\u2327 2 = f (y 0 ) \u21b5 3 64L 2 2 ,\nor we have ku vk \uf8ff 4\u2327 = \u21b5 2L2 by Lemma 3, and therefore f (b (2)\n) \uf8ff f (y 0 )\n\u21b5 3 12L 2 2\nby Lemma 1 (with \u2318 = \u21b5/L 2 ).\nLemma 2 shows we can accelerate gradient descent in a non-convex setting. Indeed, ignoring all problemdependent constants, setting \u21b5 = p \u270f in the bound ( 12) shows that we make \u2326(\u270f 3/2\n) progress at every iteration of GUARDED-NON-CONVEX-AGD, and consequently the number of iterations is bounded by O(\u270f 3/2\n). Arguing that calls to AGD-UNTIL-GUILTY each require O(\u270f 1/4 log 1 \u270f ) gradient computations yields the following complexity guarantee, which we prove in Section B.3.\nTheorem 1. Let f : R d ! R be L 1 -smooth and have L 2 - Lipschitz continuous Hessian. Let p 0 2 R d , f = f (p 0 ) inf z2R d f (z) and 0 < \u270f \uf8ff min{ 2/3 f L 1/3 2 , L 2 1 /(64L 2 )}. Set \u21b5 = 2 p L 2 \u270f (13\n)\nthen GUARDED-NON-CONVEX-AGD(f , p 0 , L 1 , \u270f, \u21b5, \u21b5 L2 ) finds a point p K such that krf (p K )k \uf8ff \u270f with at most 20 \u2022 f L 1/2 1 L 1/4 2 \u270f 7/4 log 500L 1 f \u270f 2 (14)\ngradient evaluations.\nThe conditions on \u270f simply guarantee that the clean bound ( 14) is non-trivial, as gradient descent yields better convergence guarantees for larger values of \u270f.\nWhile we state Theorem 1 in terms of gradient evaluation count, a similar bound holds for function evaluations as well. Indeed, inspection of our method reveals that each iteration of Alg. 3 evaluates the function and not the gradient at at most the three points u, u + and u ; both complexity measures are therefore of the same order.", "Accelerating non-convex optimization", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_10", "In this section, we show that when third-order derivatives are Lipschitz continuous, we can improve the convergence rate of Alg. 3 by modifying two of its subroutines. In Section 4.1 we introduce a modified version of EXPLOIT-NC-PAIR that can decrease function values further using thirdorder smoothness. In Section 4.2 we change FIND-BEST-ITERATE to provide a guarantee that f (v) is never too large.\nWe combine these two results in Section 4.3 and present our improved complexity bounds.\nAlgorithm 4 EXPLOIT-NC-PAIR 3 (f , u, v, \u2318)\n1: (u v)/ ku vk 2: \u2318 0 p \u2318(\u2318 + ku vk) ku vk 3: u + u + \u2318 0 4: v v \u2318 5: return arg min z2{v ,u+} f (z)\n4.1. Making better use of negative curvature\nOur first observation is that third-order smoothness allows us to take larger steps and make greater progress when exploiting negative curvature, as the next lemma formalizes.\nLemma 4. Let f : R d ! R have L 3 -Lipschitz third-order derivatives, u 2 R d , and 2 R d be a unit vector. If T r 2 f (u) = \u21b5 2 < 0 then, for every 0 \uf8ff \u2318 \uf8ff p 3\u21b5/L 3 , min{f (u \u2318 ), f(u + \u2318 )} \uf8ff f (u) \u21b5\u2318 2 8 .(15)\nProof. For \u2713 2 R, define h(\u2713) = f (u+\u2713 ). By assumption h 000 is L 3 -Lipschitz continuous, and therefore\nh(\u2713) \uf8ff h(0) + h 0 (0)\u2713 + h 00 (0) \u2713 2 2 + h 000 (0) \u2713 3 6 + L 3 \u2713 4 24 . Set A \u2318 = h 0 (0)\u2318 + h 000 (0)\u2318 3 /6 and set\u2318 = sign(A \u2318 )\u2318. As h 0 (0)\u2318 + h 000 (0)\u2318 3 /6 = |A \u2318 | \uf8ff 0, we have h(\u2318) \uf8ff h(0) + h 00 (0) \u2318 2 2 + L 3 \u2318 4 24 \uf8ff f (u) \u21b5\u2318 2 8\n, the last inequality using h(0) = f (u), h 00 (0) = \u21b5 2 and \u2318 2 \uf8ff 3\u21b5 L3 . That f (u +\u2318 ) = h(\u2318) gives the result. Comparing Lemma 4 to the second part of the proof of Lemma 1, we see that second-order smoothness with optimal \u2318 guarantees \u21b5 3 /(12L 2 2 ) function decrease, while third-order smoothness guarantees a 3\u21b5 2 /(8L 3 ) decrease. Recalling Theorem 1, where \u21b5 scales as a power of \u270f, this is evidently a significant improvement. Additionally, this benefit is essentially free: there is no increase in computational cost and no access to higher order derivatives. Examining the proof, we see that the result is rooted in the anti-symmetry of the odd-order terms in the Taylor expansion. This rules out extending this idea to higher orders of smoothness, as they contain symmetric fourth order terms.\nExtending this insight to the setting of Lemma 1 is complicated by the fact that, at relevant scales of ku vk, it is no longer possible to guarantee that there is negative curvature at either u or v. Nevertheless, we are able to show that a small modification of EXPLOIT-NC-PAIR achieves the required progress.\nLemma 5. Let f : R d ! R have L 3 -Lipschitz third-order derivatives. Let \u21b5 > 0 and let u and v satisfy (10) and let Algorithm 5 FIND-BEST-ITERATE 3 (f , y t 0 , u, v) 1: Let 0 \uf8ff j < t be such that v = x j 2: c j (y j + y j 1 )/2 if j > 0 else y 0 3: q j 2y\ni + 3y j 1 if j > 0 else y 0 4: return arg min z2{y0,...,yt,cj ,qj ,u} f (z)\n\u2318 \uf8ff p 2\u21b5/L 3 . Then for every ku vk \uf8ff \u2318/2, EXPLOIT- NC-PAIR 3 (f, u, v, \u2318) finds a point z such that f (z) \uf8ff max n f (v) \u21b5 4 \u2318 2 , f(u) \u21b5 12 \u2318 2 o .(16)\nWe prove Lemma 5 in Section C.1; it is essentially a more technical version of the proof of Lemma 4, where we address the asymmetry of condition ( 10) by taking steps of different sizes from u and v.", "Incorporating third-order smoothness", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_11", "An important difference between Lemmas 1 and 5 is that the former guarantees lower objective value than f (u), while the latter only improves max{f (v), f(u)}. We invoke these lemmas for v = x j for some x j produced by AGD-UNTIL-GUILTY, but Corollary 1 only bounds the function value at y j and w; f (x j ) might be much larger than f (y 0 ), rendering the progress guaranteed by Lemma 5 useless. Fortunately, we are able show that whenever this happens, there must be a point on the line that connects x j , y j and y j 1 for which the function value is much lower than f (y 0 ). We take advantage of this fact in Alg. 5, where we modify FIND-BEST-ITERATE to consider additional points, so that whenever the iterate it finds is not much better than y 0 , then f (x j ) is guaranteed to be close to f (y 0 ). We formalize this claim in the following lemma, which we prove in Section C.2. Lemma 6. Let f be L 1 -smooth and have L 3 -Lipschitz continuous third-order derivatives, and let \u2327 \uf8ff p \u21b5/(16L 3 ) with \u2327, \u21b5, L 1 , L 3 > 0.\nConsider GUARDED-NON-CONVEX-AGD with FIND-BEST-ITERATE replaced by FIND-BEST-ITERATE 3 . At any iteration, if u, v 6 = NULL and the best iterate b\n(1) satisfies f (b (1) ) f (y 0 ) \u21b5\u2327 2 then, f (v) \uf8ff f (y 0 ) + 14\u21b5\u2327 2 .\nWe now explain the idea behind the proof of Lemma 6. Let 0 \uf8ff j < t be such that v = x j (such j always exists by Corollary 1). If j = 0 then x j = y 0 and the result is trivial, so we assume j 1. Let f r : R ! R be the restriction of f to the line containing y j 1 and y j (and also q j , c j and x j ). Suppose now that f r is a cubic polynomial. Then, it is completely determined by its values at any 4 points, and f (x\nj ) = C 1 f (q j ) + C 2 f (y j 1 ) C 3 f (c j ) + C 4 f (y j )\nfor C j 0 independent of f . By substituting the bounds f (y j 1 ) _ f (y j ) \uf8ff f (y 0 ) and f (q j )^f (c j ) f (b (1) ) f (y 0 ) \u21b5\u2327 2 , we obtain an upper bound on f (x j ) when f r is cubic. To generalize this upper bound to f r with Lipschitz third-order derivative, we can simply add to it the approximation error of an appropriate third-order Taylor series expansion, which is bounded by a term proportional to L 3 \u2327 4 \uf8ff \u21b5\u2327 2 /16.", "Bounding the function values of the iterates using cubic interpolation", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_12", "With our algorithmic and analytic upgrades established, we are ready to state the enhanced performance guarantees for GUARDED-NON-CONVEX-AGD, where from here on we assume that EXPLOIT-NC-PAIR 3 and FIND-BEST-ITERATE 3 subsume EXPLOIT-NC-PAIR and FIND-BEST-ITERATE, respectively. Lemma 7. Let f : R d ! R be L 1 -smooth and have L 3 -Lipschitz continuous third-order derivatives, let \u270f, \u21b5 > 0 and p 0 2 R d . If p K 0 is the sequence of iterates produced by\nGUARDED-NON-CONVEX-AGD(f , p 0 , L 1 , \u270f, \u21b5, q 2\u21b5 L3 ), then for every 1 \uf8ff k < K, f (p k ) \uf8ff f (p k 1 ) min \u21e2 \u270f 2 5\u21b5 , \u21b5 2 32L 3 . (17\n)\nThe proof of Lemma 7 is essentially identical to the proof of Lemma 2, where we replace Lemma 1 with Lemmas 5 and 6 and set \u2327 = p \u21b5/(32L 3 ). For completeness, we give a full proof in Section C.3. The gradient evaluation complexity guarantee for third-order smoothness then follows precisely as in our proof of Theorem 1; see Sec. C.4 for a proof of the following\nTheorem 2. Let f : R d ! R be L 1 -smooth and have L 3 -Lipschitz continuous third-order derivatives. Let p 0 2 R d , f = f (p 0 ) inf z2R d f (z) and 0 < \u270f 2/3 \uf8ff min{ 1/2 f L 1/6 3 , L 1 /(8L 1/3 3 )}. If we set \u21b5 = 2L 1/3 3 \u270f 2/3 , (18\n) GUARDED-NON-CONVEX-AGD(f , p 0 , L 1 , \u270f, \u21b5, q 2\u21b5 L3 ) finds a point p K such that krf (p K )k \uf8ff \u270f and requires at most 20 \u2022 f L 1/2 1 L 1/6 3 \u270f 5/3 log \u2713 500L 1 f \u270f 2 \u25c6 (19) gradient evaluations.\nWe remark that Lemma 2 and Theorem 1 remain valid after the modifications described in this section. Thus, Alg. 3 transitions between smoothness regimes by simply varying the scaling of \u21b5 and \u2318 with \u270f.  ", "An improved rate of convergence", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_13", "The primary purpose of this paper is to demonstrate the feasibility of acceleration for non-convex problems using only first-order information. Given the long history of development of careful schemes for non-linear optimization, it is unrealistic to expect a simple implementation of the momentum-based Algorithm 3 to outperform state-of-theart methods such as non-linear conjugate gradients and L-BFGS. It is important, however, to understand the degree of non-convexity in problems we encounter in practice, and to investigate the efficacy of the negative curvature detectionand-exploitation scheme we propose.\nToward this end, we present two experiments: (1) fitting a non-linear regression model and ( 2) training a small neural network. In these experiments we compare a basic implementation of Alg. 3 with a number baseline optimization methods: gradient descent (GD), non-linear conjugate gradients (NCG) (Hager and Zhang, 2006), Accelerated Gradient Descent (AGD) with adaptive restart (O'Donoghue and Cand\u00e8s, 2015) (RAGD), and a crippled version of Alg. 3 without negative curvature exploitation (C-Alg. 3). We compare the algorithms on the number of gradient steps, but note that the number of oracle queries per step varies between methods. We provide implementation details in Section E.1.\nFor our first experiment, we study robust linear regression with the smooth biweight loss (Beaton and Tukey, 1974), f ). For 1,000 independent experiments, we randomly generate problem data to create a highly non-convex problem (see Section E.2). In Figure 1 we plot aggregate convergence time statistics, as well as gradient norm and function value trajectories for a single representative problem instance. The figure shows that gradient descent and C-Alg. 3 (which does not exploit curvature) converge more slowly than the other methods. When C-Alg. 3 stalls it is detecting negative curvature, which implies the stalling occurs around saddle points. When negative curvature exploitation is enabled, Alg. 3 is faster than RAGD, but slower than NCG. In this highly non-convex problem, different methods often converge to local minima with (sometimes significantly) different function values. However, each method found the \"best\" local minimum in a similar fraction of the generated instances, so there does not appear to be a significant difference in the ability of the methods to find \"good\" local minima in this problem ensemble.\nFor the second experiment we fit a neural network model 1 comprising three fully-connected hidden layers containing 20, 10 and 5 units, respectively, on the MNIST handwritten digits dataset (LeCun et al., 1998) (see Section E.3). Figure 2 shows a substantial performance gap between gradient descent and the other methods, including Alg. 3. However, this is not due to negative curvature exploitation; in fact, Alg. 3 never detects negative curvature in this problem, implying AGD never stalls. Moreover, RAGD never restarts. This suggests that the loss function f is \"effectively convex\" in large portions of the training trajectory, consistent with the empirical observations of Goodfellow et al. (2015); a phenomenon that may merit further investigation.\nWe conclude that our approach can augment AGD in the presence of negative curvature, but that more work is necessary to make it competitive with established methods such as non-linear conjugate gradients. For example, adaptive schemes for setting \u21b5, \u2318 and L 1 must be developed. However, the success of our method may depend on whether AGD stalls at all in real applications of non-convex optimization.", "Preliminary experiments", "background"], ["672977bfbb94e115cc778c32b3b0fff7d09102b8_14", "OH was supported by the PACCAR INC fellowship. YC and JCD were partially supported by the SAIL-Toyota Center for AI Research and NSF-CAREER award 1553086. YC was partially supported by the Stanford Graduate Fellowship and the Numerical Technologies Fellowship.", "Acknowledgment", "acknowledgement"]], "meta": {"conference": "ICML", "year": 2017, "authors": "Yair  Carmon, John C. Duchi, Oliver  Hinder, Aaron  Sidford"}}