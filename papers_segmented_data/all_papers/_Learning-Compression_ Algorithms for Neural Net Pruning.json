{"title": "\"Learning-Compression\" Algorithms for Neural Net Pruning", "paper_id": "d719009bade1c245ac6e2fa9e4cd74eddd4f34b4", "segments": [["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_0", "Pruning a neural net consists of removing weights without degrading its performance. This is an old problem of renewed interest because of the need to compress ever larger nets so they can run in mobile devices. Pruning has been traditionally done by ranking or penalizing weights according to some criterion (such as magnitude), removing low-ranked weights and retraining the remaining ones. We formulate pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition. We give a generic algorithm to solve this which alternates \"learning\" steps that optimize a regularized, data-dependent loss and \"compression\" steps that mark weights for pruning in a data-independent way. Magnitude thresholding arises naturally in the compression step, but unlike existing magnitude pruning approaches, our algorithm explores subsets of weights rather than committing irrevocably to a specific subset from the beginning. It is also able to learn automatically the best number of weights to prune in each layer of the net without incurring an exponentially costly model selection. Using a single pruninglevel user parameter, we achieve state-of-the-art pruning in LeNet and ResNets of various sizes.", "abstract", "abstract"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_1", "Pruning neural nets is an old problem that has been revived in recent years. It consists of removing weights and/or neurons with the goal of reducing the size of the net without hurting its accuracy, learning automatically the right number of neurons and weights, or avoiding overfitting. Work in the 90s produced various algorithms that generally operate by using some criterion or penalty to detect unimportant weights or neurons, removing them and retraining the remaining ones, possibly on the fly while training the net. The 2010s have shown that neural nets achieve state-of-theart performance in various applications if trained on large data sets using GPUs and using a large net, having many layers, neurons and weights. The large size of these nets (upwards of millions of weights) make them difficult to deploy in limited-computation devices such as mobile phones. This has brought a renewed interest in pruning and gener-ally in neural net compression, so that one can obtain small yet accurate nets. Pruning and compression are possible because these large nets are hugely overparameterized, and empirical evidence suggests it is easier to train a large net and compress it than to train a smaller net from start [28].\nAlthough pruning can be seen as a way to find the right architecture for a net or to improve generalization, here we focus on pruning as a way to compress a well-trained, reference net with little accuracy loss (the reference net is also helpful in telling us what is the best that we can achieve, up to local optima). Much pruning work uses a heuristic modification of the usual neural net training so that one removes weights on the fly via some criterion. While this can succeed in practice, it is not clear whether the resulting pruned net is optimal and in what sense. We take a top-down optimization view: we define the problem mathematically as an optimization over the net weights that incorporates our conflicting desires of minimizing the loss (e.g. classification error) and minimizing the number of weights. Specifically, we are inspired by recent work [2,3] that formulates neural net compression in a general way via constrained optimization and shows how this leads to a powerful weight quantization algorithm. Here, we develop and extend this approach for the problem of pruning a deep net. In addition, we seek algorithms that are able to identify exactly which weights should be zero. An example that does not satisfy this are interior-point methods (one of the best approaches for large-scale problems), since their iterates are nonzero throughout training and only converge to exact zeros in the limit. The reason to identify the zeros exactly is that, with many weights, optimizing to high accuracy is impractical, and this introduces uncertainty about which nonzero weights should really be zero based on their value.\nWe consider pruning as a form of compression, where unpruned weights w \u2208 R n are compressed into sparse weights \u03b8 \u2208 R n satisfying a condition dependent on a pruning cost function C(\u03b8) which promotes sparsity in the weight vector \u03b8, such as \u2113 0 or \u2113 1 , and is mathematically expressed as either a constraint or a penalty. The result is a \"learning-compression\" (LC) algorithm that alternates a learning step that optimizes the data-dependent loss over the real-valued weights w with a compression (pruning) step that compresses w into \u03b8, independently of the loss and data. Interestingly, for certain pruning costs this compression step naturally has the form of magnitude pruning, which gives support to using magnitude as a measure of weight saliency (as opposed to, say, curvature). However, our algorithm does not prune permanently: weights move in and out of the set of pruned weights during training until we converge on a final set. We first describe our general approach and develop it for its constraint and penalty forms. Although we focus on \u2113 0 , \u2113 1 and \u2113 2 2 , we emphasize our framework applies to other costs. Then we describe how to learn the amount of pruning per layer automatically and discuss the algorithm's behavior. Our experiments with LeNet and ResNets show that our LC algorithm achieves larger amounts of pruning with no loss degradation, and show the peculiar structure of the pruned net that arises.", NaN, "abstract"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_2", "Pruning was recognized as an important problem since the 1980s; see reviews [28] and [1, ch. 9.5]. Most methods can be classified into two types: saliency ranking methods use a criterion to estimate the importance of each weight in the net, remove less important weights and retrain the rest; and penalty methods minimize the loss L(w) plus a penalty term \u03b1 C(w) that penalizes nonzero weights, remove small weights upon convergence and retrain the rest. Many saliency criteria exist, such as magnitude |w i |, curvature using the diagonal [23] or all the Hessian entries [17,18], and sensitivity of the loss to removing w i . While most saliency methods are simple and fast, their performance is limited: they are local (the saliency estimate for each w i is valid at the reference net but not away from it), greedy (weights are pruned irrevocably, with no backtracking), and individual weight saliency is after all a heuristic estimate for the effect on the loss of the set of weights to be pruned. This can be partly improved by applying the pruning/retraining in stages (where only a few weights are pruned at a time, e.g. [33,15]), but this is timeconsuming in practice. Penalty methods were mostly based on weight decay and variations of it, penalizing \u03b1 i w 2 i or variations such as \u03b1 i w 2 i /(A + w 2 i ) [16,31] that encourage weights to be either large or small. Weight decay helps avoid overfitting and prune weights, but is not sparsifying: upon convergence, none of the weights are zero, and truncation is somewhat arbitrary just as with saliency methods. Sparsifying penalties such as \u2113 0 or \u2113 1 seem not to have been investigated, presumably because backpropagation cannot handle their nonsmoothness. Group LASSO penalties (to prune entire filters of a net) have been recently considered in [25,32]. Their SGD optimization adds a heuristic thresholding step to zero values below 10 \u22124 . However, online methods such as SGD have trouble deciding whether a given weight should be pruned or not based on a minibatch [33, section 3.1]. Finally, pruning can be com-bined with other compression techniques, such as weight quantization [12,15,3], low-rank decomposition of weight matrices [29,8,21,9,27], hashing [5], lossless compression such as Huffman codes [14], etc. Here we focus on pruning alone. Interestingly, although saliency and penalty methods appear very different, we will show they are related in our LC algorithm: an iterative form of magnitude-based pruning arises in a principled way from the use of sparsifying penalties on the loss.\n1. Neural network pruning as an optimization problem 1 We define a pruning cost as a function C: R n \u2192 R + satisfying C(0) = 0 and C(w) > 0 if w = 0. We say that C is separable if C(w) = n i=1 c(w i ) where c: R \u2192 R + is a scalar pruning cost. C should be designed such that it penalizes nonzeros in w. The pruning cost function C and the pruning operators \u03a0 + C , \u03a0 \u2264\nC defined later are central concepts in our framework. We study three important examples of C(w): w 0 (the number of nonzero elements of w), w 1 , and w 2 2 , all of which are separable. While other costs could be studied that are of practical interest, these are representative of what can be achieved in our framework and illustrate the issues of sparsification and shrinkage. \u2113 0 is arguably the most natural definition of pruning, as it is equivalent to finding the best subset of pruned weights, but it is a hard combinatorial problem. \u2113 2 2 corresponds to regular weight decay and can be optimized directly by descent methods, but it is instructive in our discussion.\nConsider then the following general formulation for learning an optimally pruned network, where L(w) is a loss function of interest (such as the classification or regression error on a training set):\nConstraint form: min w L(w) s.t. C(w) \u2264 \u03ba (1a)\nPenalty form: min w L(w) + \u03b1 C(w).\nBoth naturally aim at learning an optimal model, by minimizing the data-dependent loss L(w), but subject to having many zero weights, as given by the pruning parameters \u03ba \u2265 0 and \u03b1 \u2265 0. Although optimizing the above could be done in different ways, here we focus on a common mechanism that results in a very simple yet effective learning-compression (LC) algorithm [2] for both forms. This alternates a data-dependent step that updates the \"uncompressed parameters\" (here, all the weights in the net) with a data-independent step that compresses the parameters (here, prunes the weights). The idea is to decouple the pruning term on C from the learning term on L via an auxiliary variable \u03b8, a quadratic-penalty function and an alter-nating optimization over w and \u03b8. We describe the mathematical development for the constraint form first and for the penalty form next. Before proceeding, note that the penalty and the constraint forms define problems that are equivalent for appropriate choices of \u03ba and \u03b1. However, algorithmically they differ, and one form may be preferable over the other depending on the case; see our discussions later of factors such as computational cost, global vs local sparsity, or user friendliness of hyperparameter setting. This is particularly true with nonconvex problems, having local optima, and nonsmooth or combinatorial functions such as \u2113 0 .", "Related work", "background"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_3", "Let us introduce an auxiliary variable \u03b8 in eq. ( 1) that duplicates w:\nmin w,\u03b8 L(w) s.t. C(\u03b8) \u2264 \u03ba, w = \u03b8. (2\n)\nThis problem is in the \"model compression as constrained optimization\" form min w,\u03b8 L(w) s.t. w = \u2206(\u03b8) of [2], where the \"decompression mapping\" w = \u2206(\u03b8), which recovers the uncompressed model parameters from their compressed version, takes a very simple form: w = \u03b8 but satisfying C(\u03b8) \u2264 \u03ba, i.e., having few nonzeros. We now optimize this constrained problem via either the quadraticpenalty (QP) or augmented-Lagrangian (AL) method (applied only to the equality constraint, not to the inequality):\nQ(w, \u03b8; \u00b5) = L(w) + \u00b5 2 w \u2212 \u03b8 2 s.t. C(\u03b8) \u2264 \u03ba L A (w, \u03b8, \u03bb; \u00b5) = L(w) + \u00b5 2 w \u2212 \u03b8 2 \u2212 \u03bb T (w \u2212 \u03b8) s.t. C(\u03b8) \u2264 \u03ba.\nFor the QP, we optimize Q over (w, \u03b8) while driving \u00b5 \u2192 \u221e, so the equality constraints are satisfied in the limit. For the AL, we alternate optimizing L A over (w, \u03b8) with updating \u03bb \u2190 \u03bb\u2212\u00b5(w\u2212\u03b8) while driving \u00b5 \u2192 \u221e. The Lagrange multiplier estimates \u03bb make the iterates (w, \u03b8) be closer to the solution for the same value of \u00b5, so the AL is preferable. Finally, in order to optimize the QP or AL functions over the variables (w, \u03b8), we apply alternating optimization. This results in the following steps for the QP:\nLearning (L) step (over w) min w L(w) + \u00b5 2 w \u2212 \u03b8 2 .\nThis has the form of a usual neural net learning but with a quadratic regularizer that pulls some weights to zero (since \u03b8 will usually contain some exactly zero elements) and the rest to some other nonzero value.\nCompression (C) step (over \u03b8) \u03a0 \u2264 C (w; \u03ba) = arg min \u03b8 w \u2212 \u03b8 2 s.t. C(\u03b8) \u2264 \u03ba (where the \" \u2264 \" superindex refers to the constraint form). This has the form of a proximal operator, which we call pruning operator. It can be solved exactly for several useful costs C, including \u2113 0 , \u2113 1 and \u2113 2 2 . In our context, \"compression\" means \"weight pruning\".\nWe describe the C step in more detail later for the QP. For the AL, replace \u03b8 by \u03b8+ 1 \u00b5 \u03bb in the L step, and w by w\u2212 1 \u00b5 \u03bb in the C step. The suppl. mat. gives the algorithm pseudocode for the AL.\nNote that the C step does not actually prune weights, it simply \"marks\" weights to be pruned (by setting their \u03b8 i = 0); the w i values stay as nonzero. The L step is the one that actually updates the real-valued weights w i taking into account both the loss and the markup. As the LC algorithm alternates both steps, it explores different sets of marked weights, eventually converging to a specific set for which w i \u2212 \u2212\u2212\u2212 \u2192 \u00b5\u2192\u221e 0 and thus is actually pruned.", "Constraint form for the pruning cost", "background"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_4", "The proximal operator \u03b8 = \u03a0 \u2264 C (w; \u03ba) = arg min \u03b8 w \u2212 \u03b8 2 s.t. C(\u03b8) \u2264 \u03ba maps a real-valued weight vector w to another real-valued vector \u03b8 of the same dimension containing a certain number of zero elements, so \u03b8 is a pruned version of w (and, as we will see, it possibly shrinks its nonzero values). It has the form of a projection, or nearest point \u03b8 to w (in Euclidean distance) that lies in the feasible set C(\u03b8) \u2264 \u03ba. The projection operator leaves all weights unchanged if C(w) \u2264 \u03ba (i.e., \u03a0 \u2264 C (w; \u03ba) = w). Otherwise, the resulting \u03b8 has C(\u03b8) \u2264 \u03ba < C(w), which implies that \u03b8 is \"smaller\" than w, and indeed many weights will individually satisfy |\u03b8| i < |w i |, but some may stay or increase in magnitude.\nThe solution for several costs C corresponding to projection on \u2113 p balls is well known and is given in fig. 1 (see proofs in the suppl. mat.). When w is in the ball, \u03b8 = w. Otherwise, \u2113 0 leaves the top-\u03ba weights unchanged and prunes the rest; \u2113 1 shrinks on average the top-k weights (where k depends on w and \u03ba) and prunes the rest; and \u2113 2 2 shrinks all weights (normalizes w).\nComputationally, a simple algorithm for \u2113 0 and \u2113 1 involves sorting the elements of w in magnitude (at a runtime O(n log n) if w has n elements), and scanning this in O(n) to find the threshold \u03b7 and return the nonzeros. But both \u2113 0 and \u2113 1 can be solved is O(n) worst case runtime by using selection to find the kth value in O(n) (this can be achieved with a partial quicksort; [7, ch. 9]). For \u2113 0 , this is obvious. For \u2113 1 , see [6]. Since the number of weights in a deep net, which is our driving application, is large (upwards of millions in practice), using selection instead of sorting matters. That said, the L step dominates the C step by far.", "C step", "background"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_5", "Although the penalty form does not have the model compression as constrained optimization form of [2], we can apply the same technique to arrive at a convenient LC algorithm. First we duplicate w via an auxiliary variable \u03b8: min w,\u03b8 L(w) + \u03b1 C(\u03b8) s.t. w = \u03b8.\n(3)\nThen we optimize this via QP or AL:\nQ(w, \u03b8; \u00b5) = L(w) + \u00b5 2 w \u2212 \u03b8 2 + \u03b1 C(\u03b8) L A (w, \u03b8, \u03bb; \u00b5) = L(w) + \u00b5 2 w \u2212 \u03b8 2 \u2212 \u03bb T (w \u2212 \u03b8) + \u03b1 C(\u03b8).\nFinally, we apply alternating optimization over (w, \u03b8). This results in an L step (over w) identical to that of the constraint form, and a C step (over \u03b8)\n\u03a0 + C w; 2\u03b1 \u00b5 = arg min \u03b8 w \u2212 \u03b8 2 + 2\u03b1 \u00b5 C(\u03b8) (\nwhere the \" + \" superindex refers to the penalty form). This is again a proximal operator that can often be solved exactly, as shown next. \nProof. First, F (\u2212\u03b8; \u2212w) = (\u2212w + \u03b8) 2 + 2\u03b1 \u00b5 c(\u2212\u03b8) = (w \u2212 \u03b8) 2 + 2\u03b1\n\u00b5 c(\u03b8) = F (\u03b8; w), so F is invariant to negating \u03b8 and w. Second, let w \u2208 R. Since (w \u2212 \u03b8) 2 is smaller when \u03b8 has the same sign as w than when it has the opposite sign (for the same magnitude of \u03b8), and c(\u03b8) = c(\u2212\u03b8), then F is smaller also. Hence, \u03b8 * has the same sign as w. Finally, we prove by contradiction that \u03b8 * \u2264 w for the case w \u2265 0 w.l.o.g. Suppose \u03b8 * > w, then F (\u03b8 * ; w\n) = (w \u2212 \u03b8 * ) 2 + 2\u03b1 \u00b5 c(\u03b8 * ) \u2265 (w \u2212 \u03b8 * ) 2 + 2\u03b1 \u00b5 c(w) > 2\u03b1 \u00b5 c(w) = F (w; w), which contradicts F (\u03b8 * ; w) \u2264 F (\u03b8; w) \u2200\u03b8 \u2265 0.\nThis means that the pruning operator drives w to zero, as one would expect, but how this happens depends on the pruning cost C. Fig. 1 gives explicitly the pruning operator for several costs (see proofs in the suppl. mat.). We observe two types of behavior: sparsification, in which weights within some interval become exactly zero; and shrinkage, in which weights that do not become zero become smaller anyway. \u2113 0 sparsifies but does not shrink: w is either pruned (\u03b8 = 0) or left as is (\u03b8 = w). \u2113 1 sparsifies and shrinks: w is either pruned (\u03b8 = 0) or shifted towards zero (\u03b8 = w \u2212 sgn (w) \u03b1 \u00b5 ). \u2113 2 2 does not sparsify but shrinks: w is divided by a number bigger than 1.", "Penalty form for the pruning cost", "background"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_6", "In a neural net, a fully-connected layer has many more weights than a convolutional layer and can be pruned more aggressively. Hence, allowing a different sparsity level for each layer (say, 5% unpruned weights for the convolutional layer and 1% for the fully-connected one) will result in networks with lower loss for the same total number of weights. In the penalty form, this unfortunately requires an exponentially costly selection for the per-layer penalties (\"local\" sparsity). However, in the constraint form we can prove that using a single \u03ba parameter for the whole net (\"global\" sparsity) can find the optimal per-layer sparsities. This is remarkable because we achieve the best of both worlds: ease of use (only one pruning parameter to select) and best results. The LC algorithm automatically determines the best number of weights for each layer.\nTheorem 1.2. Let C be a separable pruning cost; \u03ba 1 , . . . , \u03ba K , \u03ba \u2208 R + with \u03ba 1 + \u2022 \u2022 \u2022 + \u03ba K \u2264 \u03ba; and S l = {w = (w 1 , . . . , w K ) \u2208 R n : C(w 1 ) \u2264 \u03ba 1 , . . . , C(w K ) \u2264 \u03ba K } and S g = {w \u2208 R n : C(w) \u2264 \u03ba}. Then S l \u2282 S g . Proof. Let w \u2208 S l . Then C(w 1 ) \u2264 \u03ba 1 , . . . , C(w K ) \u2264 \u03ba K , so C(w) = C(w 1 )+\u2022 \u2022 \u2022+C(w K ) \u2264 \u03ba 1 +\u2022 \u2022 \u2022+\u03ba K \u2264 \u03ba and w \u2208 S g .\nOur optimization applies equally easily to both global and local sparsity. For example, for the \u2113 0 case with global sparsity, the top-\u03ba weights throughout the entire net stay and the rest are pruned; how many weights are pruned in each layer in the C step arises automatically and optimally.", "Global vs local sparsity", "background"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_7", "To follow the path over \u00b5 \u2265 0 numerically, we use a multiplicative schedule \u00b5 k = \u00b5 0 a k , k = 0, 1, 2 . . . , where \u00b5 0 is given in the suppl. mat. and a > 1 is determined by trial and error (using a smaller a follows the path more slowly and generally gives a better solution, but is computationally slower). We stop when w \u2212 \u03b8 is smaller than a set tolerance and retrain the unpruned weights. This is unnecessary in theory for the \u2113 0 cost, but in practice with deep nets (which are notoriously hard to optimize accurately) retraining it will improve a bit the result. For the \u2113 1 cost retraining is necessary and will significantly improve the result (and increase on average the weights' magnitude).\nFor large enough \u00b5 the LC algorithm will identify the final set of weights that are pruned, i.e., which elements in \u03b8 are zero. We can analyze what happens at the beginning of the path (see suppl. mat.), which provides an interesting perspective on pruning/retraining algorithms (e.g. [15]). Essentially, for the constraint form we start at a point (w(0), \u03b8(0)) = (w, \u03b8 DC ) where w is a well-trained, reference model, and \u03b8 DC = \u03a0 \u2264 C (w; \u03ba). This was called direct compression (here, direct pruning) in [2], as it corresponds to pruning the reference weights independently of the loss. The weights \u03b8 DC result from magnitude pruning of the reference weights w and they produce a large loss, (penalty form). All cases result in an elementwise operator that computes \u03b8i from wi for each weight. The threshold \u03b70 equals the magnitude of the (\u03ba + 1)th largest weight. The threshold \u03b71 can be obtained by scanning wi in decreasing order of magnitude (see main text). The graph plots the elementwise pruning operator for the penalty form.\nC(w) Constraint form \u2020 \u03b8 = \u03a0 \u2264 C (w; \u03ba) Penalty form \u03b8 = \u03a0 + C w; 2\u03b1 \u00b5 w 0 w i \u2022 I |w i | > \u03b7 0 w i \u2022 I(|w i | > 2\u03b1 \u00b5 ) w 1 (w i \u2212 sgn (w i ) \u03b7 1 ) \u2022 I(|w i | > \u03b7 1 ) w i \u2212 sgn (w i ) \u03b1 \u00b5 \u2022 I |w i | > \u03b1 \u00b5 w 2 2 \u221a \u03ba w i / w 2 w i / 1 + 2\u03b1 \u00b5 \u2020 Each formula applies if C(w) > \u03ba, otherwise \u03b8 = w.\nwhich can be reduced by retraining the nonzero weights in \u03b8 DC . This pruning/retraining approach is perhaps the most widespread pruning method for deep nets; we discuss it further in section 3.\nTheorem 2.1 in [2] applies to our penalty form (3) without modification and should be easy to extend to our constraint form (2) (which contains the extra constraint C(\u03b8) \u2264 \u03ba). Essentially, it states that if we follow the path closely enough (by minimizing Q or L A for each \u00b5 via sufficiently many L and C steps), then we reach a local solution in the limit \u00b5 \u2192 \u221e. However, that theorem assumes smooth (though not necessarily convex) L(w) and C(\u03b8). This holds for \u2113 2 2 but not for the more interesting, sparsifying costs \u2113 0 and \u2113 1 . Guarantees for \u2113 0 are likely hard to come by because it defines an NP-complete problem. Guarantees for \u2113 1 may be easier to state.", "Behavior, convergence and practicalities of the LC algorithm", "background"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_8", "All four variations of our LC algorithm (\u2113 0 /\u2113 1 , constraint/penalty) arise from a common methodology: to duplicate variables w = \u03b8, apply a penalty method and optimize alternatingly over w and \u03b8. Although our focus is on deep neural nets, these algorithms also apply when the loss L(w) is quadratic and the model is linear. There is a relation with some fundamental algorithms in compressed sensing and sparse learning, specifically Lasso regression. These minimize over w objectives of the type y \u2212 Aw 2 , where w is sparse. The alternating direction method of multipliers (ADMM) for the Lasso [19] uses an \u2113 1 penalty and the resulting algorithm is the same as our \u2113 1 -penalty variation. Iterative hard thresholding (IHT) [11] seeks an (approximate) \u2113 0 -constraint solution, but its algorithm is somewhat different from our \u2113 0 -constraint variation, involving a gradient step and a hard thresholding step. Those algorithms have been found very effective in practice and enjoy certain theoretical guarantees of finding the global minimum of the \u2113 0 problem (under assumptions such as restricted isometry or incoherence on the matrix A and vectors w, y). It is unclear whether any of those guarantees may carry over to our case of interest, neural net pruning, but it is encouraging that such guarantees may hold in the linear case.", "Relation with compressed sensing and Lasso", "background"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_9", "We evaluate our LC algorithm for pruning on classification neural nets of different sizes in the MNIST (LeNet) and CIFAR10 (ResNets) datasets and compare with magnitudebased pruning, that is, pruning all but the largest magnitude weights of the reference net and retraining them. We exceed or are comparable in both training and test error with any published results we know of, at any pruning level, even though we use a single user parameter \u03ba or \u03b1 and a single round of pruning. Code/data are available from the authors.\nWe report the augmented Lagrangian results and use always a single, global parameter (\u03ba for the constraint form, \u03b1 for the penalty form). We used the Theano [30] and Lasagne [10] libraries. We initialize all algorithms from a reasonably (but not necessarily perfectly) well-trained reference model. The initial LC iteration (\u00b5 = 0) for the constraint form gives the magnitude-based pruning solution. We only prune the multiplicative weights in the net, not the biases. We report the loss and classification error in training and test, and the proportion (%) of pruned weights (total and per layer).\nThe optimization parameters are as follows throughout our experiments with minor exceptions (see suppl. mat. and [4]). We use Nesterov's accelerated gradient method [26] with momentum 0.95 for around 100k minibatches, with a learning rate of the form \u03b7 \u2022 0.99 j (where \u03b7 is between 0.02 and 0.1), running 2k iterations for every j (each a minibatch of 512 points). Our LC algorithm uses \u00b5 j = \u00b5 0 a j with \u00b5 0 = 9.76 \u2022 10 \u22125 and a = 1.1, for 0 \u2264 j \u2264 30. The jth L step runs 2k SGD iterations. We retrain the surviving weights with SGD for our LC algorithm and for magnitudebased pruning. The total runtime of our LC algorithm is roughly given by the number of L steps; we found it is to be no more than 1.5 times the runtime of the reference net.", "Experiments", "experiments"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_10", "The LeNet models [22] are a widely used benchmark that allows for comparison with published work. We randomly split the MNIST training dataset (60k grayscale images of 28 \u00d7 28, 10 digit classes) into training (90%) and validation (10%) sets. We normalize the pixel grayscales to [0,1] and then subtract the mean. The loss is the average crossentropy. LeNet300 is a 3-layer fully-connected feedforward net 784-300-100-10 with tanh activations and softmax outputs, total 266 610 learnable parameters. LeNet5 is a convolutional net with ReLU activations and softmax outputs, total 431k trainable parameters. We report results mostly for LeNet300; see full details in the suppl. mat.\nTable 1 gives a subset of pruning results for LeNet300. Generally, the constraint form does better than the penalty form, and the \u2113 0 cost does better than the \u2113 1 cost, but not significantly. Retraining the pruned net has a large effect for the \u2113 1 cost, as expected, because \u2113 1 shrinks the surviving weights: the loss decreases and the weights' magnitude increases on average. Retraining has barely any effect for the \u2113 0 cost, which does not shrink the weights.\nWe did not try to find the very best parameter settings (for the pruning cost \u03ba or \u03b1, or for the SGD and LC optimization parameters), instead we sample what can be achieved. (Note that for \u2113 0 -constraint the proportion P % of surviving weights is directly given by \u03ba, which is convenient for the user; but for \u2113 1 -constraint and the penalty forms, achieving a desired P % requires trial and error of \u03ba or \u03b1, which is cumbersome.) We can prune \u223c 98-99% of the weights with about the same loss/error as the reference. We can go beyond 99% with a minor degradation. This outperforms nearly all published work we have seen: magnitude pruning done in stages in [15] achieves 92% (a little better than the single-stage magnitude pruning we show) and [32] is much worse. Only [13] is comparable to us, however their results are not reproducible based on the information in the paper, which neglects to disclose even the per-layer pruning parameters they used. Besides, tuning by hand the pruning parameter for each layer or the stages of pruning makes the network designer effectively part of the algorithm, painstakingly so. We reiterate we simply select a single pruning parameter, which for the \u2113 0 constraint form is trivial to set: \u03ba equals the number of surviving weights.\nNow we analyze which weights and neurons get pruned and how this changes over LC iterations, as the final connectivity structure is very interesting. Fig. 2 shows the weight vectors \u03b8 over LC iterations for 5 selected neurons in the first layer of LeNet300, for pruning around 95% weights (the same neurons for each combination of \u2113 0 /\u2113 1 and constraint/penalty). As the LC algorithm iterates, \u03b8 marks weights for pruning and w approaches \u03b8 until w = \u03b8 upon convergence. Each weight vector can be shown as a 28 \u00d7 28 color image (red: positive, blue: negative, white: zero, gray: neuron pruned). The initial weights appear random and cover the entire image area. For the constraint form, the first iteration prunes all weights except the largest ones. For the penalty form, the first iteration prunes all weights, but when \u00b5 \u2248 \u00b5 0 the largest weights revive (see theoretical analysis in suppl. mat.). After that, different weights move in and out of the marked subset. The evolution of weights and neurons can be seen dramatically in an animation (suppl. mat.), in particular how for \u2113 1 the \"weight mass\" of a pruned neuron is captured by weights in other neurons. Although the weights change during training, the final weights resemble the initially pruned ones to some extent. The \u2113 1 cost changes weights more than the \u2113 0 one, and results in more neurons being pruned. The final weight vectors often segment the image into negative and positive regions reminiscent of center-surround receptive fields, but these regions are sparse rather than compact. Presumably this is because neighboring pixels are correlated and it suffices to sample a few to capture a good feature.\nAlthough our algorithm prunes weights, not neurons, we observe an aggressive neuron pruning in the first layer, much more than would be expected if weights were pruned uniformly at random. Even though there are 5% of 784 \u2248 39 surviving input weights per first-layer neuron, in fact up to 3/4 of the neurons are pruned (which hence have \u2248 120 weights); see fig. 3 (# alive weights). Likewise, about half of the input neurons (pixels) have all output weights pruned and so are pruned (mostly around the image boundaries, which are constant in MNIST; for \u2113 0 -constraint, it looks like this:\n). Indeed, the original LeNet300 architecture 784-300-100-10 becomes 400-64-99-10 with similar or even better loss (for the \u2113 1 -constraint). Hence, our pruning algorithm might be useful to do feature selection and determine the optimal number of neurons in each layer automatically.\nA neuron is pruned when all its input and output weights are pruned. We observe the input weights disappear first, followed by the output ones. \u2113 0 is slightly less effective in pruning neurons: upon convergence we often find a few neurons each having no input weights and only a few output weights. With \u2113 1 , no such neurons remain. This is visible in the green curves in fig. 3 (# alive weights), corresponding to the fan-in and fan-out of layer 1: for \u2113 1 they both go down (fan-in first, then fan-out) and join upon convergence; for \u2113 0 this happens partially (which is not a problem since such neurons can be safely removed in a postprocessing step).\nFig. 3 (right subplots) shows the weight distribution in layer 1. It starts as a zero-mean Gaussian (from the reference net). Then it becomes trimodal, with a peak at zero (pruned weights) and two skewed distributions for negative and positive weights. For \u2113 0 the gap between the last two is much wider than for \u2113 1 .", "Classification on MNIST with LeNet300 and LeNet5", "experiments"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_11", "The ResNet models [20] are one of the best performing deep nets in recent literature, and they are also very lean, achieving stateof-the-art classification error with a much smaller number of weights than other nets such as AlexNet or VGG. This makes them harder to prune, and indeed we are aware of only one other work on pruning ResNets on CIFAR10 [24].   We train ResNets of depth 32, 56 and 110 layers (0.46M, 0.85M and 1.7M parameters, respectively) on the CIFAR10 dataset using the same setup as in [20]. (see details in the suppl. mat.). Fig. 4 shows the results. We are able to achieve considerable pruning of P \u2248 5-10% surviving weights with about the same test error as the reference. The LC errors are always much lower than those of magnitude-based pruning. We found one published comparison point: [24] remove filters from convolutional layers for ResNet56/110 and achieve P = 67.6% for ResNet110 (error 6.7%) and P = 86.3% ResNet56 (error 6.94%). Our LC algorithm achieves a much stronger pruning P = 10% for ResNet110 (error 6.50%) and ResNet56 (error 6.67%).", "Classification on CIFAR10 with ResNets", "experiments"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_12", "Most pruning approaches are based on the idea of permanently removing a subset of weights (\"hard pruning\") based on some criterion that measures the importance of each weight (such as magnitude or curvature), and then retraining the remaining weights. This approach is successful: it can prune many weights with no or little loss degradation. However, it is heuristic, lacking a theoretical understanding of how good these criteria are, and greedy: its success depends on choosing the right subset to prune among all possible subsets of weights, since there is no backtracking. The pruning/retraining process may be repeated several times, each time removing a small subset of the weights. By trial-and-error, one can make this improve over choosing a single large subset, but this effectively shifts the effort of searching over solutions to the user and is not practical (taking into account the long training times required for a deep net). An important consequence of our optimization-based approach is that magnitude pruning arises naturally both in the constraint and penalty forms with \u2113 0 and \u2113 1 . Hence, our LC algorithm gives theoretical support to the use of magnitude as criterion. But it differs from previous methods in that it uses it gradually, by exploring possible sets of pruned weights while optimizing the loss over all weights, without committing irrevocably to any set (\"soft pruning\"). All the weights are there throughout training, but some are marked as currently pruned (zeros in \u03b8). Compared to hard pruning, this helps find a better subset and hence prune more weights with no or little loss degradation.\nFinally, we recommend using our LC algorithm with the \u2113 0 or \u2113 1 constraint form and a global \u03ba pruning parameter, because it learns automatically the optimal pruning level for each layer. Also, setting \u03ba for \u2113 0 is very simple: it equals the desired number of surviving weights.", "Discussion", "results"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_13", "Our algorithm vindicates magnitude-based pruning but in a soft, iterative way. It explores many weight sets in search of a good one rather than committing greedily to the largest weights in the reference model. It is easy to implement: most of the runtime is spent training the reference model with a quadratic regularization term, with fast, periodic updates to the set of weights to be pruned and to the Lagrange multipliers. A crucial advantage is that it automatically determines the best number of weights to prune in each layer even though it uses a single user parameter. This avoids an exponentially costly search over per-layer pruning parameters and vastly simplifies the network designer's job. Although we focused on model compression, our algorithm may be generally used during training to achieve good generalization and a small net.", "Conclusion", "results"], ["d719009bade1c245ac6e2fa9e4cd74eddd4f34b4_14", "Work supported by NSF award IIS-1423515, by a UC Merced Faculty Research Grant, by a Titan X Pascal GPU donated by the NVIDIA Corporation, and by computing time in the MERCED cluster (NSF grant ACI-1429783).", "Acknowledgments", "acknowledgement"]], "meta": {"conference": "CVPR", "year": 2018, "authors": "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n, Yerlan  Idelbayev"}}