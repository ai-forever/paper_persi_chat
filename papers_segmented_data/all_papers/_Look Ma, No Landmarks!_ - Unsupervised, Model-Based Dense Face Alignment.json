{"title": "\"Look Ma, No Landmarks!\" - Unsupervised, Model-Based Dense Face Alignment", "paper_id": "e30e96cff46897350a126f103942f2c116328e18", "segments": [["e30e96cff46897350a126f103942f2c116328e18_0", "In this paper, we show how to train an image-to-image network to predict dense correspondence between a face image and a 3D morphable model using only the model for supervision. We show that both geometric parameters (shape, pose and camera intrinsics) and photometric parameters (texture and lighting) can be inferred directly from the correspondence map using linear least squares and our novel inverse spherical harmonic lighting model. The least squares residuals provide an unsupervised training signal that allows us to avoid artefacts common in the literature such as shrinking and conservative underfitting. Our approach uses a network that is 10\u02c6smaller than parameter regression networks, significantly reduces sensitivity to image alignment and allows known camera calibration or multi-image constraints to be incorporated during inference. We achieve results competitive with state-of-the-art but without any auxiliary supervision used by previous methods.", "abstract", "abstract"], ["e30e96cff46897350a126f103942f2c116328e18_1", "CNN-based face image analysis with a 3D morphable model (3DMM) [7] has recently shown great promise for both 3D face reconstruction from a single image [22,31,24] and dense face alignment [3,35,38,39,8] (i.e. predicting dense correspondence from image pixels to model). These methods are supervised, limiting their application only to labelled images and not providing a general method that can be extended to new object classes.\nOne line of CNN-based 3D face reconstruction work offers the promise of overcoming this reliance using model-based autoencoders for self-supervision [29,13,10,28,6]. Here, a 3DMM and a differentiable renderer are used as a model-based decoder such that a trainable encoder (a CNN) can learn to regress semantically meaningful model parameters. In principle, model-based autoencoders can be trained in a self-supervised fashion. In practice, most rely on auxiliary supervision in the form of landmarks [29,6,32], paired identity images [10] or ground truth 3D geometry [13]. The Model-based Face Autoencoder (MoFA) of Tewari et al. [29] did demonstrate a completely unsupervised variant but the estimated", "Introduction", "introduction"], ["e30e96cff46897350a126f103942f2c116328e18_2", "Correspondence Geometry Reconstruction Albedo Fig. 1: From a single input image our network learns to predict dense correspondence. From this, we can infer least squares optimal 3DMM geometry and albedo giving high quality reconstructions with 2D transformation invariance.\nface is prone to shrinking into the inner face region and requires careful prealignment of training images and initialisation of camera parameter predictions such that the initial 3DMM models approximately align with the face images. This makes the approach unable to learn invariance to 2D transformations.\nIn this paper, we propose a completely unsupervised strategy for learning to fit a 3DMM to a single image. The main difference to previous work is that, instead of image-to-3DMM parameter regression with a contractive CNN, we propose to estimate a dense image-model correspondence map with an imageto-image CNN architecture. There are significant benefits in doing so:\n1. All 3DMM parameters can be estimated from a correspondence map (Section 2). Therefore, using a CNN to predict both geometric and photometric parameters, as done in all previous work [29,13,10,28,6], is redundant. 2. The estimated parameters are least squares optimal with respect to the input image and estimated correspondence map. Optimality for a given image is not guaranteed for a parameter regression CNN whose training objective seeks optimality only in aggregate over the whole training set. 3. Image-to-image CNNs are well suited to estimating correspondence maps with invariance to 2D transformations. Intuitively, it is enough for the correspondence CNN to learn \"part detectors\" with robustness to 2D rotation (convolution layers are already translation invariant). On the other hand, contractive CNNs are ill-suited to directly regressing geometric parameters with 2D transformation invariance [15]. This is because spatial information is lost in contractive layers and fully connected layers must exhaustively represent both features and their locations to reason about geometric parameters. 4. Image-to-image CNNs are much smaller than parameter regression networks due to the lack of fully connected layers. Concretely, we require \" 10\u02c6fewer parameters than previous CNN based approaches (e.g. 13.4M parameters for our U-Net versus 138M parameters in VGG-face used by [10,29]). 5. Every pixel in the input image can contribute to the losses during training. Previous model-based methods learn only from the parts of the image covered by the geometry of the current 3DMM estimate. In our approach, there is no longer a shortcut for the network to reduce reconstruction loss by shrinking the model to avoid difficult pixels. 6. We defer estimation of actual face geometry. Correspondence is an intermediate representation from which we infer geometry. At test time, if we have access to calibration information or have multiple images from the same camera (e.g. a video), we can exploit these constraints when we finally compute shape from the estimated correspondence map(s). Parameter regression networks cannot do this -they commit to an explanation of the shape and camera parameters for a single image with no way to inject calibration information or constraints post hoc.\nAlternatively, our approach can be viewed as a means to learn dense face alignment using model fitting as a form of self-supervision. Correspondence is, in itself, a useful representation. Once trained, the 3DMM can be discarded and the correspondence estimation network used for tasks such as landmarking or semantic segmentation without requiring ground truth labels for supervision.\nOur specific novel contributions are as follows. We interpolate a 3DMM to pixel space (Sec. 2.1) then show how to estimate both camera and shape parameters from a correspondence map using linear least squares (Sec. 2.2 and 3.2). We propose an inverse spherical harmonic lighting model enabling simultaneous least squares inverse rendering for both albedo and lighting parameters (Sec. 2.3 and 3.3). Finally, we combine the two least squares solutions with a robust residual loss, a reconstruction loss and priors to enable unsupervised training of our dense alignment network (Sec. 3.4). We make an implementation available 3 .", "Input", "introduction"], ["e30e96cff46897350a126f103942f2c116328e18_3", "Deep integration of 3DMMs The power of deep learning and CNNs has been applied to the task of face model fitting in the last 2 years. Tran et al. [31] use the results of [19] train a CNN discriminatively to regress the same parameters for any single image of the same person. Richardson et al. [22] use synthetic renderings as training data. Both these methods are supervised. MoFA [29] essentially merges analysis-by-synthesis and CNN-based regression in an autoencoder architecture in which the encoder learns the inverse problem, supervised by an appearance error provided by a fixed decoder that implements the forward process. Kim et al. [13] take a similar approach but train on synthetic data that is progressively updated to make it match the distribution of real images. Rather than require that the appearance of the reconstructed model matches that of the input image, Genova et al. [10] use a face encoder to measure similarity in an identity space. Hence, they do not estimate pose or establish correspondence to the input image, but instead ensure discriminative texture and shape are reconstructed. This can be seen as a self-supervised variant of Tran et al. [31]. A number of extension to MoFA have since been considered. Tewari et al. [28] learn a corrective space to augment the model reconstruction with additional details. Both Tran and Liu [17,32] and Tewari et al. [27] learn the model itself. In contrast to all of these approaches, we do not regress 3DMM parameters. Instead, we regress an intermediate pixel-wise representation of geometry from which geometric and photometric parameters can be directly inferred in a least squares optimal sense. Importantly, all pixels contribute to this solution, not only those covered by a rendering of the model.\nImage-to-image methods Going beyond model fitting, a number of methods make pixel-wise predictions. SFSNet [26] infers lighting and normal and albedo maps from single face images. Their training is bootstrapped using synthetic faces sampled from a model. Sela et al. [25] use an image-to-image network to predict facial depth and correspondence to a canonical model. The network is trained entirely supervised using synthetic data and model fitting requires an offline nonrigid registration to the estimated correspondences. Guler et al. [3] and Yu et al. [35] predict dense correspondence maps using an image-to-image network and supervision provided by landmark-based 3DMM fits. Feng et al. [8] predict a UV map from a 3D face to 2D image coordinates. Zhu et al. [38,39] propose the projected normalised coordinate code (PNCC) as a representation for dense correspondence. Crispell and Bazik [5] augment PNCC with a predicted 3D offset. All of these approaches are supervised. Several [25,35,5] fit a model to estimated depth or correspondence, but this is done as an offline, nonlinear optimisation. In contrast, we show how to fit a 3DMM in-network. This means that we can use the residuals as a supervisory signal for the image-to-image network, negating the need for any direct supervision.", "Related work", "background"], ["e30e96cff46897350a126f103942f2c116328e18_4", "We begin by asking: What can be estimated given dense image-model correspondence alone? Specifically, since we wish to incorporate the estimation process into a network, we are interested in what can be estimated efficiently and in a differentiable manner. Linear least squares satisfies both of these requirements and we use it to estimate optimal geometric and, subsequently, photometric parameters. This necessitates interpolating our 3DMM to pixel space which we explain first.", "3DMM parameters from image-model correspondence", "background"], ["e30e96cff46897350a126f103942f2c116328e18_5", "We represent a 3D face based on a 3DMM:\nv j p\u03b1q \" Ns`Ne \u00ff i\"1 \u03b1 i s i j`sj , r j p\u03b2q \" Nr \u00ff i\"1 \u03b2 i a i j`\u0101j (1)\nwhere v j is the 3D position and r j is the RGB albedo (or reflectance) of the jth vertex respectively. s i j is ith linear basis of the vertex position ands j is its mean. In the same manner, a i j is ith linear basis of the vertex albedo and\nvj rj v\u03b1pu, vq r \u03b2 pu, vq (a) (b) (c) (d)\nFig. 2: A 3D morphable model of geometry (a) and albedo (b) can be interpolated to a UV space (c,d) via an embedding. We refer to this as a UV-3DMM.\na j is its mean. \u03b1 i and \u03b2 i are the ith coefficient of the linear combination with \u03b1 \" r\u03b1 1 , . . . , \u03b1 Ns`Ne s T the stacked shape parameters and \u03b2 \" r\u03b2 1 , . . . , \u03b2 Nr s T the stacked albedo parameters. N s , N e and N r are the number of dimensions for neutral shape, expression and albedo respectively.\nUV interpolation of the 3DMM We compute a UV embedding for our 3DMM (in practice by flattening the mean shape -see supplementary material for details) such that every vertex is assigned a fixed 2D UV coordinate. Via barycentric interpolation we can compute a linear shape and texture model for any position, pu, vq P r\u00b41, 1s\u02c6r\u00b41, 1s, in UV space. Accordingly, we write s i pu, vq, spu, vq, a i pu, vq and\u0101pu, vq for the interpolated ith shape basis, shape mean, ith albedo basis and albedo mean at arbitrary location in UV space pu, vq. Note that pu, vq is continuous and the barycentric interpolation amounts to taking linear combinations of basis and mean values at the original vertex positions. The 3D position of the model interpolated at UV coordinate pu, vq is:\nv \u03b1 pu, vq \" S u,v \u03b1`spu, vq,(2)\nwhere S u,v \" rs 1 pu, vq, . . . , s Ns`Ne pu, vqs are the stacked shape bases for the model interpolated at UV position pu, vq. Similarly, we can write the model albedo interpolated at UV position pu, vq:\nr \u03b2 pu, vq \" A u,v \u03b2`\u0101pu, vq,(3)\nwhere again A u,v \" ra 1 pu, vq, . . . , a Nr pu, vqs are the stacked albedo bases for the model interpolated at UV position pu, vq.\nWe refer to v \u03b1 pu, vq and r \u03b2 pu, vq as a UV-3DMM (see Fig. 2).\nUV correspondence map Now, suppose that we are given a correspondence map between a face image, ipx, yq, and the UV space of our 3DMM, i.e. we are given two maps: upx, yq and vpx, yq defined for each pixel px, yq P t1, . . . , W u\u02c6t1, . . . , Hu in the face image. Each pixel provides a correspondence between image and model. We can now interpolate our 3DMM at each pixel, via the correspondence map, giving a pixel-3DMM : v \u03b1 pupx, yq, vpx, yqq and r \u03b2 pupx, yq, vpx, yqq (see Fig. 3). Details of how the interpolation is efficiently implemented in-network is described in supplementary material.  ", "Interpolating a 3DMM to UV and pixel space", "background"], ["e30e96cff46897350a126f103942f2c116328e18_6", "Assume that camera calibration information, i.e. the intrinsic matrix K P R 3\u02c63 and the extrinsic rotation R P R 3\u02c63 and translation t P R 3 , were known. Then, the perspective projection of the 3D position at model UV coordinate pu, vq to pixel position px, yq is given (up to a scaling) by:\n\u03bb \u00bb - x y 1 fi fl \" project \u03b1 pu, vq \" K \" R t \u2030 \" v \u03b1 pu, vq 1 \uf6be ,(4)\nwhere \u03bb is an arbitrary scale. Using the Direct Linear Transform [11] we can write (4) as a linear system by taking the cross product between the left and right hand sides and setting equal to the zero vector. Then, the shape parameters, \u03b1, minimising the reprojection error can be found by solving the following linear least squares problem:\nmin \u03b1 W \u00ff x\"1 H \u00ff y\"1 \u203a \u203a \u203a \u203a \u203a \u203a \u00bb - x y 1 fi fl\u02c6p roject \u03b1 pupx, yq, vpx, yqq \u203a \u203a \u203a \u203a \u203a \u203a 2 , where \" x \u2030\u02c6\" \u00bb - 0\u00b4x 3 x 2 x 3 0\u00b4x 1 x 2 x 1 0 fi fl .\n(5) Note that the residuals of the least squares solution indicate how well the model can explain a shape consistent with the correspondence map and therefore provide a measure of the plausibility of the correspondence map. In practice, \u03b1 can also be statistically regularised.\nDuring unsupervised training, we of course do not have access to camera calibration information. We later show how to rewrite (5) such that both optimal shape and camera parameters can be found algebraically using linear least squares by additionally estimating a depth map.", "Least squares shape-from-correspondence", "background"], ["e30e96cff46897350a126f103942f2c116328e18_7", "Having computed geometry from correspondence, the surface normals of the shape can be computed. Together with the original image and the correspondence from image to model, this is sufficient to reason about lighting and albedo. We now show how to simultaneously solve for lighting and albedo coefficients using linear least squares. ", "Least squares inverse rendering", "background"], ["e30e96cff46897350a126f103942f2c116328e18_8", "The spherical harmonic (SH) lighting model [20] efficiently describes how a diffuse object appears under arbitrarily complex environment illumination. At a surface point with normal direction n and RGB albedo r, the RGB colour intensity, i, is given by:\ni \" r d BpnqL,(6)\nwhere d denotes element-wise multiplication, Bpnq P R 3\u02c6N L contains the SH basis vectors which depend only on n and L P R N L\u02c63 contains the colour lighting coefficients. For an order 2 approximation, N L \" 9 and so there are 27 unknown lighting parameters. This expression is bilinear in diffuse albedo and the spherical harmonic lighting coefficients. This means there is no closed form solution for both optimal albedo and lighting simultaneously. Aldrian and Smith [2] use alternating linear least squares but this requires multiple iterations and is only optimal with respect to the parameters solved for last.\nAn inverse lighting model In contrast to the conventional model, we use spherical harmonics to represent inverse lighting. That is, a quantity that (when multiplied by the image intensity) removes the effect of shading, giving the diffuse albedo. In other words, we use the spherical harmonic basis functions to represent the reciprocal of diffuse shading:\ni d BpnqL \" r.(7)\nThis seemingly subtle difference brings a significant practical advantage: it is linear in both lighting and albedo simultaneously so we can solve for both in a single linear least squares formulation. Importantly, we show empirically in supplementary material that this inverse model can explain conventional SH lighting with very low error.\nInverse rendering with a correspondence map As in the previous section, suppose that we have an estimated correspondence map from a face image to the model. From the geometry estimated by least squares shape-fromcorrespondence, we can estimate per-vertex surface normals. Then, from the 3DMM UV map we can interpolate a surface normal, n \u03b1 pu, vq, at any position in UV space or, given the estimated image-model correspondence maps we can interpolate a pixel space normal map n \u03b1 pupx, yq, vpx, yqq (see Fig. 4(a)). Given In addition to correspondence our network also predicts a confidence map (for robustness) and depth map (enabling uncalibrated reconstruction). The least squares layer solves first for geometric and then photometric parameters.\nthe input face image, ipx, yq, we can now write a linear least squares problem for lighting and albedo parameters:\nmin L,\u03b2 W \u00ff x\"1 H \u00ff y\"1 }ipx, yq d Bpn \u03b1 pupx, yq, vpx, yqqqL\u00b4r \u03b2 pupx, yq, vpx, yqq} 2 . (8)\n3 Self-supervised learning of dense correspondence\nWe now show how an image-to-image network for dense face alignment can be trained using self-supervision (see Fig. 5). The idea is that the network predicts a correspondence map from which we implement the fitting process described in Section 2 as differentiable layers. We use a U-Net [23] as the pixel-wise prediction network though any image-to-image architecture would suffice. The network learns from losses measuring the quality of the fit to the correspondence map as well as an appearance loss computed via differentiable rendering. Some modifications are required to incorporate the least squares solutions into the network which we describe in the following sections. The various loss functions from which the network learns are combined using weights. We distinguish between those that must be manually chosen (i.e. hyperparameters of our method), denoted by \u03b7, and those that are learnt as part of the training, denoted by \u03c9.", "Spherical harmonic lighting", "background"], ["e30e96cff46897350a126f103942f2c116328e18_9", "In general, not all of the image will contain face parts. In addition, the face may be occluded by non-face objects such as glasses or unmodelled features such as beards. We do not wish these pixels to contribute to the least squares solutions. Therefore, our network also predicts a scalar confidence map wpx, yq P r0, 1s indicating whether pixel px, yq is believed to belong to the face. As with correspondence, this is learnt unsupervised without ever providing the network with ground truth face segmentations.", "Per-pixel confidence", "background"], ["e30e96cff46897350a126f103942f2c116328e18_10", "The least squares solution for geometry in (5) assumed known camera calibration. While this may be available (and can be exploited) at test time, it is not available during unsupervised training. We propose an algebraic solution that allows us to estimate both shape and camera parameters but which requires the network to also estimate a depth map, zpx, yq. Again, depth map prediction is learnt unsupervised without any ground truth depth during training. We compute the shape residuals in 3D space by back projection using inverse camera parameters and the estimated depth:\n\u03b5 geo px, yq \" \u203a \u203a zpx, yqPrx, y, 1s T`q\u00b4v \u03b1 pu, vq \u203a \u203a 2 ,(9)\nwhere the inverse camera parameters, P P R 3\u02c63 and q P R 3 , are related to standard parameters via \u03bbKR \" P\u00b41 and \u03bbKt \"\u00b4P\u00b41q with \u03bb representing the scale ambiguity. These residuals are linear in the unknown shape parameters and inverse camera parameters.\nWe can now write the linear least squares system that we solve in-network to compute optimal shape and camera parameters: \u03b1\u02da, P\u02da, q\u02da\" arg min \u03b1,P,q E geo p\u03b1, P, qq`R geo p\u03b1, P, qq,\nwhere E geo \" \u0159 x,y wpx, yq\u03b5 geo px, yq 2 is the sum of squared residuals from ( 9), weighted by the estimated per-pixel confidences and R geo \" \u03b1 T diagp\u03c9 geo q\u03b1 regularises the solution with the statistical prior, weighting each dimension with a learnable weight.\nSince ( 10) is quadratic, optimal \u03b1, P, and q can be obtained using the pseudoinverse matrix. Since the pseudoinverse is differentiable, during training loss gradients can be backpropagated through the least squares solution and into the image-to-image network.", "Uncalibrated shape-from-correspondence", "background"], ["e30e96cff46897350a126f103942f2c116328e18_11", "With the optimal shape parameters \u03b1\u02daestimated by geometric least squares, we can compute a per-pixel normal map and write the residuals of fitting our inverse lighting model:\n\u03b5 photo px, yq \" }ipx, yq d Bpn \u03b1\u02dap upx, yq, vpx, yqqqL\u00b4r \u03b2 pupx, yq, vpx, yqq} 2 .\n(11) We write a linear least squares system, this time for albedo and lighting:\n\u03b2\u02da, L\u02da\" arg min \u03b2,L E photo p\u03b2, Lq`R photo p\u03b2, Lq.(12)\nOnce again, E photo \" \u0159 x,y wpx, yq\u03b5 photo px, yq 2 is the weighted sum of squared residuals and R photo \" \u03b2 T diagp\u03c9 photo q\u03b2`\u03b7 L }L} 2 Fro regularises both albedo and lighting parameters. As for geometry, ( 12) is quadratic and so optimal \u03b2 and L can be found via the differentiable pseudoinverse.", "In-network least squares inverse rendering", "background"], ["e30e96cff46897350a126f103942f2c116328e18_12", "We train our network with four losses (described below):\nE total \" \u03b7 res E res`\u03b7rec E rec`\u03b7stat E stat`\u03b7int E int(13)\nwith \u03b7 rec \" 1.0, \u03b7 res \" 3.0, \u03b7 stat \" 1.0, and \u03b7 int \" 1.0.", "Losses", "background"], ["e30e96cff46897350a126f103942f2c116328e18_13", "The least squares layer in our network solves for optimal shape, albedo, camera and lighting parameters by minimising the geometric ( 9) and photometric ( 11) residuals. The network can learn from these residuals since they indicate how consistent the 3DMM fit is with the estimated correspondence map (and depth/confidence maps) and the image. Whereas the least squares layer required a closed form solution and therefore uses linear least squares, the loss used for network training is not so constrained. For this reason, we use a robust loss on the residuals:\nE res \" \u00ff x,y\nmin p\u03b5px, yq, 1q , where \u03b5px, yq \" \u03b7 geo \u03b5 geo px, yq`\u03b7 photo \u03b5 photo px, yq,\nand \u03b7 geo \" 20 and \u03b7 photo \" 5. This loss has an important effect: it encourages the model to expand so that more pixels in the input image can be explained by the model in both geometry and colour. For example, suppose that the pixel-wise network detects an ear with high confidence and estimates good correspondence to the ear region in the model. If the ear of the least squares 3DMM fit is not close to the detected ear pixels, this incurs a residual loss, encouraging the model to expand towards the ear. However, we must make the loss robust since every pixel in the image contributes to it, even background (we do not use the confidence map here). The clamping suppresses the effect from outlier pixels such as occlusion and background.", "Least squares residuals loss", "background"], ["e30e96cff46897350a126f103942f2c116328e18_14", "We also compute a conventional reconstruction loss using differentiable rendering to compare the fitted model to the image. Without this, the clamped residual loss does not penalise growing the face to fit to background. We render the 3DMM geometry given by the geometry least squares solution. Our differentiable renderer calculates a projection of each vertex as a 2D point on the image as well as its visibility and RGB albedo. We divide the per-vertex RGB albedo by our inverse lighting model to obtain RGB pixel intensities and measure the discrepancy to the sampled intensities:\nE rec \" 1 \u0159 Nv j\"1 w j Nv \u00ff j\"1\nw j }ipx j , y j q\u00b4r j p\u03b2\u02daq c tBpn \u03b1\u02dap upx, yq, vpx, yqqqL\u02dau} 2 , (15) where N v is the number of the vertices and w j \" 1 if a vertex is visible, zero otherwise (computed using self occlusion testing and depth testing against a zbuffer). We use differentiable bilinear sampling and ipx j , y j q represents bilinear sampling of the input image at the non-integer pixel position px j , y j q given by projection of vertex v j p\u03b1\u02daq using the estimated camera parameters.\nStatistical regularisation loss This loss encourages the network to keep the estimated face plausible in terms of the shape and albedo parameters. It is the weighted squared average of the estimated 3DMM coefficients:\nE stat \" Ns`Ne \u00ff i\"1 \u03c9 i r p\u03b1i q 2`N r \u00ff i\"1 \u03c9 i s p\u03b2i q 2 . (16\n)\nSince the 3DMM bases are normalised by their standard deviation, the statistical average of \u03b1 2 i and \u03b2 2 i should be kept to be 1 during training. We do this by controlling the loss weight \u03c9 i r and \u03c9 i s (see supplementary material). Camera intrinsics regularisation loss Finally, we employ regularisation on the estimated camera intrinsic parameters. This penalises the difference between vertical and horizontal focal length as well as the shear:\nE int \" \u03b7 asp pk 11\u00b4k22 q 2 k 2 11`k 2 22`\u03b7 sh k 2 12 k 2 11`k 2 22 ,(17)\nwhere the k ij are the elements of the intrinsic camera parameter matrix K. The first term represents the difference of vertical and horizontal focal length and the second term represents the sheer component. We normalise the loss by the horizontal and vertical focal length to avoid reducing the scale of focal length. We set \u03b7 asp \" 1.0 and \u03b7 sh \" 1.0.", "Reconstruction loss based on differentiable rendering", "background"], ["e30e96cff46897350a126f103942f2c116328e18_15", "Initialisation Supervision of our network relies on the difference of appearance between the input image and the estimated face, initial estimation must be enough close to the optimal parameters to obtain meaningful gradient from the loss function. We initialise the network (see supplementary material for details) such that for all inputs it predicts a planar depth map, a correspondence map given by the mean face centred in the image and a binary confidence map given by the rasterisation mask of the centred mean face.\nTraining data We train on \" 200k images from pre-aligned CelebA dataset [16]. We augment with random 2D similarity transformations (scale factor: r0.77, 1.3s, translation: r\u00b475, 75s pixels horizontal/vertical, rotation r\u00b4180\u02dd, 180\u02dds). The background region is filled by random images from ImageNet [14] with blended boundary. Finally, we crop the image by 224\u02c6224 pixels.\nOptimisation We use the Adadelta optimizer [36] with learning rate 0.01, batch size 3, 300k iterations. Network weights and biases are initialised by He initialisation [12]. Training takes approximately 120 hours on Nvidia GTX 1080Ti. ", "Training", "experiments"], ["e30e96cff46897350a126f103942f2c116328e18_16", "Qualitative Evaluation We qualitatively evaluate our method based on test images from CelebA dataset (Fig. 6). Our method successfully predicts 3D face including ears under arbitrary 2D similarity transformation. We compare our method with MoFA [29] which can only reconstruct the centre region of a face whereas our method can reconstruct a full head face. Our method also has bet-    ter fidelity of reconstruction due to the optimality of the least squares. We also test multiframe aggregation of the pixel-wise prediction (Fig. 7). By optimising multiframe geometry and reflectance to the intermediate output in a single optimisation, superior quality of output can be obtained. See supplementary material for additional qualitative results and comparisons.\nQuantitative Evaluation We quantitatively evaluate our method based on landmarks (Tab. 3). We follow the evaluation protocol proposed in Zhu et al. [39] and compare our result with supervised facial landmark detection methods. We evaluate landmarks obtained from both direct correspondence and fitted model.\nOur network shows comparable result to some supervised methods. We quan-Input Full w/o Eint w/o Eint&Eres Fig. 9: Ablation study to show the contribution of intrinsic parameter regularisation E int and robust residual loss E res . We show input, then for each condition we show overlaid reconstruction followed by overlaid geometry.\ntitatively evaluate our method on the NoW dataset [24] (Tab. 1, Fig. 8) and Stirling/ESRC 3D Face Database (Tab. 2) in which the error of reconstructed neutral face shape is calculated. Our method does not outperform other methods that use richer supervision though it is comparable to some supervised methods.\nAblation Study We investigate the contribution of each loss function qualitatively (Fig. 9) and quantitatively (Tab. 2). The right column in Fig. 9 shows the result trained by only the reconstruction loss and the statistical regularisation. This is a clear example of shrinking problem, and the robust residual loss significantly improves the problem. From Fig. 9 and Tab. 2, it is also clear that the intrinsic parameter regularisation enables the reconstruction of plausible and precise shape.", "Experiments", "experiments"], ["e30e96cff46897350a126f103942f2c116328e18_17", "We have presented the first method that combines trainable pixel-wise face alignment with differentiable linear least squares to reconstruct a 3D face model. To the best of our knowledge, this is the first method that enables full ear-to-ear face reconstruction under arbitrary in-plane transformation based on unsupervised training. Our approach has further potential of boosting the performance of conventional supervised face alignment methods by harnessing abundant unlabelled images as well as application to other domains in which annotated images are scarce. In future work, our method can be further improved by incorporating an occlusion model, specular reflection, and perceptual metric to alleviate the vulnerability of photometric error based optimisation. It would also be interesting to make the 3DMM learnable [32] or to estimate a corrective function [28] within our framework allowing reconstruction outside the space of the model.", "Conclusion", "results"], ["e30e96cff46897350a126f103942f2c116328e18_18", "W. Smith is supported by a Royal Academy of Engineering/The Leverhulme Trust Senior Research Fellowship.", "Acknowledgements", "acknowledgement"]], "meta": {"conference": "ECCV", "year": 2020, "authors": "Tatsuro  Koizumi, William A. P. Smith"}}