{"title": "\"Compressive neural representation of sparse, high-dimensional probabilities\"", "paper_id": "fabd2f03c98f2f54ccebde68d9fe1db31063aa7e", "segments": [["fabd2f03c98f2f54ccebde68d9fe1db31063aa7e_0", "This paper shows how sparse, high-dimensional probability distributions could be represented by neurons with exponential compression. The representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals. The compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables. When these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling. Since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed domain. Interestingly, functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons. If we use perceptrons as a simple model of feedforward computation by neurons, these results show that the mean activity of a relatively small number of neurons can accurately represent a highdimensional joint distribution implicitly, even without accounting for any noise correlations. This comprises a novel hypothesis for how neurons could encode probabilities in the brain.", "abstract", "abstract"], ["fabd2f03c98f2f54ccebde68d9fe1db31063aa7e_1", "Behavioral evidence shows that animal behaviors are often influenced not only by the content of sensory information but also by its uncertainty. Different theories have been proposed about how neuronal populations could represent this probabilistic information [1,2]. Here we propose a new theory of how neurons could represent probability distributions, based on the burgeoning field of 'compressive sensing.' An arbitrary probability distribution over multiple variables has a parameter count that is exponential in the number of variables. Representing these probabilities can therefore be prohibitively costly. One common approach is to use graphical models to parameterize the distribution in terms of a smaller number of interactions. Here I consider an alternative approach. In many cases of interest, only a few unknown states have high probabilities while the rest have neglible ones; such a distribution is called 'sparse'. I will show that sufficiently sparse distributions can be described by a number of parameters that is merely linear in the number of variables.\nUntil recently, it was generally thought that encoding of sparse signals required dense sampling at a rate greater than or equal to signal bandwidth. However, recent findings prove that it is possible to fully characterize a signal at a rate limited not by its bandwidth but by its information content [3,4,5,6] which can be much smaller. Here I apply such compression to sparse probability distributions over binary variables, which are, after all, just signals with some particular properties.\nIn most applications of compressive sensing, the ultimate goal is to reconstruct the original signal efficiently. Here, we do not wish to reconstruct the signal at all. Instead, we use the guarantees that the signal could be reconstructed to ensure that the signal is accurately represented by its compressed version. Below, when we do reconstruct it is only to show that our method actually works in practice. We don't expect that the brain needs to explicitly reconstruct a probability distribution in some canonical mathematical representation in order to gain the advantages of probabilistic reasoning.\nTraditional compressive sensing considers signals that lives in an N -dimensional space but have only S nonzero coordinates in some basis. We say that such a signal is S-sparse. If we were told the location of the nonzero entries, then we would need only S measurements to characterize their coefficients and thus the entire signal. But even if we don't know where those entries are, it still takes little more than S linear measurements to perfectly reconstruct the signal. Furthermore, those measurements can be fixed in advance without any knowledge of the structure of the signal. Under certain conditions, these excellent properties can be guaranteed [3,4,5].\nThe basic mathematical setup of compressive sensing is as follows. Assume that an N -dimensional signal s has S nonzero coefficients. We make M linear measurements y of this signal by applying the M \u00d7 N matrix A: y = As (1) We would then like to recover the original signal s from these measurements. Under conditions on the measurement matrix A described below, the original can be found perfectly by computing the vector with minimal 1 norm that reproduces the measurements,\ns = argmin s s 1 such that As = y = As (2)\nThe 1 norm is usually used instead of 0 because (2) can be solved far more efficiently [3,4,5,7].\nCompressive sensing is generally robust to two deviations from this ideal setup. First, target signals may not be strictly S-sparse. However, they may be 'compressible' in the sense that they are well approximated by an S-sparse signal. Signals whose rank-ordered coefficients fall off at least as fast as rank \u22121 satisfy this property [4]. Second, measurements may be corrupted by noise with bounded amplitude . Under these conditions, the error of the 1 -reconstructed signal\u015d is bounded by the error of the best S-sparse approximation s S plus a term proportional to the measurement noise:\n\u015d \u2212 s 2 \u2264 C 0 s S \u2212 s 2 / \u221a S + C 1 (3)\nfor some constants C 0 and C 1 [8].\nSeveral conditions on A have been used in compressive sensing to guarantee good performance [4,6,9,10,11]. Modulo various nuances, they all essentially ensure that most or all relevant sparse signals lie sufficiently far from the null space of A: It would be impossible to recover signals in the null space since their measurements are all zero and cannot therefore be distinguished. The most commonly used condition is the Restricted Isometry Property (RIP), which says that A preserves 2 norms of all S-sparse vectors within a factor of 1 \u00b1 \u03b4 S that depends on the sparsity,\n(1 \u2212 \u03b4 S ) s 2 \u2264 As 2 \u2264 (1 + \u03b4 S ) s 2(4)\nIf A satisfies the RIP with small enough \u03b4 S , then 1 recovery is guaranteed to succeed. For random matrices whose elements are independent and identically distributed Gaussian or Bernoulli variates, the RIP holds as long as the number of measurements M satisfies\nM \u2265 CS log N/S(5)\nfor some constant C that depends on \u03b4 S [8]. No other recovery method, however intractable, can perform substantially better than this [8].", "Introduction", "introduction"], ["fabd2f03c98f2f54ccebde68d9fe1db31063aa7e_2", "Compressive sensing allows us to use far fewer resources to accurately represent high-dimensional objects if they are sufficiently sparse. Even if we don't ultimately intend to reconstruct the signal, the reconstruction theorem described above (3) ensures that we have implicitly represented all the relevant information. This compression proves to be extremely useful when representing multivariate joint probability distributions, whose size is exponentially large even for the simplest binary states.\nConsider the signal to be a probability distribution over an n-dimensional binary vector x \u2208 {\u22121, +1} n , which I will write sometimes as a function p(x) and sometimes as a vector p indexed by the binary state x. I assume p is sparse in the canonical basis of delta-functions on each state, \u03b4 x,x . The dimensionality of this signal is N = 2 n , which for even modest n can be so large it cannot be represented explicitly.\nThe measurement matrix A for probability vectors has size M \u00d7 2 n . Each row corresponds to a different measurement, indexed by i. Each column corresponds to a different binary state x. This column index x ranges over all possible binary vectors of length n, in some conventional sequence. For example, if n = 3 then the column index would take the 8 values Each element of the measurement matrix, A i (x), can be viewed as a function applied to the binary state. When this matrix operates on a probability distribution p(x), the result y is a vector of M expectation values of those functions, with elements\ny i = A i p = x A i (x)p(x) = A i (x) p(x)(6)\nFor example, if A i (x) = x i then y i = x i p(x) measures the mean of x i drawn from p(x).\nFor suitable measurement matrices A, we are guaranteed accurate reconstruction of S-sparse probability distributions as long as the number of measurements is\nM \u2265 O(S log N/S) = O(Sn \u2212 S log S)(7)\nThe exponential size of the probability vector, N = 2 n , is cancelled by the logarithm. For distributions with a fixed sparseness S, the required number of measurements per variable, M/n, is then independent of the number of variables. 1 In many cases of interest it is impractical to calculate these expectation values directly: Recall that the probabilities may be too expensive to represent explicitly in the first place. One remedy is to draw T samples x t from the distribution p(x), and use a sum over these samples to approximate the expectation values,\ny i \u2248 1 T t A i (x t ) x t \u223c p(x)(8)\nThe probabilityp(x) estimated from T samples has errors with variance p(x)(1 \u2212 p(x))/T , which is bounded by 1/4T . This allows us to use the performance limits from robust compressive sensing, which according to (3) creates an error in the reconstructed probabilities that is bounded by\np \u2212 p 2 \u2264 C 0 p S \u2212 p 2 + C 1 \u221a T (9)\nwhere p S is a vector with the top S probabilities preserved and the rest set to zero. Strictly speaking, (3) applies to bounded errors, whereas here we have a bounded variance but possibly large errors.\nTo ensure accurate reconstruction, we can choose the constant C 1 large enough that errors larger than some threshold (say, 10 standard deviations) have a negligible probability.", "Compressing sparse probability distributions", "introduction"], ["fabd2f03c98f2f54ccebde68d9fe1db31063aa7e_3", "In compressive sensing it is common to use a matrix with independent Bernoulli-distributed random values, A i (x) \u223c B( 1 2 ), which guarantees A satisfies the RIP [12]. Each row of this matrix represents all possible outputs of an arbitrarily complicated Boolean function of the n binary variables x.\nBiological neural networks would have great difficulty computing such arbitrary functions in a simple manner. However, neurons can easily compute a large class of simpler boolean functions, the perceptrons. These are simple threshold functions of a weighted average of the input\nA i (x) = sgn j W ij x j \u2212 \u03b8 j (10\n)\nwhere W is an M \u00d7 n matrix. Here I take W to have elements drawn randomly from a standard normal distribution, W ij \u223c N (0, 1), and call the resultant functions 'random perceptrons'. An example measurement matrix for random perceptrons is shown in Figure 1. These functions are readily implemented by individual neurons, where x j is the instantaneous activity of neuron j, W ij is the synaptic weight between neurons i and j, and the sgn function approximates a spiking threshold at \u03b8.", "Measurements by random perceptrons", "introduction"], ["fabd2f03c98f2f54ccebde68d9fe1db31063aa7e_4", "Measurement i Figure 1: Example measurement matrix A i (x) for M = 100 random perceptrons applied to all 2 9 possible binary vectors of length n = 9.\nThe step nonlinearity sgn is not essential, but some type of nonlinearity is: Using a purely linear function of the states, A = W x, would result in measurements y = Ap = W x . This provides at most n linearly independent measurements of p(x), even when M > n. In most cases this is not enough to adequately capture the full distribution. Nonlinear A i (x) allow a greater number of linearly independent measurements of p(x). Although the dimensionality of W is merely M \u00d7 n, which is much smaller than the 2 n -dimensional space of probabilities, (10) can generate O(2 n 2 ) distinct perceptrons [13]. By including an appropriate threshold, a perceptron can assign any individual state x a positive response and assign a negative response to every other state. This shows that random perceptrons generate the canonical basis and can thus span the space of possible p(x).\nIn what follows, I assume that \u03b8 = 0 for simplicity.\nIn the Appendix I prove that random perceptrons with zero threshold satisfy the requirements for RIPless compressive sensing [6] in the limit of large n. Present research is directed toward deriving the condition number of these measurement matrices for finite n, in order to provide rigorous bounds on the number of measurements required in practice. Below I present empirical evidence that even a small number of random perceptrons largely preserves the information about sparse distributions.\n3 Experiments", "State vector x", "introduction"], ["fabd2f03c98f2f54ccebde68d9fe1db31063aa7e_5", "To test random perceptrons in compressive sensing of probabilities, I generated sparse distributions using small Boltzmann machines [14], and compressed them using random perceptrons driven by samples from the Boltzmann machine. Performance was then judged by comparing 1 reconstructions to the true distributions, which are exactly calculable for modest n.\nIn a Boltzmann Machine, binary states x occur with probabilities given by the Boltzmann distribution with energy function E(x),\np(x) \u221d e \u2212E(x) E(x) = \u2212b x \u2212 x Jx (11\n)\ndetermined by biases b and pairwise couplings J. Sampling from this distribution can be accomplished by running Glauber dynamics [15], at each time step turning a unit on with probability\np(x i = +1|x \\i ) = 1/(1 + e \u2212\u2206E ), where \u2206E = E(x i = +1, x \\i ) \u2212 E(x i = \u22121, x \\i ).\nHere x \\i is the vector of all components of x except the ith.\nFor simulations I distinguished between two types of units, hidden and visible, x = (h, v). On each trial I first generated a sample of all units according to (11). I then fixed only the visible units and allowed the hidden units to fluctuate according to the conditional probability p(h|v) to be represented. This probability is given again by the Boltzmann distribution, now with energy function\nE(h|v) = \u2212(b h \u2212 J hv v) h \u2212 h J hh h (12\n)\nAll bias terms b were set to zero, and all pairwise couplings J were random draws from a zeromean normal distribution, J ij \u223c N (0, 1 3 ). Experiments used n hidden and n visible units, with n \u2208 {8, 10, 12}. This distribution of couplings produced sparse posterior distributions whose rankordered probabilities fell faster than rank \u22121 and were thus compressible [4].\nThe compression was accomplished by passing the hidden unit activities h through random perceptrons a with weights W , according to a = sgn (W h). These perceptron activities fluctuate along with their inputs. The mean activity of these perceptron units compressively senses the probability distribution according to (8). This process of sampling and then compressing a Boltzmann distribution can be implemented by the simple neural network shown in Figure 2. We are not ultimately interested in reconstruction of the large, sparse distribution, but rather the distribution's compressed representation. Nonetheless, reconstruction is useful to show that the information has been preserved. I reconstruct sparse probabilities using nonnegative 1 minimization with measurement constraints [16,17], minimizing\np 1 + \u03bb Ap \u2212 y 2 2 (13\n)\nwhere \u03bb is a regularization parameter that was set to 2T in all simulations. Reconstructions were quite good, as shown in Figure 3. Even with far fewer measurements than signal dimensions, reconstruction accuracy is limited only by the sampling of the posterior. Enough random perceptrons do not lose any available information.\nIn the context of probability distributions, 1 reconstruction has a serious flaw: All distributions have the same 1 norm: p 1 = x p(x) = 1! To minimize the 1 norm, therefore, the estimate will not be a probability distribution. Nonetheless, the individual probabilities of the most significant states are accurately reconstructed, and only the highly improbable states are set to zero. Figure 3B shows that the shortfall is small: 1 reconstruction recovers over 90% of the total probability mass.", "Fidelity of compressed sparse distributions", "introduction"], ["fabd2f03c98f2f54ccebde68d9fe1db31063aa7e_6", "There is value in being able to compactly represent these high-dimensional objects. However, it would be especially useful to perform probabilistic computations using these representations, such as marginalization and evidence integration. Since marginalization is a linear operation on the probability distribution, this is readily implementable in the linearly compressed domain. In contrast, evidence integration is a multiplicative process acting in the canonical basis, so this operation will be more complicated after the linear distortions of compressive measurement A. Nonetheless, such computations should be feasible as long as the informative relationships are preserved in the compressed space: Similar distributions should have similar compressive representations, and dissimilar distributions should have dissimilar compressive representations. In fact, that is precisely the guarantee of compressive sensing: topological properties of the underlying space are preserved in the compressive domain [18]. Figure 4 illustrates how not only are individual sparse distributions recoverable despite significant compression, but the topology of the set of all such distributions is retained.\nFor this experiment, an input x is drawn from a dictionary of input patterns X \u2282 {+1, \u22121} n . Each pattern in X is a translation of a single binary template x 0 whose elements are generated by thresholding a noisy sinusoid (Figure 4A): x 0 j = sgn [4 sin (2\u03c0j/n) + \u03b7 j ] with \u03b7 j \u223c N (0, 1). On each trial, one of these possible patterns is drawn randomly with equal probability 1/|X |, and then is measured by a noisy process that randomly flips bits with a probability \u03b7 = 0.35 to give a noisy pattern r. This process induces a posterior distribution over the possible input patterns\np(x|r) = 1 Z p(x) i p(r i |x i ) = 1 Z p(x)\u03b7 N \u2212h(x,r) (1 \u2212 \u03b7) h(x,r)(14)\nwhere h(x, r) is the Hamming distance between x and r. This posterior is nonzero for all patterns in the dictionary. The noise level and the similarities between the dictionary elements together control the sparseness. 1000 trials of this process generates samples from the set of all possible posterior distributions. Just as the underlying set of inputs has a translation symmetry, the set of all possible posterior distributions has a cyclic permutation symmetry. This symmetry can be revealed by a nonlinear embedding [19] of the set of posteriors into two dimensions (Figure 4B).\nCompressive sensing of these posteriors by 10 random perceptrons produces a much lowerdimensional embedding that preserves this symmetry. Figure 4C shows that the same nonlinear embedding algorithm applied to the reduced representation, and one sees the same topological pattern. In compressive sensing, similarity is measured by Euclidean distance. When applied to probability distributions it will be interesting to examine instead how well information-geometric measures like the Kullback-Leibler divergence are preserved under this dimensionality reduction [20].", "Preserving computationally important relationships", "introduction"], ["fabd2f03c98f2f54ccebde68d9fe1db31063aa7e_7", "Probabilistic inference appears to be essential for both animals and machines to perform well on complex tasks with natural levels of ambiguity, but it remains unclear how the brain represents and (A) The process of generating posterior distributions: (i) A set of 100 possible patterns is generated as cyclic translations of a binary pattern (only 9 shown). With uniform probability, one of these patterns is selected (ii), and a noisy version is obtained by randomly flipping bits with probability 0.35 (iii). From such noisy patterns, an observer can infer posterior probability distributions over possible inputs (iv). (B) The set of posteriors from 1000 iterations of this process is nonlinearly mapped [19] from 100 dimensions to 2 dimensions. Each point represents one posterior and is colored according to the actual pattern from which the noisy observations were made. The permutation symmetry of this process is revealed as a circle in this mapping. (C) This circular structure is retained even after each posterior is compressed into the mean output of 10 random perceptrons. manipulates probability. Present population models of neural inference either struggle with highdimensional distributions [1] or encode them by hard-to-measure high-order correlations [2]. Here I have proposed an alternative mechanism by which the brain could efficiently represent probabilities: random perceptrons. In this model, information about probabilities is compressed and distributed in neural population activity. Amazingly, the brain need not measure any correlations between the perceptron outputs to capture the joint statistics of the sparse input distribution. Only the mean activities are required. Figure 2 illustrates one network that implements this new representation, and many variations on this circuit are possible.\nSuccessful encoding in this compressed representation requires that the input distribution be sparse.\nPosterior distributions over sensory stimuli like natural images are indeed expected to be highly sparse: the features are sparse [21], the prior over images is sparse [22], and the likelihood produced by sensory evidence is usually restrictive, so the posteriors should be even sparser. Still, it will be important to quantify just how sparse the relevant posteriors are under different conditions. This would permit us to predict how neural representations in a fixed population should degrade as sensory evidence becomes weaker.\nBrains appear to have a mix of structure and randomness. The results presented here show that purely random connections are sufficient to ensure that a sparse probability distribution is properly encoded. Surprisingly, more structured connections cannot allow a network with the same computational elements to encode distributions with substantially fewer neurons, since compressive sensing is already nearly optimal [8]. On the other hand, some representational structure may make it easier to perform computations later. Note that unknown randomness is not an impediment to further processing, as reconstruction can be performed even without explicit knowledge of random perceptron measurement matrix [23].\nEven in the most convenient representations, inference is generally intractable and requires approximation. Since compressive sensing preserves the essential geometric relationships of the signal space, learning and inference based on these relationships may be no harder after the compression, and could even be more efficient due to the reduced dimensionality. Biologically plausible mechanisms for implementing probabilistic computations in the compressed representation is important work for the future.", "Discussion", "results"], ["fabd2f03c98f2f54ccebde68d9fe1db31063aa7e_8", "To evaluate the quality of the compressive sensing matrix A, we need to ensure that S-sparse vectors are not projected to zero by the action of A. Here I show that the random perceptrons are asymptotically well-conditioned:\u00c2 \u00c2 \u2192 I for large n and M , where\u00c2 = A/ \u221a M . This ensures that distinct inputs yield distinct measurements.\nFirst I compute the mean and variance of the mean inner product C xx W between columns of\u00c2 for a given pair of states x = x . For compactness I will write w i for the ith row of the perceptron weight matrix W . Angle brackets W indicate averages over random perceptron weights W ij \u223c N (0, 1). We find\nC xx W = i\u00c2 i (x)\u00c2 i (x ) W = 1 M i sgn (w i \u2022x) sgn (w i \u2022x ) W (15\n)\nand since the different w i are independent, this implies that\nC xx W = sgn (w i \u2022x) sgn (w i \u2022x ) W (16)\nThe n-dimensional half-space in W where sgn (w i \u2022 x) = +1 intersects with the corresponding half-space for x in a wedge-shaped region with an angle of \u03b8 = cos \u22121 (x \u2022 x / x 2 x 2 ). This angle is related to the Hamming distance h = h(x, x ):\n\u03b8(h) = cos \u22121 (x \u2022 x /n) = cos \u22121 (1 \u2212 2h/n)(17)\nThe signs of w i \u2022x and w i \u2022x agree within this wedge region and its reflection about W = 0, and disagree in the supplementary wedges. The mean inner product is therefore\nC xx W =P [ sgn (w i \u2022x) = sgn (w i \u2022x )] \u2212 P [ sgn (w i \u2022x) = sgn (w i \u2022x )](18)\n=1 \u2212 2 \u03c0 \u03b8(h)\nThe variance of C xx caused by variability in W is given by\nV xx = C 2 xx W \u2212 C xx 2 W (20\n)\n= i=j \u00c2 2 i (x)\u00c2 2 i (x ) W + i =j \u00c2 i (x)\u00c2 i (x )\u00c2 j (x)\u00c2 j (x ) W \u2212 C xx 2 W (21\n) = i sgn (w i \u2022x) 2 M sgn (w i \u2022x ) 2 M W + i =j sgn (w i \u2022x) \u221a M sgn (w i \u2022x ) \u221a M 2 W \u2212 C xx 2 W (22) = 1 M + M 2 \u2212 M M 2 (1 \u2212 2\u03b8(h)/\u03c0) 2 \u2212 C xx 2 W (23) = 1 M 1 \u2212 1 \u2212 2 \u03c0 \u03b8(h(x, x )) 2(24)\nThis variance falls with M , so for large numbers of measurements M the inner products between columns concentrates around the various state-dependent mean values (19).\nNext I consider the diversity of inner products for different pairs (x, x ) of binary state vectors. I take the limit of large M so that the diversity is dominated by variations over the particular pairs, rather than by variations over measurements. The mean inner product depends only on the Hamming distance h between x and x , which for sparse signals with random support has a binomial distribution, p(h) = n h 2 \u2212n with mean n/2 and variance n/4. Designating by an overbar the average over randomly chosen states x and x , the mean C and variance \u03b4C 2 of the inner product are\nC = C xx W = 1 \u2212 2 \u03c0 cos \u22121 (1 \u2212 2h n ) = 0 (25\n)\n\u03b4C 2 = \u03b4h 2 \u2202C \u2202h 2 = n 4 16 \u03c0 2 n 2 = 4 \u03c0 2 n (26)\nThis proves that in the limit of large n and M , different columns of the random perceptron measurement matrix have inner products that concentrate around 0. The matrix of inner products is thus orthonormal almost surely,\u00c2 \u00c2 \u2192 I. Consequently, with enough measurements the random perceptrons asymptotically provide an isometry. Future work will investigate how the measurement matrix behaves for finite n and M , which will determine the number of measurements required in practice to capture a signal of a given sparseness.", "Appendix: Asymptotic orthogonality of random perceptron matrix", "results"], ["fabd2f03c98f2f54ccebde68d9fe1db31063aa7e_9", "Thanks to Alex Pouget, Jeff Beck, Shannon Starr, and Carmelita Navasca for helpful conversations.", "Acknowledgments", "acknowledgement"]], "meta": {"conference": "NIPS", "year": 2012, "authors": "Xaq  Pitkow"}}