{"title": "\"Factual\" or \"Emotional\": Stylized Image Captioning with Adaptive Learning and Attention", "paper_id": "abc7998326cc4fc3c9c0c3a9ede8ae2538439966", "segments": [["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_0", "Generating stylized captions for an image is an emerging topic in image captioning. Given an image as input, it requires the system to generate a caption that has a specific style (e.g., humorous, romantic, positive, and negative) while describing the image content semantically accurately. In this paper, we propose a novel stylized image captioning model that effectively takes both requirements into consideration. To this end, we first devise a new variant of LSTM, named style-factual LSTM, as the building block of our model. It uses two groups of matrices to capture the factual and stylized knowledge, respectively, and automatically learns the word-level weights of the two groups based on previous context. In addition, when we train the model to capture stylized elements, we propose an adaptive learning approach based on a reference factual model, it provides factual knowledge to the model as the model learns from stylized caption labels, and can adaptively compute how much information to supply at each time step. We evaluate our model on two stylized image captioning datasets, which contain humorous/romantic captions and positive/negative captions, respectively. Experiments shows that our proposed model outperforms the state-ofthe-art approaches, without using extra ground truth supervision.", "abstract", "abstract"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_1", "Automatically generating coherent captions for images has attracted remarkable attention for its strong applicability, such as picture auto-commenting [23] and helping blind people to see [11]. This task is often referred to as image captioning, which combines computer vision, natural language processing and artificial intelligence. Most recent image captioning systems focus on generating an objective, neutral and indicative caption without any style characteristics, which is defined as a factual caption. However, the art of language motivates researchers to generate captions with different styles, which can give people different feelings when focusing on a specific image. The \"style\" can refer to multiple meanings. For example, as shown in Figure 1, in terms of the fashion of the caption, caption style can be either \"romantic\" or \"humorous\". In addition, in terms of the sentiment it brings to people, caption style can be either \"positive\" or \"negative\". Without doubt, generating such kinds of captions with different styles will greatly enrich the expressibility of the captions and make them more attractive.\nIdeally, a high-performing stylized image captioning model should satisfy two requirements: 1) it generates appropriate stylized words/phrases in appropriate positions of the caption, 2) it still describes the image content accurately. Focused on stylized caption generation, existing state-of-the-art work [28] [9] train their captioning models based on two datasets separately, a large dataset with paired images and ground truth factual captions, and a small dataset with paired images and stylized ground truth captions. From the large factual dataset, the model is learned to generate factual captions that can correctly describe the images; from the small stylized dataset, the model is learned to transform factual captions to stylized captions by incorporating suitable non-factual words/phrases at correct positions of the caption. In the training and predicting process, how to effectively take these two aspects into consideration is paramount for the model to generate high quality stylized captions.\nTo combine and preserve the knowledges learned from both factual and stylized dataset, Gan et al. [9] propose a factored LSTM, which factorizes matrix W x\u2022 into three matrices (U x\u2022 , S x\u2022 , V x\u2022 ). U x\u2022 and V x\u2022 are updated by the ground truth factual captions while S x\u2022 is updated by ground truth captions with a specific style. In the predicting process, U x\u2022 , S x\u2022 and V x\u2022 are combined to generate the stylized caption. Since U x\u2022 and V x\u2022 preserve the factual information and S x\u2022 preserves the stylized information, the model can thus generate stylized captions that correspond to input images. However, for both the training and predicting processes, factored LSTM cannot differentiate whether paying more attention to the fact-related part (i.e. U x\u2022 and V x\u2022 ) or the style-related part (i.e. S x\u2022 ). It is natural that when the model focuses on predicting a stylized word, it should pay more attention to the style-related part, and vice versa. Mathews et al. [28] consider this problem and propose Senticap, which consists of two parallel LSTMs -one updated by the factual captions and one updated by the sentimental captions. When predicting a word, Senticap obtains the result by weighting the predicted word probability distributions of the two LSTMs. However, directly weighting the high level probability distributions can be too \"coarse\" in that it doesn't consider the low level attention effect on stylized and factual elements. In addition, Senticap obtains the weights of the two distributions by predicting the sentiment strength of the current word. In this step, it uses the extra ground truth word sentiment strength label, which is unavailable for other datasets.\nIn this paper, we propose a novel stylized image captioning model. In particular, we first design a new style-factual LSTM as a core building block of our model. Compared with factored LSTM, it combines fact-related and stylerelated parts of LSTM in a different way and incorporates self-attention for this two parts. More concretely, for both input word embedding feature and input", "Introduction", "introduction"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_2", "A man holds a surfboard on the beach.\nA man with his surfboard stands in the sand, hoping there are no crabs.", "Factual Humorous", "introduction"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_3", "A man holds his snowboard in the sand wishing each grain were a snowflake.", "Romantic", "introduction"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_4", "1. An awesome picture of a great building in a small town.\n2. An excellent photo of a neon sign hanging in front of a store.", "Positive", "introduction"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_5", "1. A black and white photo of an ugly building with a stupid sign out front. 2. Terrible picture to see front of a building and neon sign. hidden state of LSTM, we assign two independent groups of matrices to capture the factual and stylized knowledges, respectively. At each time step, it feeds an effective attention mechanism to weight the importance of the two groups of parameters based on previous context information, and combines the two groups of parameters by weighted-sum operation. In addition, to help the model preserve factual information while learning from stylized captions, we develop an adaptive learning approach that feeds a reference factual model as a guidance. At each time step, the model can adaptively learn whether to focus more on the ground truth stylized label or on the factual guidance, based on the similarity between the outputs of the real stylized captioning model and the reference factual model. Overall, both improvements help the model capture and combine the factual and stylized knowledge in a better way.\nIn summary, the main contributions of this paper are:\n\u2022 We propose a new stylized image captioning model, with a core building block named style-factual LSTM. Style-factual LSTM incorporates two groups of parameters with dynamic attention weights into an LSTM, to adaptively adjust the relative attention weights between the fact and style-related parts. \u2022 We develop a new learning approach to training the model on stylized captions, which adds the factual output of reference model as a guidance. The model can automatically adjust the strength of guidance based on ground truth stylized caption and reference model output without using additional information. \u2022 Our model outperforms the state-of the-art methods on both image style captioning and image sentiment captioning task, in terms of both the relevance to the image and the appropriateness of the style. \u2022 We visualize the corresponding attention weights for both the style-factual LSTM and the adaptive learning approach, and show explainable improvements in the results.", "Negative", "introduction"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_6", "Stylized image captioning is mainly related to two research topics: image captioning and style transfer. In this section, we provide the background of image captioning, attention model and style transfer, respectively.\nImage Captioning. Image captioning has received much attention in recent years due to the advances in computer vision and natural language processing. Early image captioning methods [8][7][18] [22][19] [21][6] generate sentences by combining words which are extracted from corresponding images. A downside of these methods is that their performance is limited by empirical language models. To alleviate the problem, retrieval-based frameworks are developed [20][31][14] [19]. They first retrieve similarity images of the input image from a database, then generate new descriptions for the query image by using the captions of retrieved images. However, this kind of approach relies heavily on the image database. Modern approaches [17] [39]. The attention models enable deeper image understanding by assigning different attention weights to different image regions. Bottom-up and top-down combined attention models [45] [1] are also proposed to take even one step further. In [24], the authors propose a novel adaptive attention model with a visual sentinel. This model not only can determine where to attend to in images, but also adaptively decide whether it needs to attend the image or to the LSTM decoder according to different words. Motivated by this work, we develop a novel joint style-factual attention architecture to make the model adaptively learns from the factual part and stylized part.\nStyle Transfer. Most style transfer works [10][16] [30][41] focus on image style transfer. These works utilize the Gram matrix of hidden layers to measure the distance between different styles. In the meantime, pure text style transfer is making breakthrough as the development of nature language processing. For example, Shen et al. [35] propose a cross-alignment method to transfer text into different styles by generating a shared latent content space. Hu et al. [15] propose a neural generative model that combines variational auto-encoders (VAEs) and holistic attribute discriminators, to generate sentences while controlling the attributes. Combined with the above topics, in recent years, researchers begin to focus on stylized image captioning. Gan et al. and Mathews et al. propose StyleNet [9] and SentiCap [28] to generate image captions with specific styles and sentiments, respectively. Along the same direction, we propose a novel stylized image captioning model that achieves promising performance on both tasks.", "Related Work", "background"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_7", "In this section, we formally present our stylized image captioning model. Specifically, we introduce the basic encoder-decoder image captioning model in Section 3.1. In Section 3.2, we present style-factual LSTM as the core building block of our framework. In Section 3.3, we present the overall learning strategy of the style-factual LSTM and in Section 3.4, we describe an adaptive learning approach to help the model generate stylized captions without deviating from the related image content.", "Method", "methodology"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_8", "We first describe the basic encoder-decoder model [42] for image caption generation. Giving an image I and its corresponding caption y = {y 1 , ..., y T }, the encoder-decoder model minimizes the following maximum likelihood estimation (MLE) loss function:\n\u03b8 * = arg min \u03b8 I,y log p(y|I; \u03b8)(1)\nwhere \u03b8 denotes the parameters of the model. By applying chain rule, the log likelihood of the joint probability distribution can be expressed as follows:\nlog p(y) = T t=1 log p(y t |y 1 , ..., y t\u22121 , I)(2)\nwhere we drop the dependency on \u03b8 for convenience.\nFor the encoder-decoder image captioning model, LSTM is commonly used to model p(y t |y 1 , ..., y t\u22121 , I). Specifically, it can be expressed as:\np(y t+1 |y 1 , ..., y t , I) = f (h t ) h t = LST M (x t , h t\u22121 ) (3)\nwhere h t is the hidden state of LSTM at time t, f (\u2022) is a non-linear sub-network which maps h t into word probability distribution. For t > 0, x t is the word embedding feature of word y t ; for t = 0, x 0 is the image feature of I.", "Encoder-decoder Image Captioning Model", "methodology"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_9", "To make our model capable of generating a stylized caption consistent with the image content, we devise the style-factual LSTM, which feeds two new groups of matrices S x\u2022 and S h\u2022 as the counterparts of W x\u2022 and W h\u2022 , to learn to stylize the caption. In addition, at time step t, adaptive weights g xt and g ht are synchronously learned to adjust the relative attention weights between W x\u2022 and S x\u2022 as well as W h\u2022 and S h\u2022 . The structure of style-factual LSTM is shown as Figure 2. In particular, the style-factual LSTM are defined as follows: where W x\u2022 and W h\u2022 are responsible for generating the factual caption based on the input image, while S x\u2022 and S h\u2022 are responsible for adding specific style into the caption. At time step t, the style-factual LSTM feeds h t\u22121 into two independent sub-networks with one output node, which in the end figures out g xt and g ht after using the sigmoid unit to map the outputs to the range of (0, 1). Intuitively, when the model aims to predict a factual word, g xt and g ht should be close to 0, which encourages the model to predict the word based on W x\u2022 and W h\u2022 . On the other hand, when the model focuses on predicting a stylized word, g xt and g ht should be close to 1, which encourages the model to predict the word based on S x\u2022 and S h\u2022 .\ni t = \u03c3((g xt S xi + (1 \u2212 g xt )W xi )x t + (g ht S hi + (1 \u2212 g ht )W hi )h t\u22121 + b i ) f t = \u03c3((g xt S xf + (1 \u2212 g xt )W xf )x t + (g ht S hf + (1 \u2212 g ht )W hf )h t\u22121 + b f ) o t = \u03c3((g xt S xo + (1 \u2212 g xt )W xo )x t + (g ht S ho + (1 \u2212 g ht )W ho )h t\u22121 + b o ) c t = \u03c6((g xt S xc + (1 \u2212 g xt )W xc )x t + (g ht S hc + (1 \u2212 g ht )W hc )h t\u22121 + b c ) c t = f t \u2299 c t\u22121 + i t \u2299 c t h t = o t \u2299 \u03c6(c t )(4)", "Style-factual LSTM", "methodology"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_10", "Similar to [9][25], we adopt a two-stage learning strategy to train our model. For each epoch, our model is sequentially trained by two independent stages. In the first stage, we manually fix g xt and g ht to 0, freezing the style-related matrices S x\u2022 and S h\u2022 . We train the model using the paired images and ground truth factual captions. In accordance with [42], for an image-caption pair, we first extract the deep-level feature of the image using a pre-trained CNN, and then map it into an appropriate space by a linear transformation matrix. For each word, we embed its corresponding one-hot vector by a word embedding layer such that each word embedding feature has the same dimension as the transformed image feature.  the parameters of the two attention sub-networks are updated concurrently with the whole network. Instead of only using the MLE loss, in Section 3.4, we will propose a novel approach to training our model in this stage.\nFor the test stage, to generate a stylized caption based on an image, we still compute g xt and g ht by the attention sub-networks, which activates S x\u2022 and S h\u2022 . The classical beam search approach is used to predict the caption.", "Overall Learning Strategy", "methodology"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_11", "Our goal is to generate stylized captions that can accurately describe the image at the same time. Considering our style-factual LSTM, if we directly use the MLE loss to update S x\u2022 and S h\u2022 based on Section 3.3, it will only be updated via a few ground truth stylized captions, without learning anything from the much more massive ground truth factual captions. This may lead to the situation where the generated stylized caption cannot describe the images well. Intuitively, in a specific time step, when the generated word is unrelated to style, we encourage the model to learn more from the ground truth factual captions, instead of just a small number of the ground truth stylized captions.\nMotivated by this consideration, we propose an adaptive learning approach, for which the model concurrently learns information from the ground truth stylized captions and the reference factual model, and adaptively adjusts their relative learning strengths.\nIn the second stage of the training process, giving an image and the corresponding ground truth stylized caption, in addition to predicting the stylized caption by the real model as Section 3.3, the framework also gives the predicted \"factual version\" output based on the reference model. Specifically, for reference model, we set g xt and g ht to 0, which freezes S x\u2022 and S h\u2022 as the first training stage, so that the reference model will generate its output based on W x\u2022 and W h\u2022 . Noted that W x\u2022 and W h\u2022 are trained by the ground truth factual captions.\nAt time step t, denote the predicted word probability distribution by the real model as P t s , and the predicted word probability distribution by the reference model as P t r , we first compute their KullbackLeibler divergence (KL-divergence) as follows:\nD(P t s ||P t r ) = w\u2208W P t s (w) log P t s (w) P t r (w) (5\n)\nwhere W is the word vocabulary. Intuitively, if the model focuses on generating a factual word, we aim to decrease D(P t s ||P t r ), which makes P t s similar to P t r . In contrast, if the model focuses on generating a stylized word, we update the model by the MLE loss based on the corresponding ground truth stylized word.\nTo judge whether the current predicted word is related to style or not, we compute the inner product of P t s and P t r as the factual strength of the predicted word, we denote it as g t ip , and use it to adjust the weight between MLE and KL-divergence losses. In essence, g t ip represents the similarity between the word probability distributions P t s and P t r . When g t ip is close to 0, P t s has a higher possibility to correspond to a stylized word, because the reference model does not have the capacity to generate stylized words, which in the end makes g t ip small. In this situation, a higher attention weight should be given to the MLE loss. On the other hand, when g t ip is large, P t s has a higher possibility to correspond to a factual word, we then give KL-divergence losses higher significance.\nThe complete framework with the proposed adaptive learning approach is shown in Figure 3. In the end, the new loss function for the second training stage is expressed as follows:\nLoss = T t=1 \u2212(1 \u2212 g t ip )logP t s (y t ) + \u03b1 \u2022 T t=1 g t ip D(P t s ||P t r ) (6\n)\nwhere \u03b1 is a hyper-parameter to control the relative importance of the two loss terms. In the training process, g t ip and P t r do not participate in the back propagation. Still, for the style-factual LSTM, only S x\u2022 , S h\u2022 and parameters of two attention sub-networks are updated.", "Adaptive Learning with Reference Factual Model", "methodology"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_12", "We perform extensive experiments to evaluate the proposed models. Experiments are evaluated by standard image captioning measurements -BLEU, Meteor, Rouge-L and CIDEr. We will first discuss the datasets and model settings used in the experiments. We then compare and analyze the results of the proposed model with the state-of-the-art stylized image captioning models.", "Experiments", "experiments"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_13", "At present, there are two datasets related to stylized image captioning. First, Gan et al. [9] collect a FlickrStyle10K dataset that contains 10K Flickr images with stylized captions. It should be noted that only the 7K training set are public. In particular, for the 7K images, each image is labeled with 5 factual captions, 1 humorous caption and 1 romantic caption. We randomly select 6000 of them as the training set, and 1000 of them as the test set. For the training set, we randomly split 10% of them as the validation set to adjust the hyperparameters. Second, Mathews et al. [28]  We extract image features by CNN. To make fair comparisons, for image sentiment captioning, we extract the 4096-dimension image feature by the second to last fully-connected layer of VGG-16 [36]. For stylized image captioning, we extract the 2048-dimension image feature by the last pooling layer of ResNet152 [12]. These settings are consistent with the corresponding works. Same as [28], we set the dimension of both word embedding feature and LSTM hidden state to 512 (this setting applys to all the proposed and baseline models in our experiments). For both style captioning and sentiment captioning, we use the Adam algorithm for model updating with a mini-batch size of 64 for both stages. The learning rate is set to 0.001. For style captioning, the hyper-parameter \u03b1 mentioned in Section 3.4 is set to 1.1, for sentiment captioning, \u03b1 is set to 0.9 and 1.5 for positive and negative captioning, which leads to the best performance in the validation set. Also, for style captioning, we directly input images into ResNet without normalization, which achieves better performance.", "Datasets and Model Settings", "experiments"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_14", "Experiment settings We first evaluate our proposed model on the style captioning dataset. Consistent with [9], following baselines are used for comparison: Our goal is to generate captions that are both appropriately stylized and consistent with the image. There are no definite ways to separately measure these two aspects. To measure them comprehensively, for stylized captions generated by different models, we compute the BLEU-1,2,3,4, ROUGE, CIDEr, METEOR scores based on both the ground truth stylized captions and ground truth factual captions. High-performance on both situations will demonstrate the effectiveness of the stylized image captioning model for both requirements. Because we split the dataset in a different way, we re-implement all the models and compute the scores instead of directly citing them from [9]. Experiment results Table 1 shows the quantitative results of different models based on different types of ground truth captions. Considering that for each image of the test set, we only have one ground truth stylized caption instead of five, excepts CIDEr, the overall performance of other measures based on the ground truth stylized captions is reasonably lower than [9], because these measures are sensitive to the number of ground truth captions of each image. From the results, we can see that our proposed model achieves the best performance by almost all measures, regardless of testing on stylized or factual references. This demonstrates the effectiveness of our proposed model. In addition, we could see that feeding adaptive learning approach into our model can remarkably improve the scores based on factual references, for both humorous and romantic caption generations. This indicates the improvement for generated captions' affinity toward the images. Compared with directly training the model by stylized references using MLE loss, adaptive learning can guide the model to preserve factual information in a better way, when it focuses on generating a non-stylized word. In order to prove that the proposed model is effective, we visualize the attention weights of g xt , g ht and 1 \u2212 g ip mentioned in Section 3 on several examples. Specifically, we directly input the ground truth stylized caption into the trained model step by step, so that at each time step, the model will give a predicted word based on the current input word and previous hidden state. This setting simulates the training process. For each time step, Figure 4 shows the ground truth output word and the corresponding attention weights. From the first example, we could see that when the model aims to predict stylized words, \"seeing\", \"their\", \"favourite\", \"player\", g xt (red line) and g ht (green line) increase remarkably, indicating that when the model predicts these words, it pays more attention to the S x\u2022 and S h\u2022 matrices, which capture the stylized information. Otherwise, it will focus more on W x\u2022 and W h\u2022 , which are learned to generate factual words. On the other hand, from the fourth row, when it aims to generate words \"air\", \"when\", \"their\", \"favourite\", the predicted word probability distribution similarity between the real and reference models is very low, this encourages the model to directly learn to generate these words by the MLE loss. Otherwise, it will pay considerable attention to the output of the reference model, which contains knowledge learned from ground truth factual captions. For the other three examples, still, when generating stylized phrases (i.e. \"looking for a me\", \"celebrating the fun of childhood\" and \"thinks ice cream help\"), overall, the style-factual LSTM can effectively give more attention to S x\u2022 and S h\u2022 , such that A man is doing a stunt on a bike, trying to reach outer space.\nA man is riding a bicycle on a dirt road, speed to finish the line.\nTwo horses are racing along a track to win the race.\nTwo greyhounds are racing on a track, speed to finish the line.", "Performance on stylized image captioning Dataset", "experiments"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_15", "A black dog is running through a field to meet his lover.", "A black and white dog is running through the grass to catch bones.", "experiments"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_16", "A man is climbing a large rock to conquer the high.", "A man is rock climbing on a rock wall like a lizard.", "experiments"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_17", "A group of kids are playing in a water fountain, enjoying the joys of childhood.", "A group of children playing in a fountain with full of joy.", "experiments"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_18", "Fig. 6. Examples of stylized captions generated by our model for different images.\nit will be trained mostly by corresponding ground truth words. When generating non-stylized words, the model will focus more on the factual part in the training and predicting process. It should be noticed that the first word always gets a relative high value for g xt . This is reasonable because it is usually the same word (i.e. \"a\") for both factual and stylized captions, the model thus cannot learn to give more attention to fact-related matrices at this very beginning. Also, some articles and prepositions, such as \"a\", \"of\", has low 1 \u2212 g ip even if they belong to a stylized phrase. This is also reasonable and acceptable, because both the real model and reference model can predict it, there is no need to pay all the attention to the corresponding ground truth stylized word.\nTo further substantiate that our model successfully differentiates between stylized words and factual words, following the visualization process, we compute the mean value of 1 \u2212 g ip and g ht for each word in stylized dataset. As Figure 5 shows, words that appear frequently in the stylized parts but rarely in the factual parts tend to get higher g ht . Such as \"gremlin\", \"pokeman\", \"smiley\" in humorous sentences and \"courage\", \"beauty\", \"lover\" in romantic sentences. Words that appear in the stylized and factual parts with similar frequencies are likely to hold neutral value, such as \"with\", \"go\", \"of\", \"about\". Words such as \"swimmer\", \"person\", \"skate\", \"cup\", which appear mostly in the factual parts rather than the stylized parts, tend to have lower g ht scores. Since g ht represents the stylized weights in the style-factual LSTM, the result of g ht substantiates that the style-factual LSTM is able to differentiate between stylized and factual words. When it comes to 1 \u2212 g ip , the first kind of words we mentioned above still receive high scores. However, we do not observe any clear border between the second and third kinds of words as g ht shows. Still, we attribute it to the fact that predicting a factual noun is overall more difficult than predicting an article or preposition, which makes its corresponding inner product lower, and thus makes 1 \u2212 g ip higher.\nTo make our discussion more intuitive, we show several stylized captions generated by our model in Figure 6. As Figure 6 shows, our model can generate stylized captions that accurately describe the corresponding images. For different images, the generated captions contain appropriate humorous phrases like \"reach outer space\",\"catch bones\",\"like a lizard\" and appropriate romantic phrases like \"to meet his lover\",\"speed to finish the line\",\"conquer the high\". We also evaluate our model on the image sentiment caption dataset which is collected by [28]. Following [28], we compare the proposed model with several baselines. Besides NIC, ANP-Replace is based on NIC. For each caption generated by NIC, it randomly chooses a noun and adds the most common adjective of the corresponding sentiment for the chosen noun. In a similar way, ANP-Scoring uses multi-class logistic regression to select the most likely adjective for the chosen noun. LSTM-Transfer earns a fine-tuned LSTM from the sentiment dataset with additional regularization as [34]. Senticap implements a switching LSTM with word-level regularization to generate stylized captions. It should be mentioned that Senticap utilizes ground truth word sentiment strength in their regularization, which are labeled by humans. In contrast, our model only needs ground truth image-caption pairs without extra information.", "Above: Humorous Below: Romantics", "experiments"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_19", "Table 2 shows the performance of different models on the sentiment captioning dataset. The performance score of all baselines are directly cited from [28].\nA bad view of a living room with a couch and a broken window.\nA dead bird standing on a beach next to a body of water.\nA group of people sit on a bench in front of a ugly building.\nA dirty cat sits on the edge of a toilet.\nA nice living room with a couch and a relaxing chair.\nA plate of delicious food with a good cup of coffee.\nA cute cat sitting on top of a couch next to a beautiful window.\nA pretty woman hitting a tennis ball with a tennis racquet. We can see that for positive caption generation, the performance of our proposed model remarkably outperforms other baselines, with the highest scores by almost all measures. For negative caption generation, the performance of our model is competitive with Senticap while outperforming all others. Overall, without using extra ground truth information, our model achieves the best performance for generating image captions with sentiment. Figure . 7 illustrates several sentiment captions generated by our model, as it can effectively generate captions with the sentiment elements being specified.", "Performance on Image Sentiment Captioning Dataset", "experiments"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_21", "In this paper, we present a new stylized image captioning model. We design a style-factual LSTM as the core building block of the model, which feeds two groups of matrices into the LSTM to capture both factual and stylized information. To allow the model to preserve factual information in a better way, we leverage the reference model and develop an adaptive learning approach to adaptively adding factual information into the model, based on the prediction similarity between the real and reference models. Experiments on two stylized image captioning datasets demonstrate the effectiveness of our proposed approach. It outperforms the state-of-the-art models for stylized image captioning without using extra ground truth information. Furthermore, visualization of different attention weights demonstrates that our model can indeed differentiate the factual part and stylized part of a caption automatically, and adjust the attention weights adaptively for better learning and prediction.", "Conclusions", "results"], ["abc7998326cc4fc3c9c0c3a9ede8ae2538439966_22", "We would like to thank the support of New York State through the Goergen Institute for Data Science, our corporate sponsor Adobe and NSF Award #1704309.", "Acknowledgment", "acknowledgement"]], "meta": {"conference": "ECCV", "year": 2018, "authors": "Tianlang  Chen, Zhongping  Zhang, Quanzeng  You, Chen  Fang, Zhaowen  Wang, Hailin  Jin, Jiebo  Luo"}}